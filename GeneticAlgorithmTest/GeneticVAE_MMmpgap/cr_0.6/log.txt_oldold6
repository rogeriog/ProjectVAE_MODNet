start
Sun Feb 19 20:33:08 CET 2023
2023-02-19 20:33:10.615278: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-19 20:34:21,782 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7fcb0f341fd0> object, created with modnet version 0.1.12
NAN values: 12054
NAN values remaining: 0
        AtomicOrbitals|HOMO_character  ...  BondFractions|B - B bond frac.
id                                     ...                                
0                                 3.0  ...                             0.0
1                                 3.0  ...                             0.0
2                                 2.0  ...                             0.0
3                                 2.0  ...                             0.0
4                                 2.0  ...                             0.0
...                               ...  ...                             ...
106108                            3.0  ...                             0.0
106109                            2.0  ...                             0.0
106110                            3.0  ...                             0.0
106111                            3.0  ...                             0.0
106112                            1.0  ...                             0.0

[106113 rows x 1336 columns]
[1.6 175 0.002 256 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
2023-02-19 20:34:25.998286: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-19 20:34:26.762170: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2023-02-19 20:34:26.762431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38220 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:a1:00.0, compute capability: 8.0
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2022)         2557830     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2022)        8088        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2022)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1533434     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1533434     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4678212     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 10,310,998
Trainable params: 10,301,394
Non-trainable params: 9,604
__________________________________________________________________________________________________
Epoch 1/175
2023-02-19 20:34:33.141156: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2023-02-19 20:34:33.181716: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7fc7b0027010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-02-19 20:34:33.181750: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA A100-PCIE-40GB, Compute Capability 8.0
2023-02-19 20:34:33.190540: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-02-19 20:34:34.603290: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
374/374 - 11s - loss: 0.0409 - val_loss: 0.0193 - 11s/epoch - 30ms/step
Epoch 2/175
374/374 - 2s - loss: 0.0178 - val_loss: 0.0177 - 2s/epoch - 6ms/step
Epoch 3/175
374/374 - 2s - loss: 0.0151 - val_loss: 0.0188 - 2s/epoch - 5ms/step
Epoch 4/175
374/374 - 2s - loss: 0.0147 - val_loss: 0.0197 - 2s/epoch - 5ms/step
Epoch 5/175
374/374 - 2s - loss: 0.0145 - val_loss: 0.0153 - 2s/epoch - 6ms/step
Epoch 6/175
374/374 - 2s - loss: 0.0142 - val_loss: 0.0634 - 2s/epoch - 5ms/step
Epoch 7/175
374/374 - 2s - loss: 0.0151 - val_loss: 0.0165 - 2s/epoch - 6ms/step
Epoch 8/175
374/374 - 2s - loss: 0.0138 - val_loss: 0.0364 - 2s/epoch - 6ms/step
Epoch 9/175
374/374 - 2s - loss: 0.0137 - val_loss: 0.0173 - 2s/epoch - 5ms/step
Epoch 10/175
374/374 - 2s - loss: 0.0133 - val_loss: 0.0157 - 2s/epoch - 6ms/step
Epoch 11/175
374/374 - 2s - loss: 0.0130 - val_loss: 0.0135 - 2s/epoch - 6ms/step
Epoch 12/175
374/374 - 2s - loss: 0.0128 - val_loss: 0.0135 - 2s/epoch - 5ms/step
Epoch 13/175
374/374 - 2s - loss: 0.0127 - val_loss: 0.0139 - 2s/epoch - 5ms/step
Epoch 14/175
374/374 - 2s - loss: 0.0126 - val_loss: 0.0127 - 2s/epoch - 6ms/step
Epoch 15/175
374/374 - 2s - loss: 0.0124 - val_loss: 0.0125 - 2s/epoch - 6ms/step
Epoch 16/175
374/374 - 2s - loss: 0.0123 - val_loss: 0.0125 - 2s/epoch - 6ms/step
Epoch 17/175
374/374 - 2s - loss: 0.0123 - val_loss: 0.0122 - 2s/epoch - 5ms/step
Epoch 18/175
374/374 - 2s - loss: 0.0122 - val_loss: 0.0121 - 2s/epoch - 6ms/step
Epoch 19/175
374/374 - 2s - loss: 0.0121 - val_loss: 0.0121 - 2s/epoch - 5ms/step
Epoch 20/175
374/374 - 2s - loss: 0.0121 - val_loss: 0.0120 - 2s/epoch - 6ms/step
Epoch 21/175
374/374 - 2s - loss: 0.0121 - val_loss: 0.0130 - 2s/epoch - 6ms/step
Epoch 22/175
374/374 - 2s - loss: 0.0131 - val_loss: 0.0121 - 2s/epoch - 6ms/step
Epoch 23/175
374/374 - 2s - loss: 0.0125 - val_loss: 0.0138 - 2s/epoch - 5ms/step
Epoch 24/175
374/374 - 2s - loss: 0.0142 - val_loss: 0.0134 - 2s/epoch - 6ms/step
Epoch 25/175
374/374 - 2s - loss: 0.0124 - val_loss: 0.0121 - 2s/epoch - 6ms/step
Epoch 26/175
374/374 - 2s - loss: 0.0121 - val_loss: 0.0120 - 2s/epoch - 6ms/step
Epoch 27/175
374/374 - 2s - loss: 0.0120 - val_loss: 0.0119 - 2s/epoch - 5ms/step
Epoch 28/175
374/374 - 2s - loss: 0.0120 - val_loss: 0.0120 - 2s/epoch - 5ms/step
Epoch 29/175
374/374 - 2s - loss: 0.0123 - val_loss: 0.0137 - 2s/epoch - 5ms/step
Epoch 30/175
374/374 - 2s - loss: 0.0138 - val_loss: 0.0125 - 2s/epoch - 6ms/step
Epoch 31/175
374/374 - 2s - loss: 0.0130 - val_loss: 0.0121 - 2s/epoch - 5ms/step
Epoch 32/175
374/374 - 2s - loss: 0.0121 - val_loss: 0.0120 - 2s/epoch - 5ms/step
Epoch 33/175
374/374 - 2s - loss: 0.0120 - val_loss: 0.0119 - 2s/epoch - 6ms/step
Epoch 34/175
374/374 - 2s - loss: 0.0120 - val_loss: 0.0118 - 2s/epoch - 5ms/step
Epoch 35/175
374/374 - 2s - loss: 0.0120 - val_loss: 0.0119 - 2s/epoch - 5ms/step
Epoch 36/175
374/374 - 2s - loss: 0.0119 - val_loss: 0.0119 - 2s/epoch - 6ms/step
Epoch 37/175
374/374 - 2s - loss: 0.0119 - val_loss: 0.0118 - 2s/epoch - 5ms/step
Epoch 38/175
374/374 - 2s - loss: 0.0118 - val_loss: 0.0118 - 2s/epoch - 6ms/step
Epoch 39/175
374/374 - 2s - loss: 0.0118 - val_loss: 0.0117 - 2s/epoch - 6ms/step
Epoch 40/175
374/374 - 2s - loss: 0.0119 - val_loss: 0.0127 - 2s/epoch - 6ms/step
Epoch 41/175
374/374 - 2s - loss: 0.0142 - val_loss: 0.0120 - 2s/epoch - 6ms/step
Epoch 42/175
374/374 - 2s - loss: 0.0120 - val_loss: 0.0118 - 2s/epoch - 6ms/step
Epoch 43/175
374/374 - 2s - loss: 0.0118 - val_loss: 0.0116 - 2s/epoch - 5ms/step
Epoch 44/175
374/374 - 2s - loss: 0.0117 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 45/175
374/374 - 2s - loss: 0.0115 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 46/175
374/374 - 2s - loss: 0.0115 - val_loss: 0.0113 - 2s/epoch - 5ms/step
Epoch 47/175
374/374 - 2s - loss: 0.0113 - val_loss: 0.0112 - 2s/epoch - 5ms/step
Epoch 48/175
374/374 - 2s - loss: 0.0113 - val_loss: 0.0111 - 2s/epoch - 5ms/step
Epoch 49/175
374/374 - 2s - loss: 0.0112 - val_loss: 0.0111 - 2s/epoch - 5ms/step
Epoch 50/175
374/374 - 2s - loss: 0.0139 - val_loss: 0.0131 - 2s/epoch - 6ms/step
Epoch 51/175
374/374 - 2s - loss: 0.0174 - val_loss: 0.0130 - 2s/epoch - 5ms/step
Epoch 52/175
374/374 - 2s - loss: 0.0119 - val_loss: 0.0126 - 2s/epoch - 6ms/step
Epoch 53/175
374/374 - 2s - loss: 0.0148 - val_loss: 0.0118 - 2s/epoch - 5ms/step
Epoch 54/175
374/374 - 2s - loss: 0.0117 - val_loss: 0.0114 - 2s/epoch - 5ms/step
Epoch 55/175
374/374 - 2s - loss: 0.0115 - val_loss: 0.0113 - 2s/epoch - 5ms/step
Epoch 56/175
374/374 - 2s - loss: 0.0114 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 57/175
374/374 - 2s - loss: 0.0113 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 58/175
374/374 - 2s - loss: 0.0112 - val_loss: 0.0111 - 2s/epoch - 5ms/step
Epoch 59/175
374/374 - 2s - loss: 0.0112 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 60/175
374/374 - 2s - loss: 0.0112 - val_loss: 0.0110 - 2s/epoch - 5ms/step
Epoch 61/175
374/374 - 2s - loss: 0.0112 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 62/175
374/374 - 2s - loss: 0.0111 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 63/175
374/374 - 2s - loss: 0.0111 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 64/175
374/374 - 2s - loss: 0.0113 - val_loss: 0.0134 - 2s/epoch - 5ms/step
Epoch 65/175
374/374 - 2s - loss: 0.0175 - val_loss: 0.0133 - 2s/epoch - 6ms/step
Epoch 66/175
374/374 - 2s - loss: 0.0140 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 67/175
374/374 - 2s - loss: 0.0116 - val_loss: 0.0124 - 2s/epoch - 6ms/step
Epoch 68/175
374/374 - 2s - loss: 0.0132 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 69/175
374/374 - 2s - loss: 0.0114 - val_loss: 0.0112 - 2s/epoch - 5ms/step
Epoch 70/175
374/374 - 2s - loss: 0.0113 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 71/175
374/374 - 2s - loss: 0.0112 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 72/175
374/374 - 2s - loss: 0.0130 - val_loss: 0.0253 - 2s/epoch - 6ms/step
Epoch 73/175
374/374 - 2s - loss: 0.0300 - val_loss: 0.0129 - 2s/epoch - 6ms/step
Epoch 74/175
374/374 - 2s - loss: 0.0122 - val_loss: 0.0124 - 2s/epoch - 5ms/step
Epoch 75/175
374/374 - 2s - loss: 0.0144 - val_loss: 0.0119 - 2s/epoch - 6ms/step
Epoch 76/175
374/374 - 2s - loss: 0.0119 - val_loss: 0.0116 - 2s/epoch - 5ms/step
Epoch 77/175
374/374 - 2s - loss: 0.0117 - val_loss: 0.0115 - 2s/epoch - 5ms/step
Epoch 78/175
374/374 - 2s - loss: 0.0116 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 79/175
374/374 - 2s - loss: 0.0115 - val_loss: 0.0112 - 2s/epoch - 5ms/step
Epoch 80/175
374/374 - 2s - loss: 0.0114 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 81/175
374/374 - 2s - loss: 0.0113 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 82/175
374/374 - 2s - loss: 0.0113 - val_loss: 0.0112 - 2s/epoch - 5ms/step
Epoch 83/175
374/374 - 2s - loss: 0.0113 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 84/175
374/374 - 2s - loss: 0.0112 - val_loss: 0.0110 - 2s/epoch - 5ms/step
Epoch 85/175
374/374 - 2s - loss: 0.0112 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 86/175
374/374 - 2s - loss: 0.0111 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 87/175
374/374 - 2s - loss: 0.0111 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 88/175
374/374 - 2s - loss: 0.0112 - val_loss: 0.0178 - 2s/epoch - 5ms/step
Epoch 89/175
374/374 - 2s - loss: 0.0210 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 90/175
374/374 - 2s - loss: 0.0115 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 91/175
374/374 - 2s - loss: 0.0113 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 92/175
374/374 - 2s - loss: 0.0112 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 93/175
374/374 - 2s - loss: 0.0112 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 94/175
374/374 - 2s - loss: 0.0111 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 95/175
374/374 - 2s - loss: 0.0111 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 96/175
374/374 - 2s - loss: 0.0110 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 97/175
374/374 - 2s - loss: 0.0110 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 98/175
374/374 - 2s - loss: 0.0110 - val_loss: 0.0109 - 2s/epoch - 5ms/step
Epoch 99/175
374/374 - 2s - loss: 0.0110 - val_loss: 0.0109 - 2s/epoch - 5ms/step
Epoch 100/175
374/374 - 2s - loss: 0.0109 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 101/175
374/374 - 2s - loss: 0.0109 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 102/175
374/374 - 2s - loss: 0.0109 - val_loss: 0.0108 - 2s/epoch - 5ms/step
Epoch 103/175
374/374 - 2s - loss: 0.0109 - val_loss: 0.0108 - 2s/epoch - 5ms/step
Epoch 104/175
374/374 - 2s - loss: 0.0109 - val_loss: 0.0108 - 2s/epoch - 5ms/step
Epoch 105/175
374/374 - 2s - loss: 0.0108 - val_loss: 0.0108 - 2s/epoch - 5ms/step
Epoch 106/175
374/374 - 2s - loss: 0.0108 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 107/175
374/374 - 2s - loss: 0.0108 - val_loss: 0.0107 - 2s/epoch - 5ms/step
Epoch 108/175
374/374 - 2s - loss: 0.0108 - val_loss: 0.0200 - 2s/epoch - 6ms/step
Epoch 109/175
374/374 - 2s - loss: 0.0120 - val_loss: 0.0138 - 2s/epoch - 6ms/step
Epoch 110/175
374/374 - 2s - loss: 0.0121 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 111/175
374/374 - 2s - loss: 0.0108 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 112/175
374/374 - 2s - loss: 0.0107 - val_loss: 0.0106 - 2s/epoch - 5ms/step
Epoch 113/175
374/374 - 2s - loss: 0.0107 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 114/175
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 115/175
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 5ms/step
Epoch 116/175
374/374 - 2s - loss: 0.0106 - val_loss: 0.0128 - 2s/epoch - 6ms/step
Epoch 117/175
374/374 - 2s - loss: 0.0109 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 118/175
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 119/175
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 5ms/step
Epoch 120/175
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 5ms/step
Epoch 121/175
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 122/175
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 123/175
374/374 - 2s - loss: 0.0103 - val_loss: 0.0121 - 2s/epoch - 6ms/step
Epoch 124/175
374/374 - 2s - loss: 0.0116 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 125/175
374/374 - 2s - loss: 0.0103 - val_loss: 0.0141 - 2s/epoch - 6ms/step
Epoch 126/175
374/374 - 2s - loss: 0.0110 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 127/175
374/374 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 128/175
374/374 - 2s - loss: 0.0103 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 129/175
374/374 - 2s - loss: 0.0105 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 130/175
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 131/175
374/374 - 2s - loss: 0.0102 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 132/175
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 133/175
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 134/175
374/374 - 2s - loss: 0.0102 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 135/175
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 136/175
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 137/175
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 5ms/step
Epoch 138/175
374/374 - 2s - loss: 0.0101 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 139/175
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 140/175
374/374 - 2s - loss: 0.0101 - val_loss: 0.0099 - 2s/epoch - 5ms/step
Epoch 141/175
374/374 - 2s - loss: 0.0100 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 142/175
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 143/175
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 144/175
374/374 - 2s - loss: 0.0100 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 145/175
374/374 - 2s - loss: 0.0100 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 146/175
374/374 - 2s - loss: 0.0100 - val_loss: 0.0098 - 2s/epoch - 5ms/step
Epoch 147/175
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 148/175
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 5ms/step
Epoch 149/175
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 150/175
374/374 - 2s - loss: 0.0099 - val_loss: 0.0099 - 2s/epoch - 5ms/step
Epoch 151/175
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 5ms/step
Epoch 152/175
374/374 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 5ms/step
Epoch 153/175
374/374 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 5ms/step
Epoch 154/175
374/374 - 2s - loss: 0.0099 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 155/175
374/374 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 156/175
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 157/175
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 5ms/step
Epoch 158/175
374/374 - 2s - loss: 0.0098 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 159/175
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 160/175
374/374 - 2s - loss: 0.0098 - val_loss: 0.0101 - 2s/epoch - 5ms/step
Epoch 161/175
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 162/175
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 163/175
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 164/175
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 165/175
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 166/175
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 167/175
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 168/175
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 169/175
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 170/175
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 171/175
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 172/175
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 173/175
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 174/175
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 175/175
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009511343203485012
  1/332 [..............................] - ETA: 1:33 23/332 [=>............................] - ETA: 0s   46/332 [===>..........................] - ETA: 0s 69/332 [=====>........................] - ETA: 0s 92/332 [=======>......................] - ETA: 0s115/332 [=========>....................] - ETA: 0s138/332 [===========>..................] - ETA: 0s161/332 [=============>................] - ETA: 0s184/332 [===============>..............] - ETA: 0s207/332 [=================>............] - ETA: 0s230/332 [===================>..........] - ETA: 0s253/332 [=====================>........] - ETA: 0s276/332 [=======================>......] - ETA: 0s299/332 [==========================>...] - ETA: 0s322/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.07976924487320468
cosine 0.06260429588390484
MAE: 0.03704647
RMSE: 0.081001066
r2: 0.5743609020679965
RMSE zero-vector: 0.23411466903540806
['1.6custom_VAE', 'mse', 256, 175, 0.002, 0.6, 758, 0.00964913610368967, 0.009511343203485012, 0.07976924487320468, 0.06260429588390484, 0.03704646974802017, 0.08100106567144394, 0.5743609020679965, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 175 0.0018 128 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Epoch 1/175
747/747 - 8s - loss: 0.0307 - val_loss: 0.0189 - 8s/epoch - 11ms/step
Epoch 2/175
747/747 - 4s - loss: 0.0161 - val_loss: 0.0184 - 4s/epoch - 5ms/step
Epoch 3/175
747/747 - 4s - loss: 0.0150 - val_loss: 0.0156 - 4s/epoch - 5ms/step
Epoch 4/175
747/747 - 4s - loss: 0.0143 - val_loss: 0.0168 - 4s/epoch - 5ms/step
Epoch 5/175
747/747 - 4s - loss: 0.0137 - val_loss: 0.0137 - 4s/epoch - 5ms/step
Epoch 6/175
747/747 - 4s - loss: 0.0131 - val_loss: 0.0133 - 4s/epoch - 5ms/step
Epoch 7/175
747/747 - 4s - loss: 0.0128 - val_loss: 0.0125 - 4s/epoch - 5ms/step
Epoch 8/175
747/747 - 4s - loss: 0.0126 - val_loss: 0.0124 - 4s/epoch - 5ms/step
Epoch 9/175
747/747 - 4s - loss: 0.0124 - val_loss: 0.0122 - 4s/epoch - 5ms/step
Epoch 10/175
747/747 - 4s - loss: 0.0123 - val_loss: 0.0122 - 4s/epoch - 5ms/step
Epoch 11/175
747/747 - 4s - loss: 0.0122 - val_loss: 0.0121 - 4s/epoch - 5ms/step
Epoch 12/175
747/747 - 4s - loss: 0.0121 - val_loss: 0.0120 - 4s/epoch - 5ms/step
Epoch 13/175
747/747 - 4s - loss: 0.0121 - val_loss: 0.0119 - 4s/epoch - 5ms/step
Epoch 14/175
747/747 - 4s - loss: 0.0120 - val_loss: 0.0119 - 4s/epoch - 5ms/step
Epoch 15/175
747/747 - 4s - loss: 0.0120 - val_loss: 0.0118 - 4s/epoch - 5ms/step
Epoch 16/175
747/747 - 4s - loss: 0.0119 - val_loss: 0.0118 - 4s/epoch - 5ms/step
Epoch 17/175
747/747 - 4s - loss: 0.0119 - val_loss: 0.0118 - 4s/epoch - 5ms/step
Epoch 18/175
747/747 - 4s - loss: 0.0119 - val_loss: 0.0117 - 4s/epoch - 5ms/step
Epoch 19/175
747/747 - 4s - loss: 0.0119 - val_loss: 0.0118 - 4s/epoch - 5ms/step
Epoch 20/175
747/747 - 4s - loss: 0.0119 - val_loss: 0.0117 - 4s/epoch - 5ms/step
Epoch 21/175
747/747 - 4s - loss: 0.0118 - val_loss: 0.0116 - 4s/epoch - 5ms/step
Epoch 22/175
747/747 - 4s - loss: 0.0117 - val_loss: 0.0116 - 4s/epoch - 5ms/step
Epoch 23/175
747/747 - 4s - loss: 0.0116 - val_loss: 0.0114 - 4s/epoch - 5ms/step
Epoch 24/175
747/747 - 4s - loss: 0.0115 - val_loss: 0.0113 - 4s/epoch - 5ms/step
Epoch 25/175
747/747 - 4s - loss: 0.0114 - val_loss: 0.0112 - 4s/epoch - 5ms/step
Epoch 26/175
747/747 - 4s - loss: 0.0113 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 27/175
747/747 - 4s - loss: 0.0112 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 28/175
747/747 - 4s - loss: 0.0111 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 29/175
747/747 - 4s - loss: 0.0110 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 30/175
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 31/175
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 32/175
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 33/175
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 34/175
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 35/175
747/747 - 4s - loss: 0.0106 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 36/175
747/747 - 4s - loss: 0.0106 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 37/175
747/747 - 4s - loss: 0.0106 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 38/175
747/747 - 4s - loss: 0.0106 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 39/175
747/747 - 4s - loss: 0.0106 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 40/175
747/747 - 4s - loss: 0.0105 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 41/175
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 42/175
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 43/175
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 44/175
747/747 - 4s - loss: 0.0105 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 45/175
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 46/175
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 47/175
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 48/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 49/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 50/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 51/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 52/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 53/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 54/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 55/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 56/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 57/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 58/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 59/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 60/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 61/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 62/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 63/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 64/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 65/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 66/175
747/747 - 4s - loss: 0.0102 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 67/175
747/747 - 4s - loss: 0.0102 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 68/175
747/747 - 4s - loss: 0.0102 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 69/175
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 70/175
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 71/175
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 72/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 73/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 74/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 75/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 76/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 77/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 78/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 79/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 80/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 81/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 82/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 83/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 84/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 85/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 86/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 87/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 88/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 89/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 90/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 91/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 92/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 93/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 94/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 95/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 96/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 97/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 98/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 99/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 100/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 101/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 102/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 103/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 104/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 105/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 106/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 107/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 108/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 109/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 110/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 111/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 112/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 113/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 114/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 115/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 116/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 117/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 118/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 119/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 120/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 121/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 122/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 123/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 124/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 125/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 126/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 127/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 128/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 129/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 130/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 131/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 132/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 133/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 134/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 135/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 136/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 137/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 138/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 139/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 140/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 141/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 142/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 143/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 144/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 145/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 146/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 147/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 148/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 149/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 150/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 151/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 152/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 153/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 154/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 155/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 156/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 157/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 158/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 159/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 160/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 161/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 162/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 163/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 164/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 165/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 166/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 167/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 168/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 169/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 170/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 171/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 172/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 173/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 174/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 175/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009529916569590569
  1/332 [..............................] - ETA: 1:19 23/332 [=>............................] - ETA: 0s   45/332 [===>..........................] - ETA: 0s 67/332 [=====>........................] - ETA: 0s 89/332 [=======>......................] - ETA: 0s112/332 [=========>....................] - ETA: 0s135/332 [===========>..................] - ETA: 0s158/332 [=============>................] - ETA: 0s180/332 [===============>..............] - ETA: 0s203/332 [=================>............] - ETA: 0s226/332 [===================>..........] - ETA: 0s249/332 [=====================>........] - ETA: 0s272/332 [=======================>......] - ETA: 0s295/332 [=========================>....] - ETA: 0s318/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.0783148590648035
cosine 0.06144086705416138
MAE: 0.036587197
RMSE: 0.080330774
r2: 0.5813761704661641
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'mse', 128, 175, 0.0018, 0.6, 758, 0.009711941704154015, 0.009529916569590569, 0.0783148590648035, 0.06144086705416138, 0.036587197333574295, 0.08033077418804169, 0.5813761704661641, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 180 0.002 128 2] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2396038     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2396038     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         6984938     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 15,787,054
Trainable params: 15,772,898
Non-trainable params: 14,156
__________________________________________________________________________________________________
Epoch 1/180
747/747 - 8s - loss: 0.0231 - val_loss: 0.0105 - 8s/epoch - 11ms/step
Epoch 2/180
747/747 - 4s - loss: 0.0089 - val_loss: 0.0176 - 4s/epoch - 5ms/step
Epoch 3/180
747/747 - 4s - loss: 0.0085 - val_loss: 0.0089 - 4s/epoch - 5ms/step
Epoch 4/180
747/747 - 4s - loss: 0.0081 - val_loss: 0.0258 - 4s/epoch - 5ms/step
Epoch 5/180
747/747 - 4s - loss: 0.0076 - val_loss: 0.0077 - 4s/epoch - 5ms/step
Epoch 6/180
747/747 - 4s - loss: 0.0071 - val_loss: 0.0073 - 4s/epoch - 5ms/step
Epoch 7/180
747/747 - 4s - loss: 0.0067 - val_loss: 0.0068 - 4s/epoch - 5ms/step
Epoch 8/180
747/747 - 4s - loss: 0.0065 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 9/180
747/747 - 4s - loss: 0.0072 - val_loss: 0.0064 - 4s/epoch - 5ms/step
Epoch 10/180
747/747 - 4s - loss: 0.0064 - val_loss: 0.0063 - 4s/epoch - 5ms/step
Epoch 11/180
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 12/180
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 13/180
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 14/180
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 15/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 16/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 17/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 18/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 19/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 20/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 21/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 22/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 23/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 24/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 25/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 26/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 27/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 28/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 29/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 30/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 31/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 32/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 33/180
747/747 - 4s - loss: 0.0063 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 34/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 35/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 36/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 37/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 38/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 39/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 40/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 41/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 42/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 43/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 44/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 45/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 46/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 47/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 48/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 49/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 50/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 51/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 52/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 53/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 54/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 55/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 56/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 57/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 58/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 59/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 60/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 61/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 62/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 63/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 64/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 65/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 66/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 67/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 68/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 69/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 70/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 71/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 72/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 73/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 74/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 75/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 76/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 77/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 78/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 79/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 80/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 81/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 82/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 83/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 84/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 85/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 86/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 87/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 88/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 89/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 90/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 91/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 92/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 93/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 94/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 95/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 96/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 97/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 98/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 99/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 100/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 101/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 102/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 103/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 104/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 105/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 106/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 107/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 108/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 109/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 110/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 111/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 112/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 113/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 114/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 115/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 116/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 117/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 118/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 119/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 120/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 121/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 122/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 123/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 124/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 125/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 126/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 127/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 128/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 129/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 130/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 131/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 132/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 133/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 134/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 135/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 136/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 137/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 138/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 139/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 140/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 141/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 142/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 143/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 144/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 145/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 146/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 147/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 148/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 149/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 150/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 151/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 152/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 153/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 154/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 155/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 156/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 157/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 158/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 159/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 160/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 161/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 162/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 163/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 164/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 165/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 166/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 167/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 168/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 169/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 170/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 171/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 172/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 173/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 174/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 175/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 176/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 177/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 178/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 179/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 180/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.005998977459967136
  1/332 [..............................] - ETA: 1:19 23/332 [=>............................] - ETA: 0s   45/332 [===>..........................] - ETA: 0s 68/332 [=====>........................] - ETA: 0s 91/332 [=======>......................] - ETA: 0s114/332 [=========>....................] - ETA: 0s136/332 [===========>..................] - ETA: 0s158/332 [=============>................] - ETA: 0s180/332 [===============>..............] - ETA: 0s202/332 [=================>............] - ETA: 0s224/332 [===================>..........] - ETA: 0s247/332 [=====================>........] - ETA: 0s269/332 [=======================>......] - ETA: 0s291/332 [=========================>....] - ETA: 0s313/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.12338247254372901
cosine 0.09650221869737033
MAE: 0.046635985
RMSE: 0.09993087
r2: 0.35217235623355975
RMSE zero-vector: 0.23411466903540806
['2.5custom_VAE', 'logcosh', 128, 180, 0.002, 0.6, 758, 0.006069683935493231, 0.005998977459967136, 0.12338247254372901, 0.09650221869737033, 0.046635985374450684, 0.09993086755275726, 0.35217235623355975, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.6 175 0.002 128 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3286)         4156790     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3286)        13144       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3286)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7240340     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,393,366
Trainable params: 16,378,706
Non-trainable params: 14,660
__________________________________________________________________________________________________
Epoch 1/175
747/747 - 8s - loss: 0.0382 - val_loss: 0.0312 - 8s/epoch - 11ms/step
Epoch 2/175
747/747 - 4s - loss: 0.0183 - val_loss: 0.0320 - 4s/epoch - 5ms/step
Epoch 3/175
747/747 - 4s - loss: 0.0157 - val_loss: 0.0264 - 4s/epoch - 5ms/step
Epoch 4/175
747/747 - 4s - loss: 0.0148 - val_loss: 0.0209 - 4s/epoch - 5ms/step
Epoch 5/175
747/747 - 4s - loss: 0.0142 - val_loss: 0.0151 - 4s/epoch - 5ms/step
Epoch 6/175
747/747 - 4s - loss: 0.0132 - val_loss: 0.0130 - 4s/epoch - 5ms/step
Epoch 7/175
747/747 - 4s - loss: 0.0127 - val_loss: 0.0141 - 4s/epoch - 5ms/step
Epoch 8/175
747/747 - 4s - loss: 0.0135 - val_loss: 0.0122 - 4s/epoch - 5ms/step
Epoch 9/175
747/747 - 4s - loss: 0.0123 - val_loss: 0.0118 - 4s/epoch - 5ms/step
Epoch 10/175
747/747 - 4s - loss: 0.0117 - val_loss: 0.0115 - 4s/epoch - 5ms/step
Epoch 11/175
747/747 - 4s - loss: 0.0116 - val_loss: 0.0113 - 4s/epoch - 5ms/step
Epoch 12/175
747/747 - 4s - loss: 0.0114 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 13/175
747/747 - 4s - loss: 0.0113 - val_loss: 0.0113 - 4s/epoch - 5ms/step
Epoch 14/175
747/747 - 4s - loss: 0.0115 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 15/175
747/747 - 4s - loss: 0.0112 - val_loss: 0.0113 - 4s/epoch - 5ms/step
Epoch 16/175
747/747 - 4s - loss: 0.0113 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 17/175
747/747 - 4s - loss: 0.0111 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 18/175
747/747 - 4s - loss: 0.0111 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 19/175
747/747 - 4s - loss: 0.0110 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 20/175
747/747 - 4s - loss: 0.0110 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 21/175
747/747 - 4s - loss: 0.0110 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 22/175
747/747 - 4s - loss: 0.0110 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 23/175
747/747 - 4s - loss: 0.0110 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 24/175
747/747 - 4s - loss: 0.0113 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 25/175
747/747 - 4s - loss: 0.0110 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 26/175
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 27/175
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 28/175
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 29/175
747/747 - 4s - loss: 0.0106 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 30/175
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 31/175
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 32/175
747/747 - 4s - loss: 0.0108 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 33/175
747/747 - 4s - loss: 0.0106 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 34/175
747/747 - 4s - loss: 0.0106 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 35/175
747/747 - 4s - loss: 0.0108 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 36/175
747/747 - 4s - loss: 0.0106 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 37/175
747/747 - 4s - loss: 0.0111 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 38/175
747/747 - 4s - loss: 0.0109 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 39/175
747/747 - 4s - loss: 0.0106 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 40/175
747/747 - 4s - loss: 0.0106 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 41/175
747/747 - 4s - loss: 0.0105 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 42/175
747/747 - 4s - loss: 0.0106 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 43/175
747/747 - 4s - loss: 0.0107 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 44/175
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 45/175
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 46/175
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 47/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 48/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 49/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 50/175
747/747 - 4s - loss: 0.0105 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 51/175
747/747 - 4s - loss: 0.0105 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 52/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 53/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 54/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 55/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 56/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 57/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 58/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 59/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 60/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 61/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 62/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 63/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 64/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 65/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 66/175
747/747 - 4s - loss: 0.0102 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 67/175
747/747 - 4s - loss: 0.0102 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 68/175
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 69/175
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 70/175
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 71/175
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 72/175
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 73/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 74/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 75/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 76/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 77/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 78/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 79/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 80/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 81/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 82/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 83/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 84/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 85/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 86/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 87/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 88/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 89/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 90/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 91/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 92/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 93/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 94/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 95/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 96/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 97/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 98/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 99/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 100/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 101/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 102/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 103/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 104/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 105/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 106/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 107/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 108/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 109/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 110/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 111/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 112/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 113/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 114/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 115/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 116/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 117/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 118/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 119/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 120/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 121/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 122/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 123/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 124/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 125/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 126/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 127/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 128/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 129/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 130/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 131/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 132/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 133/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 134/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 135/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 136/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 137/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 138/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 139/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 140/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 141/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 142/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 143/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 144/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 145/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 146/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 147/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 148/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 149/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 150/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 151/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 152/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 153/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 154/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 155/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 156/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 157/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 158/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 159/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 160/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 161/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 162/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 163/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 164/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 165/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 166/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 167/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 168/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 169/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 170/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 171/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 172/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 173/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 174/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 175/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009428001008927822
  1/332 [..............................] - ETA: 1:18 23/332 [=>............................] - ETA: 0s   46/332 [===>..........................] - ETA: 0s 69/332 [=====>........................] - ETA: 0s 92/332 [=======>......................] - ETA: 0s114/332 [=========>....................] - ETA: 0s136/332 [===========>..................] - ETA: 0s158/332 [=============>................] - ETA: 0s180/332 [===============>..............] - ETA: 0s202/332 [=================>............] - ETA: 0s224/332 [===================>..........] - ETA: 0s246/332 [=====================>........] - ETA: 0s268/332 [=======================>......] - ETA: 0s290/332 [=========================>....] - ETA: 0s312/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.07672786045941685
cosine 0.06022519522507686
MAE: 0.036065705
RMSE: 0.07956707
r2: 0.5892980839546972
RMSE zero-vector: 0.23411466903540806
['2.6custom_VAE', 'mse', 128, 175, 0.002, 0.6, 758, 0.0096143102273345, 0.009428001008927822, 0.07672786045941685, 0.06022519522507686, 0.03606570512056351, 0.0795670673251152, 0.5892980839546972, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.6 175 0.002 128 2] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3286)         4156790     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3286)        13144       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3286)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7240340     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,393,366
Trainable params: 16,378,706
Non-trainable params: 14,660
__________________________________________________________________________________________________
Epoch 1/175
747/747 - 7s - loss: 0.0226 - val_loss: 0.0221 - 7s/epoch - 9ms/step
Epoch 2/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0116 - 4s/epoch - 5ms/step
Epoch 3/175
747/747 - 4s - loss: 0.0083 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 4/175
747/747 - 4s - loss: 0.0081 - val_loss: 0.0117 - 4s/epoch - 5ms/step
Epoch 5/175
747/747 - 4s - loss: 0.0076 - val_loss: 0.0079 - 4s/epoch - 5ms/step
Epoch 6/175
747/747 - 4s - loss: 0.0072 - val_loss: 0.0176 - 4s/epoch - 5ms/step
Epoch 7/175
747/747 - 4s - loss: 0.0073 - val_loss: 0.0068 - 4s/epoch - 5ms/step
Epoch 8/175
747/747 - 4s - loss: 0.0068 - val_loss: 0.0070 - 4s/epoch - 5ms/step
Epoch 9/175
747/747 - 4s - loss: 0.0068 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 10/175
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 11/175
747/747 - 4s - loss: 0.0066 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 12/175
747/747 - 4s - loss: 0.0066 - val_loss: 0.0064 - 4s/epoch - 5ms/step
Epoch 13/175
747/747 - 4s - loss: 0.0064 - val_loss: 0.0064 - 4s/epoch - 5ms/step
Epoch 14/175
747/747 - 4s - loss: 0.0065 - val_loss: 0.0063 - 4s/epoch - 5ms/step
Epoch 15/175
747/747 - 4s - loss: 0.0063 - val_loss: 0.0063 - 4s/epoch - 5ms/step
Epoch 16/175
747/747 - 4s - loss: 0.0064 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 17/175
747/747 - 4s - loss: 0.0063 - val_loss: 0.0063 - 4s/epoch - 5ms/step
Epoch 18/175
747/747 - 4s - loss: 0.0064 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 19/175
747/747 - 4s - loss: 0.0063 - val_loss: 0.0064 - 4s/epoch - 5ms/step
Epoch 20/175
747/747 - 4s - loss: 0.0067 - val_loss: 0.0065 - 4s/epoch - 5ms/step
Epoch 21/175
747/747 - 4s - loss: 0.0066 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 22/175
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 23/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 24/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 25/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0065 - 4s/epoch - 5ms/step
Epoch 26/175
747/747 - 4s - loss: 0.0065 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 27/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 28/175
747/747 - 4s - loss: 0.0063 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 29/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 30/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 31/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 32/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 33/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 34/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 35/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 36/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 37/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 38/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 39/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 40/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 41/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 42/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 43/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 44/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 45/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 46/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 47/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 48/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 49/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 50/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 51/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 52/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 53/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 54/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 55/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 56/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 57/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 58/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 59/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 60/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 61/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 62/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 63/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 64/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 65/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 66/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 67/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 68/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 69/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 70/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 71/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 72/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 73/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 74/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 75/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 76/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 77/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 78/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 79/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 80/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 81/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 82/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 83/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 84/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 85/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 86/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 87/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 88/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 89/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 90/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 91/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 92/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 93/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 94/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 95/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 96/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 97/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 98/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 99/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 100/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 101/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 102/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 103/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 104/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 105/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 106/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 107/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 108/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 109/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 110/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 111/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 112/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 113/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 114/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 115/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 116/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 117/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 118/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 119/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 120/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 121/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 122/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 123/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 124/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 125/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 126/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 127/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 128/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 129/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 130/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 131/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 132/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 133/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 134/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 135/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 136/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 137/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 138/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 139/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 140/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 141/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 142/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 143/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 144/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 145/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 146/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 147/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 148/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 149/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 150/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 151/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 152/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 153/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 154/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 155/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 156/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 157/175
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 158/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 159/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 160/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 161/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 162/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 163/175
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 164/175
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 165/175
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 166/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 167/175
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 168/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 169/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 170/175
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 171/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 172/175
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 173/175
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 174/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 175/175
747/747 - 4s - loss: 0.0059 - val_loss: 0.0059 - 4s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.005890522617846727
  1/332 [..............................] - ETA: 1:20 23/332 [=>............................] - ETA: 0s   46/332 [===>..........................] - ETA: 0s 68/332 [=====>........................] - ETA: 0s 89/332 [=======>......................] - ETA: 0s112/332 [=========>....................] - ETA: 0s134/332 [===========>..................] - ETA: 0s156/332 [=============>................] - ETA: 0s179/332 [===============>..............] - ETA: 0s201/332 [=================>............] - ETA: 0s224/332 [===================>..........] - ETA: 0s247/332 [=====================>........] - ETA: 0s270/332 [=======================>......] - ETA: 0s293/332 [=========================>....] - ETA: 0s316/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.11580230972753845
cosine 0.09060851926853915
MAE: 0.044824034
RMSE: 0.096968114
r2: 0.39001637289993235
RMSE zero-vector: 0.23411466903540806
['2.6custom_VAE', 'logcosh', 128, 175, 0.002, 0.6, 758, 0.0059468853287398815, 0.005890522617846727, 0.11580230972753845, 0.09060851926853915, 0.044824033975601196, 0.09696811437606812, 0.39001637289993235, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.7 175 0.0015999999999999999 256 2] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3412)         4316180     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3412)        13648       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3412)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2587054     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2587054     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7495742     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,999,678
Trainable params: 16,984,514
Non-trainable params: 15,164
__________________________________________________________________________________________________
Epoch 1/175
374/374 - 7s - loss: 0.0319 - val_loss: 0.0303 - 7s/epoch - 20ms/step
Epoch 2/175
374/374 - 2s - loss: 0.0103 - val_loss: 0.0251 - 2s/epoch - 6ms/step
Epoch 3/175
374/374 - 2s - loss: 0.0092 - val_loss: 0.0126 - 2s/epoch - 6ms/step
Epoch 4/175
374/374 - 2s - loss: 0.0084 - val_loss: 0.0126 - 2s/epoch - 6ms/step
Epoch 5/175
374/374 - 2s - loss: 0.0081 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 6/175
374/374 - 2s - loss: 0.0078 - val_loss: 0.0599 - 2s/epoch - 6ms/step
Epoch 7/175
374/374 - 2s - loss: 0.0106 - val_loss: 0.0075 - 2s/epoch - 6ms/step
Epoch 8/175
374/374 - 2s - loss: 0.0069 - val_loss: 0.0074 - 2s/epoch - 6ms/step
Epoch 9/175
374/374 - 2s - loss: 0.0068 - val_loss: 0.0128 - 2s/epoch - 6ms/step
Epoch 10/175
374/374 - 2s - loss: 0.0068 - val_loss: 0.0072 - 2s/epoch - 6ms/step
Epoch 11/175
374/374 - 2s - loss: 0.0066 - val_loss: 0.0070 - 2s/epoch - 6ms/step
Epoch 12/175
374/374 - 2s - loss: 0.0066 - val_loss: 0.0759 - 2s/epoch - 6ms/step
Epoch 13/175
374/374 - 2s - loss: 0.0082 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 14/175
374/374 - 2s - loss: 0.0065 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 15/175
374/374 - 2s - loss: 0.0065 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 16/175
374/374 - 2s - loss: 0.0064 - val_loss: 0.0083 - 2s/epoch - 6ms/step
Epoch 17/175
374/374 - 2s - loss: 0.0068 - val_loss: 0.0071 - 2s/epoch - 6ms/step
Epoch 18/175
374/374 - 2s - loss: 0.0064 - val_loss: 0.0091 - 2s/epoch - 6ms/step
Epoch 19/175
374/374 - 2s - loss: 0.0073 - val_loss: 0.0065 - 2s/epoch - 5ms/step
Epoch 20/175
374/374 - 2s - loss: 0.0067 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 21/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 22/175
374/374 - 2s - loss: 0.0064 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 23/175
374/374 - 2s - loss: 0.0071 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 24/175
374/374 - 2s - loss: 0.0066 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 25/175
374/374 - 2s - loss: 0.0065 - val_loss: 0.0068 - 2s/epoch - 6ms/step
Epoch 26/175
374/374 - 2s - loss: 0.0079 - val_loss: 0.0069 - 2s/epoch - 6ms/step
Epoch 27/175
374/374 - 2s - loss: 0.0092 - val_loss: 0.0074 - 2s/epoch - 6ms/step
Epoch 28/175
374/374 - 2s - loss: 0.0095 - val_loss: 0.0070 - 2s/epoch - 6ms/step
Epoch 29/175
374/374 - 2s - loss: 0.0068 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 30/175
374/374 - 2s - loss: 0.0066 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 31/175
374/374 - 2s - loss: 0.0065 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 32/175
374/374 - 2s - loss: 0.0065 - val_loss: 0.0078 - 2s/epoch - 6ms/step
Epoch 33/175
374/374 - 2s - loss: 0.0088 - val_loss: 0.0076 - 2s/epoch - 6ms/step
Epoch 34/175
374/374 - 2s - loss: 0.0081 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 35/175
374/374 - 2s - loss: 0.0067 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 36/175
374/374 - 2s - loss: 0.0065 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 37/175
374/374 - 2s - loss: 0.0065 - val_loss: 0.0081 - 2s/epoch - 6ms/step
Epoch 38/175
374/374 - 2s - loss: 0.0088 - val_loss: 0.0068 - 2s/epoch - 6ms/step
Epoch 39/175
374/374 - 2s - loss: 0.0067 - val_loss: 0.0086 - 2s/epoch - 6ms/step
Epoch 40/175
374/374 - 2s - loss: 0.0116 - val_loss: 0.0081 - 2s/epoch - 6ms/step
Epoch 41/175
374/374 - 2s - loss: 0.0074 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 42/175
374/374 - 2s - loss: 0.0067 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 43/175
374/374 - 2s - loss: 0.0066 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 44/175
374/374 - 2s - loss: 0.0066 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 45/175
374/374 - 2s - loss: 0.0065 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 46/175
374/374 - 2s - loss: 0.0064 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 47/175
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 48/175
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 49/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 50/175
374/374 - 2s - loss: 0.0065 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 51/175
374/374 - 2s - loss: 0.0121 - val_loss: 0.0069 - 2s/epoch - 6ms/step
Epoch 52/175
374/374 - 2s - loss: 0.0067 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 53/175
374/374 - 2s - loss: 0.0065 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 54/175
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 55/175
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 56/175
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 57/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 58/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 59/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 60/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 61/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0078 - 2s/epoch - 6ms/step
Epoch 62/175
374/374 - 2s - loss: 0.0070 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 63/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 64/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0079 - 2s/epoch - 6ms/step
Epoch 65/175
374/374 - 2s - loss: 0.0071 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 66/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 67/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 68/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 69/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 70/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 71/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 72/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 73/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 74/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 75/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 76/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 77/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 78/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 79/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 80/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0072 - 2s/epoch - 6ms/step
Epoch 81/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 82/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 83/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 84/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 85/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 86/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 87/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 88/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 89/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 90/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 91/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 92/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 93/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 94/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 95/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 96/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0072 - 2s/epoch - 6ms/step
Epoch 97/175
374/374 - 2s - loss: 0.0069 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 98/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 99/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 100/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 101/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 102/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 103/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 104/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 105/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 106/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 107/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 108/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 109/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 110/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 111/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 112/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 113/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 114/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 115/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 116/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 117/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 118/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 119/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 120/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 121/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 122/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 123/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 124/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 125/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 126/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 127/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 128/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 129/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 130/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 131/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 132/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 133/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 134/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 135/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 136/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 137/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 138/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 139/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 140/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 141/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 142/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 143/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 144/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 145/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 146/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 147/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 148/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 149/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 150/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 151/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 152/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 153/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 154/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 155/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 156/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 157/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 158/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 159/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 160/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 161/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 162/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 163/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 164/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 165/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 166/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 167/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 168/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 169/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 170/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 171/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 172/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 173/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 174/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 175/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.006019556894898415
  1/332 [..............................] - ETA: 46s 39/332 [==>...........................] - ETA: 0s  77/332 [=====>........................] - ETA: 0s116/332 [=========>....................] - ETA: 0s155/332 [=============>................] - ETA: 0s194/332 [================>.............] - ETA: 0s232/332 [===================>..........] - ETA: 0s268/332 [=======================>......] - ETA: 0s307/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.12559734894853694
cosine 0.09832202315982046
MAE: 0.04697313
RMSE: 0.10075993
r2: 0.3413784246332088
RMSE zero-vector: 0.23411466903540806
['2.7custom_VAE', 'logcosh', 256, 175, 0.0015999999999999999, 0.6, 758, 0.00607556477189064, 0.006019556894898415, 0.12559734894853694, 0.09832202315982046, 0.046973131597042084, 0.10075993090867996, 0.3413784246332088, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 170 0.002 256 2] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Epoch 1/170
374/374 - 5s - loss: 0.0249 - val_loss: 0.0175 - 5s/epoch - 13ms/step
Epoch 2/170
374/374 - 2s - loss: 0.0091 - val_loss: 0.0190 - 2s/epoch - 6ms/step
Epoch 3/170
374/374 - 2s - loss: 0.0080 - val_loss: 0.0091 - 2s/epoch - 6ms/step
Epoch 4/170
374/374 - 2s - loss: 0.0078 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 5/170
374/374 - 2s - loss: 0.0078 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 6/170
374/374 - 2s - loss: 0.0078 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 7/170
374/374 - 2s - loss: 0.0075 - val_loss: 0.0087 - 2s/epoch - 5ms/step
Epoch 8/170
374/374 - 2s - loss: 0.0073 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 9/170
374/374 - 2s - loss: 0.0073 - val_loss: 0.0098 - 2s/epoch - 5ms/step
Epoch 10/170
374/374 - 2s - loss: 0.0072 - val_loss: 0.0074 - 2s/epoch - 6ms/step
Epoch 11/170
374/374 - 2s - loss: 0.0070 - val_loss: 0.0074 - 2s/epoch - 5ms/step
Epoch 12/170
374/374 - 2s - loss: 0.0070 - val_loss: 0.0072 - 2s/epoch - 6ms/step
Epoch 13/170
374/374 - 2s - loss: 0.0068 - val_loss: 0.0083 - 2s/epoch - 6ms/step
Epoch 14/170
374/374 - 2s - loss: 0.0067 - val_loss: 0.0066 - 2s/epoch - 5ms/step
Epoch 15/170
374/374 - 2s - loss: 0.0065 - val_loss: 0.0211 - 2s/epoch - 6ms/step
Epoch 16/170
374/374 - 2s - loss: 0.0067 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 17/170
374/374 - 2s - loss: 0.0065 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 18/170
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 19/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 20/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 21/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0070 - 2s/epoch - 6ms/step
Epoch 22/170
374/374 - 2s - loss: 0.0064 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 23/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 24/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 25/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 26/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 27/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 28/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 29/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 30/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 31/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 32/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 33/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 34/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 35/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 36/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 37/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 38/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 39/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 40/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 41/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 42/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 43/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 44/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 45/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 46/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 47/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 48/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 49/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 50/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 51/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 52/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 53/170
374/374 - 2s - loss: 0.0066 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 54/170
374/374 - 2s - loss: 0.0065 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 55/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 56/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 57/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 58/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 59/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 60/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 61/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 62/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 63/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 64/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 65/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 66/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 67/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 68/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 69/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 70/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 71/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 72/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 73/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 74/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 75/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 76/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 77/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 78/170
374/374 - 2s - loss: 0.0065 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 79/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 80/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 81/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 82/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 83/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 84/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 85/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 86/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 87/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 88/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 89/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 90/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 91/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 92/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 93/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 94/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 95/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 96/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 97/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 98/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 99/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 100/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 101/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 102/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 103/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 104/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 105/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 106/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 107/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 108/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 109/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 110/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 5ms/step
Epoch 111/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 112/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 113/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 114/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 115/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 116/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 117/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 118/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 119/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 120/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 121/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 122/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 123/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 124/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 125/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 126/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 127/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 128/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 129/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 130/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 131/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 132/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 133/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 134/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 135/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 136/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 137/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 138/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 139/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 140/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 141/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 142/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 143/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 144/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 145/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 146/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 147/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 148/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 149/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 150/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 151/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 152/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 153/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 154/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 155/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 156/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 157/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 158/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 159/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 160/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 161/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 162/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 163/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 164/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 165/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 166/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 167/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 168/170
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 169/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 5ms/step
Epoch 170/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0060170660726726055
  1/332 [..............................] - ETA: 46s 38/332 [==>...........................] - ETA: 0s  77/332 [=====>........................] - ETA: 0s116/332 [=========>....................] - ETA: 0s155/332 [=============>................] - ETA: 0s194/332 [================>.............] - ETA: 0s233/332 [====================>.........] - ETA: 0s272/332 [=======================>......] - ETA: 0s311/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.12426545348789303
cosine 0.0971738323990552
MAE: 0.046776146
RMSE: 0.100269355
r2: 0.34777648895233626
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'logcosh', 256, 170, 0.002, 0.6, 758, 0.006052996963262558, 0.0060170660726726055, 0.12426545348789303, 0.0971738323990552, 0.046776145696640015, 0.10026935487985611, 0.34777648895233626, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 175 0.0018 128 2] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1769)        7076        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1769)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4165381     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,093,562
Trainable params: 9,084,970
Non-trainable params: 8,592
__________________________________________________________________________________________________
Epoch 1/175
747/747 - 9s - loss: 0.0181 - val_loss: 0.0209 - 9s/epoch - 12ms/step
Epoch 2/175
747/747 - 4s - loss: 0.0084 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 3/175
747/747 - 4s - loss: 0.0079 - val_loss: 0.0088 - 4s/epoch - 5ms/step
Epoch 4/175
747/747 - 4s - loss: 0.0076 - val_loss: 0.0086 - 4s/epoch - 5ms/step
Epoch 5/175
747/747 - 4s - loss: 0.0074 - val_loss: 0.0075 - 4s/epoch - 5ms/step
Epoch 6/175
747/747 - 4s - loss: 0.0071 - val_loss: 0.0072 - 4s/epoch - 5ms/step
Epoch 7/175
747/747 - 4s - loss: 0.0070 - val_loss: 0.0069 - 4s/epoch - 5ms/step
Epoch 8/175
747/747 - 4s - loss: 0.0069 - val_loss: 0.0068 - 4s/epoch - 5ms/step
Epoch 9/175
747/747 - 4s - loss: 0.0068 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 10/175
747/747 - 4s - loss: 0.0068 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 11/175
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 12/175
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 13/175
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 14/175
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 15/175
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 16/175
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 17/175
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 18/175
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 19/175
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 20/175
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 21/175
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 22/175
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 23/175
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 24/175
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 25/175
747/747 - 4s - loss: 0.0066 - val_loss: 0.0065 - 4s/epoch - 5ms/step
Epoch 26/175
747/747 - 4s - loss: 0.0064 - val_loss: 0.0063 - 4s/epoch - 5ms/step
Epoch 27/175
747/747 - 4s - loss: 0.0064 - val_loss: 0.0063 - 4s/epoch - 5ms/step
Epoch 28/175
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 29/175
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 30/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 31/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 32/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 33/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 34/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 35/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 36/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 37/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 38/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 39/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 40/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 41/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 42/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 43/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 44/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 45/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 46/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 47/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 48/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 49/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 50/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 51/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 52/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 53/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 54/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 55/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 56/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 57/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 58/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 59/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 60/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 61/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 62/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 63/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 64/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 65/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 66/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 67/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 68/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 69/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 70/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 71/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 72/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 73/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 74/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 75/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 76/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 77/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 78/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 79/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 80/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 81/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 82/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 83/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 84/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 85/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 86/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 87/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 88/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 89/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 90/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 91/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 92/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 93/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 94/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 95/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 96/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 97/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 98/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 99/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 100/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 101/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 102/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 103/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 104/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 105/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 106/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 107/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 108/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 109/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 110/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 111/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 112/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 113/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 114/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 115/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 116/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 117/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 118/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 119/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 120/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 121/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 122/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 123/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 124/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 125/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 126/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 127/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 128/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 129/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 130/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 131/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 132/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 133/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 134/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 135/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 136/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 137/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 138/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 139/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 140/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 141/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 142/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 143/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 144/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 145/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 146/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 147/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 148/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 149/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 150/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 151/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 152/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 153/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 154/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 155/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 156/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 157/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 158/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 159/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 160/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 161/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 162/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 163/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 164/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 165/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 166/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 167/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 168/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 169/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 170/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 171/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 172/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 173/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 174/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 175/175
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.005914497654885054
  1/332 [..............................] - ETA: 49s 39/332 [==>...........................] - ETA: 0s  77/332 [=====>........................] - ETA: 0s116/332 [=========>....................] - ETA: 0s155/332 [=============>................] - ETA: 0s194/332 [================>.............] - ETA: 0s233/332 [====================>.........] - ETA: 0s272/332 [=======================>......] - ETA: 0s311/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.11813392575803541
cosine 0.09237271862415561
MAE: 0.045293342
RMSE: 0.09786013
r2: 0.3787425149885794
RMSE zero-vector: 0.23411466903540806
['1.4custom_VAE', 'logcosh', 128, 175, 0.0018, 0.6, 758, 0.005974857602268457, 0.005914497654885054, 0.11813392575803541, 0.09237271862415561, 0.04529334232211113, 0.09786012768745422, 0.3787425149885794, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.6 175 0.002 64 2] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3286)         4156790     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3286)        13144       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3286)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7240340     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,393,366
Trainable params: 16,378,706
Non-trainable params: 14,660
__________________________________________________________________________________________________
Epoch 1/175
1493/1493 - 10s - loss: 0.0181 - val_loss: 0.0102 - 10s/epoch - 6ms/step
Epoch 2/175
1493/1493 - 7s - loss: 0.0093 - val_loss: 0.0077 - 7s/epoch - 5ms/step
Epoch 3/175
1493/1493 - 7s - loss: 0.0074 - val_loss: 0.0071 - 7s/epoch - 5ms/step
Epoch 4/175
1493/1493 - 7s - loss: 0.0069 - val_loss: 0.0068 - 7s/epoch - 5ms/step
Epoch 5/175
1493/1493 - 7s - loss: 0.0068 - val_loss: 0.0067 - 7s/epoch - 5ms/step
Epoch 6/175
1493/1493 - 7s - loss: 0.0068 - val_loss: 0.0067 - 7s/epoch - 4ms/step
Epoch 7/175
1493/1493 - 7s - loss: 0.0067 - val_loss: 0.0067 - 7s/epoch - 5ms/step
Epoch 8/175
1493/1493 - 7s - loss: 0.0067 - val_loss: 0.0067 - 7s/epoch - 5ms/step
Epoch 9/175
1493/1493 - 7s - loss: 0.0068 - val_loss: 0.0067 - 7s/epoch - 5ms/step
Epoch 10/175
1493/1493 - 7s - loss: 0.0067 - val_loss: 0.0067 - 7s/epoch - 5ms/step
Epoch 11/175
1493/1493 - 7s - loss: 0.0067 - val_loss: 0.0066 - 7s/epoch - 4ms/step
Epoch 12/175
1493/1493 - 7s - loss: 0.0067 - val_loss: 0.0067 - 7s/epoch - 5ms/step
Epoch 13/175
1493/1493 - 7s - loss: 0.0067 - val_loss: 0.0067 - 7s/epoch - 5ms/step
Epoch 14/175
1493/1493 - 7s - loss: 0.0067 - val_loss: 0.0066 - 7s/epoch - 5ms/step
Epoch 15/175
1493/1493 - 7s - loss: 0.0067 - val_loss: 0.0066 - 7s/epoch - 4ms/step
Epoch 16/175
1493/1493 - 7s - loss: 0.0065 - val_loss: 0.0064 - 7s/epoch - 5ms/step
Epoch 17/175
1493/1493 - 7s - loss: 0.0064 - val_loss: 0.0063 - 7s/epoch - 5ms/step
Epoch 18/175
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0064 - 7s/epoch - 5ms/step
Epoch 19/175
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 20/175
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 21/175
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 22/175
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 23/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 24/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 25/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 26/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 27/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 28/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 29/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 30/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 31/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 4ms/step
Epoch 32/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 33/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0063 - 7s/epoch - 5ms/step
Epoch 34/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 35/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 36/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 37/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0063 - 7s/epoch - 5ms/step
Epoch 38/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0063 - 7s/epoch - 4ms/step
Epoch 39/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 40/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 41/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0063 - 7s/epoch - 5ms/step
Epoch 42/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 43/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 44/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 45/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 46/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 47/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 4ms/step
Epoch 48/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 49/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 50/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 51/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 52/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 53/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 54/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 55/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 56/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 4ms/step
Epoch 57/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 58/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 59/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 60/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 61/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 62/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 4ms/step
Epoch 63/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 64/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 65/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 66/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 67/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 68/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 69/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 70/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 71/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 72/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 73/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0063 - 7s/epoch - 5ms/step
Epoch 74/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 75/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 76/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 77/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 4ms/step
Epoch 78/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 79/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 80/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 81/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 82/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 83/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 84/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 85/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 86/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 87/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 88/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 89/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0063 - 7s/epoch - 5ms/step
Epoch 90/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 91/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 92/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 93/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 94/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 4ms/step
Epoch 95/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 96/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 97/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 98/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 99/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 100/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 101/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 4ms/step
Epoch 102/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 103/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 104/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 105/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 106/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 107/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 108/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 109/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 110/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 111/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 112/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 113/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 114/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 115/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 116/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 117/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 118/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 119/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 120/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 121/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 122/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 123/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 124/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 125/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 126/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 127/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 128/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 129/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 130/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 131/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 132/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 133/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 134/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 135/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 136/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 137/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 138/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 139/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 140/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 141/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 142/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 143/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 144/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 145/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 146/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 147/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 148/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 149/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 150/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 151/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 152/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 153/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 154/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 155/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 156/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 157/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 158/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 159/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 160/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 161/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 162/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 163/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 164/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 165/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 166/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 167/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 168/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 169/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 170/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 171/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 172/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 173/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 174/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 175/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.00617789663374424
  1/332 [..............................] - ETA: 1:21 23/332 [=>............................] - ETA: 0s   45/332 [===>..........................] - ETA: 0s 68/332 [=====>........................] - ETA: 0s 90/332 [=======>......................] - ETA: 0s113/332 [=========>....................] - ETA: 0s134/332 [===========>..................] - ETA: 0s156/332 [=============>................] - ETA: 0s179/332 [===============>..............] - ETA: 0s202/332 [=================>............] - ETA: 0s224/332 [===================>..........] - ETA: 0s247/332 [=====================>........] - ETA: 0s270/332 [=======================>......] - ETA: 0s293/332 [=========================>....] - ETA: 0s316/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.12723950738957185
cosine 0.09957497967129782
MAE: 0.04765106
RMSE: 0.10283126
r2: 0.3140215178374304
RMSE zero-vector: 0.23411466903540806
['2.6custom_VAE', 'logcosh', 64, 175, 0.002, 0.6, 758, 0.006124721374362707, 0.00617789663374424, 0.12723950738957185, 0.09957497967129782, 0.04765105992555618, 0.10283125936985016, 0.3140215178374304, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.8000000000000003 180 0.0018 256 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3539)         4476835     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3539)        14156       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3539)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2683320     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2683320     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7753171     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 17,610,802
Trainable params: 17,595,130
Non-trainable params: 15,672
__________________________________________________________________________________________________
Epoch 1/180
374/374 - 7s - loss: 0.0548 - val_loss: 0.0736 - 7s/epoch - 20ms/step
Epoch 2/180
374/374 - 2s - loss: 0.0197 - val_loss: 0.1257 - 2s/epoch - 6ms/step
Epoch 3/180
374/374 - 2s - loss: 0.0191 - val_loss: 0.0271 - 2s/epoch - 6ms/step
Epoch 4/180
374/374 - 2s - loss: 0.0170 - val_loss: 0.0200 - 2s/epoch - 6ms/step
Epoch 5/180
374/374 - 2s - loss: 0.0145 - val_loss: 0.0615 - 2s/epoch - 6ms/step
Epoch 6/180
374/374 - 2s - loss: 0.0157 - val_loss: 0.0151 - 2s/epoch - 6ms/step
Epoch 7/180
374/374 - 2s - loss: 0.0137 - val_loss: 0.0176 - 2s/epoch - 6ms/step
Epoch 8/180
374/374 - 2s - loss: 0.0137 - val_loss: 0.2245 - 2s/epoch - 6ms/step
Epoch 9/180
374/374 - 2s - loss: 0.0210 - val_loss: 0.0135 - 2s/epoch - 6ms/step
Epoch 10/180
374/374 - 2s - loss: 0.0131 - val_loss: 0.0206 - 2s/epoch - 6ms/step
Epoch 11/180
374/374 - 2s - loss: 0.0137 - val_loss: 0.0137 - 2s/epoch - 6ms/step
Epoch 12/180
374/374 - 2s - loss: 0.0132 - val_loss: 0.0125 - 2s/epoch - 6ms/step
Epoch 13/180
374/374 - 2s - loss: 0.0126 - val_loss: 0.0124 - 2s/epoch - 6ms/step
Epoch 14/180
374/374 - 2s - loss: 0.0124 - val_loss: 0.0137 - 2s/epoch - 6ms/step
Epoch 15/180
374/374 - 2s - loss: 0.0157 - val_loss: 0.0145 - 2s/epoch - 6ms/step
Epoch 16/180
374/374 - 2s - loss: 0.0155 - val_loss: 0.0143 - 2s/epoch - 6ms/step
Epoch 17/180
374/374 - 2s - loss: 0.0151 - val_loss: 0.0135 - 2s/epoch - 6ms/step
Epoch 18/180
374/374 - 2s - loss: 0.0159 - val_loss: 0.0166 - 2s/epoch - 6ms/step
Epoch 19/180
374/374 - 2s - loss: 0.0484 - val_loss: 0.0202 - 2s/epoch - 6ms/step
Epoch 20/180
374/374 - 2s - loss: 0.0408 - val_loss: 0.0247 - 2s/epoch - 6ms/step
Epoch 21/180
374/374 - 2s - loss: 0.0213 - val_loss: 0.0167 - 2s/epoch - 6ms/step
Epoch 22/180
374/374 - 2s - loss: 0.0176 - val_loss: 0.0172 - 2s/epoch - 6ms/step
Epoch 23/180
374/374 - 2s - loss: 0.0450 - val_loss: 0.0203 - 2s/epoch - 6ms/step
Epoch 24/180
374/374 - 2s - loss: 0.0190 - val_loss: 0.0170 - 2s/epoch - 6ms/step
Epoch 25/180
374/374 - 2s - loss: 0.0189 - val_loss: 0.0166 - 2s/epoch - 6ms/step
Epoch 26/180
374/374 - 2s - loss: 0.0202 - val_loss: 0.0191 - 2s/epoch - 6ms/step
Epoch 27/180
374/374 - 2s - loss: 0.0411 - val_loss: 0.0174 - 2s/epoch - 6ms/step
Epoch 28/180
374/374 - 2s - loss: 0.0173 - val_loss: 0.0201 - 2s/epoch - 6ms/step
Epoch 29/180
374/374 - 2s - loss: 0.0347 - val_loss: 0.0175 - 2s/epoch - 6ms/step
Epoch 30/180
374/374 - 2s - loss: 0.0206 - val_loss: 0.0163 - 2s/epoch - 6ms/step
Epoch 31/180
374/374 - 2s - loss: 0.0164 - val_loss: 0.0161 - 2s/epoch - 6ms/step
Epoch 32/180
374/374 - 2s - loss: 0.0163 - val_loss: 0.0151 - 2s/epoch - 6ms/step
Epoch 33/180
374/374 - 2s - loss: 0.0154 - val_loss: 0.0147 - 2s/epoch - 6ms/step
Epoch 34/180
374/374 - 2s - loss: 0.0149 - val_loss: 0.0144 - 2s/epoch - 6ms/step
Epoch 35/180
374/374 - 2s - loss: 0.0146 - val_loss: 0.0145 - 2s/epoch - 6ms/step
Epoch 36/180
374/374 - 2s - loss: 0.0150 - val_loss: 0.0140 - 2s/epoch - 6ms/step
Epoch 37/180
374/374 - 2s - loss: 0.0144 - val_loss: 0.0139 - 2s/epoch - 6ms/step
Epoch 38/180
374/374 - 2s - loss: 0.0141 - val_loss: 0.0136 - 2s/epoch - 6ms/step
Epoch 39/180
374/374 - 2s - loss: 0.0137 - val_loss: 0.0142 - 2s/epoch - 6ms/step
Epoch 40/180
374/374 - 2s - loss: 0.0158 - val_loss: 0.0135 - 2s/epoch - 6ms/step
Epoch 41/180
374/374 - 2s - loss: 0.0137 - val_loss: 0.0145 - 2s/epoch - 6ms/step
Epoch 42/180
374/374 - 2s - loss: 0.0148 - val_loss: 0.0132 - 2s/epoch - 6ms/step
Epoch 43/180
374/374 - 2s - loss: 0.0134 - val_loss: 0.0130 - 2s/epoch - 6ms/step
Epoch 44/180
374/374 - 2s - loss: 0.0132 - val_loss: 0.0129 - 2s/epoch - 6ms/step
Epoch 45/180
374/374 - 2s - loss: 0.0130 - val_loss: 0.0127 - 2s/epoch - 6ms/step
Epoch 46/180
374/374 - 2s - loss: 0.0128 - val_loss: 0.0212 - 2s/epoch - 6ms/step
Epoch 47/180
374/374 - 2s - loss: 0.0193 - val_loss: 0.0153 - 2s/epoch - 6ms/step
Epoch 48/180
374/374 - 2s - loss: 0.0139 - val_loss: 0.0125 - 2s/epoch - 6ms/step
Epoch 49/180
374/374 - 2s - loss: 0.0125 - val_loss: 0.0123 - 2s/epoch - 6ms/step
Epoch 50/180
374/374 - 2s - loss: 0.0123 - val_loss: 0.0120 - 2s/epoch - 6ms/step
Epoch 51/180
374/374 - 2s - loss: 0.0121 - val_loss: 0.0119 - 2s/epoch - 6ms/step
Epoch 52/180
374/374 - 2s - loss: 0.0120 - val_loss: 0.0118 - 2s/epoch - 6ms/step
Epoch 53/180
374/374 - 2s - loss: 0.0119 - val_loss: 0.0117 - 2s/epoch - 6ms/step
Epoch 54/180
374/374 - 2s - loss: 0.0118 - val_loss: 0.0117 - 2s/epoch - 6ms/step
Epoch 55/180
374/374 - 2s - loss: 0.0117 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 56/180
374/374 - 2s - loss: 0.0116 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 57/180
374/374 - 2s - loss: 0.0118 - val_loss: 0.0193 - 2s/epoch - 6ms/step
Epoch 58/180
374/374 - 2s - loss: 0.0219 - val_loss: 0.0251 - 2s/epoch - 6ms/step
Epoch 59/180
374/374 - 2s - loss: 0.0133 - val_loss: 0.0160 - 2s/epoch - 6ms/step
Epoch 60/180
374/374 - 2s - loss: 0.0171 - val_loss: 0.0123 - 2s/epoch - 6ms/step
Epoch 61/180
374/374 - 2s - loss: 0.0121 - val_loss: 0.0119 - 2s/epoch - 6ms/step
Epoch 62/180
374/374 - 2s - loss: 0.0119 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 63/180
374/374 - 2s - loss: 0.0117 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 64/180
374/374 - 2s - loss: 0.0116 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 65/180
374/374 - 2s - loss: 0.0116 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 66/180
374/374 - 2s - loss: 0.0117 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 67/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 68/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0133 - 2s/epoch - 6ms/step
Epoch 69/180
374/374 - 2s - loss: 0.0133 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 70/180
374/374 - 2s - loss: 0.0666 - val_loss: 0.0175 - 2s/epoch - 6ms/step
Epoch 71/180
374/374 - 2s - loss: 0.0333 - val_loss: 0.0152 - 2s/epoch - 6ms/step
Epoch 72/180
374/374 - 2s - loss: 0.0909 - val_loss: 0.0200 - 2s/epoch - 6ms/step
Epoch 73/180
374/374 - 2s - loss: 0.0646 - val_loss: 0.0188 - 2s/epoch - 6ms/step
Epoch 74/180
374/374 - 2s - loss: 0.0160 - val_loss: 0.0147 - 2s/epoch - 6ms/step
Epoch 75/180
374/374 - 2s - loss: 0.0146 - val_loss: 0.0137 - 2s/epoch - 6ms/step
Epoch 76/180
374/374 - 2s - loss: 0.0140 - val_loss: 0.0136 - 2s/epoch - 6ms/step
Epoch 77/180
374/374 - 2s - loss: 0.0138 - val_loss: 0.0131 - 2s/epoch - 6ms/step
Epoch 78/180
374/374 - 2s - loss: 0.0133 - val_loss: 0.0132 - 2s/epoch - 6ms/step
Epoch 79/180
374/374 - 2s - loss: 0.0131 - val_loss: 0.0128 - 2s/epoch - 6ms/step
Epoch 80/180
374/374 - 2s - loss: 0.0134 - val_loss: 0.0181 - 2s/epoch - 6ms/step
Epoch 81/180
374/374 - 2s - loss: 0.0266 - val_loss: 0.0146 - 2s/epoch - 6ms/step
Epoch 82/180
374/374 - 2s - loss: 0.0140 - val_loss: 0.0133 - 2s/epoch - 6ms/step
Epoch 83/180
374/374 - 2s - loss: 0.0134 - val_loss: 0.0130 - 2s/epoch - 6ms/step
Epoch 84/180
374/374 - 2s - loss: 0.0131 - val_loss: 0.0128 - 2s/epoch - 6ms/step
Epoch 85/180
374/374 - 2s - loss: 0.0133 - val_loss: 0.0289 - 2s/epoch - 6ms/step
Epoch 86/180
374/374 - 2s - loss: 0.0446 - val_loss: 0.0153 - 2s/epoch - 6ms/step
Epoch 87/180
374/374 - 2s - loss: 0.0148 - val_loss: 0.0139 - 2s/epoch - 6ms/step
Epoch 88/180
374/374 - 2s - loss: 0.0139 - val_loss: 0.0134 - 2s/epoch - 6ms/step
Epoch 89/180
374/374 - 2s - loss: 0.0134 - val_loss: 0.0130 - 2s/epoch - 6ms/step
Epoch 90/180
374/374 - 2s - loss: 0.0131 - val_loss: 0.0127 - 2s/epoch - 6ms/step
Epoch 91/180
374/374 - 2s - loss: 0.0129 - val_loss: 0.0125 - 2s/epoch - 6ms/step
Epoch 92/180
374/374 - 2s - loss: 0.0127 - val_loss: 0.0124 - 2s/epoch - 6ms/step
Epoch 93/180
374/374 - 2s - loss: 0.0129 - val_loss: 0.0330 - 2s/epoch - 6ms/step
Epoch 94/180
374/374 - 2s - loss: 0.0664 - val_loss: 0.0167 - 2s/epoch - 6ms/step
Epoch 95/180
374/374 - 2s - loss: 0.0151 - val_loss: 0.0146 - 2s/epoch - 6ms/step
Epoch 96/180
374/374 - 2s - loss: 0.0140 - val_loss: 0.0135 - 2s/epoch - 6ms/step
Epoch 97/180
374/374 - 2s - loss: 0.0134 - val_loss: 0.0128 - 2s/epoch - 6ms/step
Epoch 98/180
374/374 - 2s - loss: 0.0130 - val_loss: 0.0127 - 2s/epoch - 6ms/step
Epoch 99/180
374/374 - 2s - loss: 0.0127 - val_loss: 0.0126 - 2s/epoch - 6ms/step
Epoch 100/180
374/374 - 2s - loss: 0.0127 - val_loss: 0.0123 - 2s/epoch - 6ms/step
Epoch 101/180
374/374 - 2s - loss: 0.0123 - val_loss: 0.0121 - 2s/epoch - 6ms/step
Epoch 102/180
374/374 - 2s - loss: 0.0122 - val_loss: 0.0121 - 2s/epoch - 6ms/step
Epoch 103/180
374/374 - 2s - loss: 0.0121 - val_loss: 0.0121 - 2s/epoch - 6ms/step
Epoch 104/180
374/374 - 2s - loss: 0.0119 - val_loss: 0.0117 - 2s/epoch - 6ms/step
Epoch 105/180
374/374 - 2s - loss: 0.0118 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 106/180
374/374 - 2s - loss: 0.0117 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 107/180
374/374 - 2s - loss: 0.0117 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 108/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 109/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 110/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 111/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0117 - 2s/epoch - 6ms/step
Epoch 112/180
374/374 - 2s - loss: 0.0113 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 113/180
374/374 - 2s - loss: 0.0112 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 114/180
374/374 - 2s - loss: 0.0113 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 115/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 116/180
374/374 - 2s - loss: 0.0111 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 117/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 118/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 119/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 120/180
374/374 - 2s - loss: 0.0109 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 121/180
374/374 - 2s - loss: 0.0109 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 122/180
374/374 - 2s - loss: 0.0109 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 123/180
374/374 - 2s - loss: 0.0108 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 124/180
374/374 - 2s - loss: 0.0108 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 125/180
374/374 - 2s - loss: 0.0108 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 126/180
374/374 - 2s - loss: 0.0108 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 127/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0130 - 2s/epoch - 6ms/step
Epoch 128/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0121 - 2s/epoch - 6ms/step
Epoch 129/180
374/374 - 2s - loss: 0.0109 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 130/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 131/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 132/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 133/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 134/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0120 - 2s/epoch - 6ms/step
Epoch 135/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 136/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 137/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 138/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 139/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 140/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 141/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 142/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0130 - 2s/epoch - 6ms/step
Epoch 143/180
374/374 - 2s - loss: 0.0133 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 144/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 145/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 146/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 147/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 148/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 149/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 150/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0137 - 2s/epoch - 6ms/step
Epoch 151/180
374/374 - 2s - loss: 0.0133 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 152/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0129 - 2s/epoch - 6ms/step
Epoch 153/180
374/374 - 2s - loss: 0.0158 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 154/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 155/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 156/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 157/180
374/374 - 2s - loss: 0.0122 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 158/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 159/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0160 - 2s/epoch - 6ms/step
Epoch 160/180
374/374 - 2s - loss: 0.0163 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 161/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 162/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 163/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 164/180
374/374 - 2s - loss: 0.0116 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 165/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 166/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 167/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 168/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 169/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 170/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 171/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 172/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 173/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0117 - 2s/epoch - 6ms/step
Epoch 174/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 175/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 176/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 177/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 178/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 179/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 180/180
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.010088670998811722
  1/332 [..............................] - ETA: 1:15 23/332 [=>............................] - ETA: 0s   45/332 [===>..........................] - ETA: 0s 68/332 [=====>........................] - ETA: 0s 90/332 [=======>......................] - ETA: 0s112/332 [=========>....................] - ETA: 0s134/332 [===========>..................] - ETA: 0s156/332 [=============>................] - ETA: 0s179/332 [===============>..............] - ETA: 0s201/332 [=================>............] - ETA: 0s223/332 [===================>..........] - ETA: 0s246/332 [=====================>........] - ETA: 0s268/332 [=======================>......] - ETA: 0s290/332 [=========================>....] - ETA: 0s313/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.09088795895829005
cosine 0.07129185425754407
MAE: 0.04000629
RMSE: 0.086322084
r2: 0.5166031013019811
RMSE zero-vector: 0.23411466903540806
['2.8000000000000003custom_VAE', 'mse', 256, 180, 0.0018, 0.6, 758, 0.01022637914866209, 0.010088670998811722, 0.09088795895829005, 0.07129185425754407, 0.04000629112124443, 0.08632208406925201, 0.5166031013019811, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Saved GA instance to file: ./tmp//ga_instance_generation_1.pkl
[2.5 180 0.002 128 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2396038     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2396038     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         6984938     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 15,787,054
Trainable params: 15,772,898
Non-trainable params: 14,156
__________________________________________________________________________________________________
Epoch 1/180
747/747 - 7s - loss: 0.0377 - val_loss: 0.0280 - 7s/epoch - 9ms/step
Epoch 2/180
747/747 - 4s - loss: 0.0170 - val_loss: 0.0178 - 4s/epoch - 5ms/step
Epoch 3/180
747/747 - 4s - loss: 0.0150 - val_loss: 0.0161 - 4s/epoch - 5ms/step
Epoch 4/180
747/747 - 4s - loss: 0.0142 - val_loss: 0.0606 - 4s/epoch - 5ms/step
Epoch 5/180
747/747 - 4s - loss: 0.0164 - val_loss: 0.0130 - 4s/epoch - 5ms/step
Epoch 6/180
747/747 - 4s - loss: 0.0129 - val_loss: 0.0129 - 4s/epoch - 5ms/step
Epoch 7/180
747/747 - 4s - loss: 0.0127 - val_loss: 0.0125 - 4s/epoch - 5ms/step
Epoch 8/180
747/747 - 4s - loss: 0.0125 - val_loss: 0.0123 - 4s/epoch - 5ms/step
Epoch 9/180
747/747 - 4s - loss: 0.0127 - val_loss: 0.0131 - 4s/epoch - 5ms/step
Epoch 10/180
747/747 - 4s - loss: 0.0136 - val_loss: 0.0129 - 4s/epoch - 5ms/step
Epoch 11/180
747/747 - 4s - loss: 0.0134 - val_loss: 0.0122 - 4s/epoch - 5ms/step
Epoch 12/180
747/747 - 4s - loss: 0.0123 - val_loss: 0.0121 - 4s/epoch - 5ms/step
Epoch 13/180
747/747 - 4s - loss: 0.0122 - val_loss: 0.0120 - 4s/epoch - 5ms/step
Epoch 14/180
747/747 - 4s - loss: 0.0121 - val_loss: 0.0118 - 4s/epoch - 5ms/step
Epoch 15/180
747/747 - 4s - loss: 0.0119 - val_loss: 0.0121 - 4s/epoch - 5ms/step
Epoch 16/180
747/747 - 4s - loss: 0.0118 - val_loss: 0.0114 - 4s/epoch - 5ms/step
Epoch 17/180
747/747 - 4s - loss: 0.0116 - val_loss: 0.0126 - 4s/epoch - 5ms/step
Epoch 18/180
747/747 - 4s - loss: 0.0132 - val_loss: 0.0115 - 4s/epoch - 5ms/step
Epoch 19/180
747/747 - 4s - loss: 0.0121 - val_loss: 0.0127 - 4s/epoch - 5ms/step
Epoch 20/180
747/747 - 4s - loss: 0.0146 - val_loss: 0.0185 - 4s/epoch - 5ms/step
Epoch 21/180
747/747 - 4s - loss: 0.0119 - val_loss: 0.0115 - 4s/epoch - 5ms/step
Epoch 22/180
747/747 - 4s - loss: 0.0221 - val_loss: 0.0131 - 4s/epoch - 5ms/step
Epoch 23/180
747/747 - 4s - loss: 0.0124 - val_loss: 0.0153 - 4s/epoch - 5ms/step
Epoch 24/180
747/747 - 4s - loss: 0.0120 - val_loss: 0.0159 - 4s/epoch - 5ms/step
Epoch 25/180
747/747 - 4s - loss: 0.0167 - val_loss: 0.0124 - 4s/epoch - 5ms/step
Epoch 26/180
747/747 - 4s - loss: 0.0119 - val_loss: 0.0124 - 4s/epoch - 5ms/step
Epoch 27/180
747/747 - 4s - loss: 0.0122 - val_loss: 0.0116 - 4s/epoch - 5ms/step
Epoch 28/180
747/747 - 4s - loss: 0.0130 - val_loss: 0.0116 - 4s/epoch - 5ms/step
Epoch 29/180
747/747 - 4s - loss: 0.0116 - val_loss: 0.0114 - 4s/epoch - 5ms/step
Epoch 30/180
747/747 - 4s - loss: 0.0116 - val_loss: 0.0113 - 4s/epoch - 5ms/step
Epoch 31/180
747/747 - 4s - loss: 0.0115 - val_loss: 0.0174 - 4s/epoch - 5ms/step
Epoch 32/180
747/747 - 4s - loss: 0.0140 - val_loss: 0.0121 - 4s/epoch - 5ms/step
Epoch 33/180
747/747 - 4s - loss: 0.0119 - val_loss: 0.0112 - 4s/epoch - 5ms/step
Epoch 34/180
747/747 - 4s - loss: 0.0113 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 35/180
747/747 - 4s - loss: 0.0113 - val_loss: 0.0176 - 4s/epoch - 5ms/step
Epoch 36/180
747/747 - 4s - loss: 0.0120 - val_loss: 0.0167 - 4s/epoch - 5ms/step
Epoch 37/180
747/747 - 4s - loss: 0.0150 - val_loss: 0.0112 - 4s/epoch - 5ms/step
Epoch 38/180
747/747 - 4s - loss: 0.0114 - val_loss: 0.0139 - 4s/epoch - 5ms/step
Epoch 39/180
747/747 - 4s - loss: 0.0135 - val_loss: 0.0114 - 4s/epoch - 5ms/step
Epoch 40/180
747/747 - 4s - loss: 0.0116 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 41/180
747/747 - 4s - loss: 0.0112 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 42/180
747/747 - 4s - loss: 0.0112 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 43/180
747/747 - 4s - loss: 0.0111 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 44/180
747/747 - 4s - loss: 0.0111 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 45/180
747/747 - 4s - loss: 0.0111 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 46/180
747/747 - 4s - loss: 0.0110 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 47/180
747/747 - 4s - loss: 0.0110 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 48/180
747/747 - 4s - loss: 0.0110 - val_loss: 0.0115 - 4s/epoch - 5ms/step
Epoch 49/180
747/747 - 4s - loss: 0.0110 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 50/180
747/747 - 4s - loss: 0.0109 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 51/180
747/747 - 4s - loss: 0.0109 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 52/180
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 53/180
747/747 - 4s - loss: 0.0109 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 54/180
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 55/180
747/747 - 4s - loss: 0.0108 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 56/180
747/747 - 4s - loss: 0.0108 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 57/180
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 58/180
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 59/180
747/747 - 4s - loss: 0.0108 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 60/180
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 61/180
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 62/180
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 63/180
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 64/180
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 65/180
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 66/180
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 67/180
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 68/180
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 69/180
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 70/180
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 71/180
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 72/180
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 73/180
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 74/180
747/747 - 4s - loss: 0.0106 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 75/180
747/747 - 4s - loss: 0.0106 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 76/180
747/747 - 4s - loss: 0.0106 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 77/180
747/747 - 4s - loss: 0.0105 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 78/180
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 79/180
747/747 - 4s - loss: 0.0104 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 80/180
747/747 - 4s - loss: 0.0104 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 81/180
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 82/180
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 83/180
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 84/180
747/747 - 4s - loss: 0.0102 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 85/180
747/747 - 4s - loss: 0.0102 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 86/180
747/747 - 4s - loss: 0.0102 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 87/180
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 88/180
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 89/180
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 90/180
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 91/180
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 92/180
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 93/180
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 94/180
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 95/180
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 96/180
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 97/180
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 98/180
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 99/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 100/180
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 101/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 102/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 103/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 104/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 105/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 106/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 107/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 108/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 109/180
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 110/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 111/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 112/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 113/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 114/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 115/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 116/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 117/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 118/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 119/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 120/180
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 121/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 122/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 123/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 124/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 125/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 126/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 127/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 128/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 129/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 130/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 131/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 132/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 133/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 134/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 135/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 136/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 137/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 138/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 139/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 140/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 141/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 142/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 143/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 144/180
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 145/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 146/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 147/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 148/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 149/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 150/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 151/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 152/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 153/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 154/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 155/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 156/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 157/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 158/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 159/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 160/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 161/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 162/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 163/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 164/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 165/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 166/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 167/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 168/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 169/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 170/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 171/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 172/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 173/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 174/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 175/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 176/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 177/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 178/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 179/180
747/747 - 4s - loss: 0.0095 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 180/180
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009376135654747486
  1/332 [..............................] - ETA: 1:19 23/332 [=>............................] - ETA: 0s   45/332 [===>..........................] - ETA: 0s 67/332 [=====>........................] - ETA: 0s 89/332 [=======>......................] - ETA: 0s111/332 [=========>....................] - ETA: 0s133/332 [===========>..................] - ETA: 0s155/332 [=============>................] - ETA: 0s177/332 [==============>...............] - ETA: 0s199/332 [================>.............] - ETA: 0s221/332 [==================>...........] - ETA: 0s243/332 [====================>.........] - ETA: 0s259/332 [======================>.......] - ETA: 0s281/332 [========================>.....] - ETA: 0s303/332 [==========================>...] - ETA: 0s325/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.07483636954349254
cosine 0.058743692257698196
MAE: 0.036127497
RMSE: 0.0786473
r2: 0.5987382269698257
RMSE zero-vector: 0.23411466903540806
['2.5custom_VAE', 'mse', 128, 180, 0.002, 0.6, 758, 0.009554644115269184, 0.009376135654747486, 0.07483636954349254, 0.058743692257698196, 0.036127496510744095, 0.07864730060100555, 0.5987382269698257, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.7 170 0.002 256 2] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3412)         4316180     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3412)        13648       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3412)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2587054     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2587054     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7495742     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,999,678
Trainable params: 16,984,514
Non-trainable params: 15,164
__________________________________________________________________________________________________
Epoch 1/170
374/374 - 5s - loss: 0.0336 - val_loss: 0.0173 - 5s/epoch - 13ms/step
Epoch 2/170
374/374 - 2s - loss: 0.0092 - val_loss: 0.0769 - 2s/epoch - 6ms/step
Epoch 3/170
374/374 - 2s - loss: 0.0127 - val_loss: 0.0128 - 2s/epoch - 6ms/step
Epoch 4/170
374/374 - 2s - loss: 0.0082 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 5/170
374/374 - 2s - loss: 0.0082 - val_loss: 0.0114 - 2s/epoch - 5ms/step
Epoch 6/170
374/374 - 2s - loss: 0.0082 - val_loss: 0.0296 - 2s/epoch - 6ms/step
Epoch 7/170
374/374 - 2s - loss: 0.0083 - val_loss: 0.0092 - 2s/epoch - 6ms/step
Epoch 8/170
374/374 - 2s - loss: 0.0077 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 9/170
374/374 - 2s - loss: 0.0075 - val_loss: 0.0081 - 2s/epoch - 6ms/step
Epoch 10/170
374/374 - 2s - loss: 0.0073 - val_loss: 0.0087 - 2s/epoch - 6ms/step
Epoch 11/170
374/374 - 2s - loss: 0.0073 - val_loss: 0.0088 - 2s/epoch - 6ms/step
Epoch 12/170
374/374 - 2s - loss: 0.0071 - val_loss: 0.0073 - 2s/epoch - 6ms/step
Epoch 13/170
374/374 - 2s - loss: 0.0070 - val_loss: 0.0075 - 2s/epoch - 6ms/step
Epoch 14/170
374/374 - 2s - loss: 0.0069 - val_loss: 0.0071 - 2s/epoch - 6ms/step
Epoch 15/170
374/374 - 2s - loss: 0.0068 - val_loss: 0.0069 - 2s/epoch - 6ms/step
Epoch 16/170
374/374 - 2s - loss: 0.0066 - val_loss: 0.0130 - 2s/epoch - 6ms/step
Epoch 17/170
374/374 - 2s - loss: 0.0082 - val_loss: 0.0069 - 2s/epoch - 6ms/step
Epoch 18/170
374/374 - 2s - loss: 0.0065 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 19/170
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 20/170
374/374 - 2s - loss: 0.0069 - val_loss: 0.0092 - 2s/epoch - 6ms/step
Epoch 21/170
374/374 - 2s - loss: 0.0115 - val_loss: 0.0818 - 2s/epoch - 6ms/step
Epoch 22/170
374/374 - 2s - loss: 0.0141 - val_loss: 0.0076 - 2s/epoch - 6ms/step
Epoch 23/170
374/374 - 2s - loss: 0.0072 - val_loss: 0.0069 - 2s/epoch - 6ms/step
Epoch 24/170
374/374 - 2s - loss: 0.0070 - val_loss: 0.0068 - 2s/epoch - 6ms/step
Epoch 25/170
374/374 - 2s - loss: 0.0067 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 26/170
374/374 - 2s - loss: 0.0066 - val_loss: 0.0071 - 2s/epoch - 6ms/step
Epoch 27/170
374/374 - 2s - loss: 0.0080 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 28/170
374/374 - 2s - loss: 0.0173 - val_loss: 0.0081 - 2s/epoch - 6ms/step
Epoch 29/170
374/374 - 2s - loss: 0.0081 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 30/170
374/374 - 2s - loss: 0.0150 - val_loss: 0.0076 - 2s/epoch - 6ms/step
Epoch 31/170
374/374 - 2s - loss: 0.0120 - val_loss: 0.0254 - 2s/epoch - 6ms/step
Epoch 32/170
374/374 - 2s - loss: 0.0731 - val_loss: 0.0128 - 2s/epoch - 6ms/step
Epoch 33/170
374/374 - 2s - loss: 0.0095 - val_loss: 0.0085 - 2s/epoch - 6ms/step
Epoch 34/170
374/374 - 2s - loss: 0.0085 - val_loss: 0.0092 - 2s/epoch - 6ms/step
Epoch 35/170
374/374 - 2s - loss: 0.0087 - val_loss: 0.0077 - 2s/epoch - 6ms/step
Epoch 36/170
374/374 - 2s - loss: 0.0077 - val_loss: 0.0075 - 2s/epoch - 6ms/step
Epoch 37/170
374/374 - 2s - loss: 0.0075 - val_loss: 0.0079 - 2s/epoch - 6ms/step
Epoch 38/170
374/374 - 2s - loss: 0.0074 - val_loss: 0.0077 - 2s/epoch - 6ms/step
Epoch 39/170
374/374 - 2s - loss: 0.0072 - val_loss: 0.0074 - 2s/epoch - 6ms/step
Epoch 40/170
374/374 - 2s - loss: 0.0073 - val_loss: 0.0076 - 2s/epoch - 6ms/step
Epoch 41/170
374/374 - 2s - loss: 0.0071 - val_loss: 0.0078 - 2s/epoch - 6ms/step
Epoch 42/170
374/374 - 2s - loss: 0.0072 - val_loss: 0.0087 - 2s/epoch - 6ms/step
Epoch 43/170
374/374 - 2s - loss: 0.0071 - val_loss: 0.0069 - 2s/epoch - 6ms/step
Epoch 44/170
374/374 - 2s - loss: 0.0069 - val_loss: 0.0074 - 2s/epoch - 6ms/step
Epoch 45/170
374/374 - 2s - loss: 0.0069 - val_loss: 0.0127 - 2s/epoch - 6ms/step
Epoch 46/170
374/374 - 2s - loss: 0.0083 - val_loss: 0.0068 - 2s/epoch - 6ms/step
Epoch 47/170
374/374 - 2s - loss: 0.0068 - val_loss: 0.0069 - 2s/epoch - 6ms/step
Epoch 48/170
374/374 - 2s - loss: 0.0068 - val_loss: 0.0071 - 2s/epoch - 6ms/step
Epoch 49/170
374/374 - 2s - loss: 0.0068 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 50/170
374/374 - 2s - loss: 0.0066 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 51/170
374/374 - 2s - loss: 0.0066 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 52/170
374/374 - 2s - loss: 0.0071 - val_loss: 0.0075 - 2s/epoch - 6ms/step
Epoch 53/170
374/374 - 2s - loss: 0.0068 - val_loss: 0.0222 - 2s/epoch - 6ms/step
Epoch 54/170
374/374 - 2s - loss: 0.0144 - val_loss: 0.0069 - 2s/epoch - 6ms/step
Epoch 55/170
374/374 - 2s - loss: 0.0068 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 56/170
374/374 - 2s - loss: 0.0066 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 57/170
374/374 - 2s - loss: 0.0066 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 58/170
374/374 - 2s - loss: 0.0065 - val_loss: 0.0141 - 2s/epoch - 6ms/step
Epoch 59/170
374/374 - 2s - loss: 0.0104 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 60/170
374/374 - 2s - loss: 0.0066 - val_loss: 0.0072 - 2s/epoch - 6ms/step
Epoch 61/170
374/374 - 2s - loss: 0.0065 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 62/170
374/374 - 2s - loss: 0.0065 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 63/170
374/374 - 2s - loss: 0.0064 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 64/170
374/374 - 2s - loss: 0.0064 - val_loss: 0.0182 - 2s/epoch - 6ms/step
Epoch 65/170
374/374 - 2s - loss: 0.0128 - val_loss: 0.0068 - 2s/epoch - 6ms/step
Epoch 66/170
374/374 - 2s - loss: 0.0066 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 67/170
374/374 - 2s - loss: 0.0065 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 68/170
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 69/170
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 70/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 71/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 72/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0068 - 2s/epoch - 6ms/step
Epoch 73/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 74/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 75/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 76/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 77/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0142 - 2s/epoch - 6ms/step
Epoch 78/170
374/374 - 2s - loss: 0.0086 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 79/170
374/374 - 2s - loss: 0.0064 - val_loss: 0.0071 - 2s/epoch - 6ms/step
Epoch 80/170
374/374 - 2s - loss: 0.0069 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 81/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 82/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 83/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 84/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 85/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 86/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 87/170
374/374 - 2s - loss: 0.0086 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 88/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 89/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 90/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 91/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 92/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 93/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 94/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 95/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 96/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0081 - 2s/epoch - 6ms/step
Epoch 97/170
374/374 - 2s - loss: 0.0099 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 98/170
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 99/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 100/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 101/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 102/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 103/170
374/374 - 2s - loss: 0.0064 - val_loss: 0.0076 - 2s/epoch - 6ms/step
Epoch 104/170
374/374 - 2s - loss: 0.0089 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 105/170
374/374 - 2s - loss: 0.0064 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 106/170
374/374 - 2s - loss: 0.0065 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 107/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 108/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 109/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 110/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 111/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 112/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 113/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 114/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 115/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 116/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 117/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 118/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 119/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 120/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 121/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 122/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 123/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 124/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 125/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 126/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 127/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0079 - 2s/epoch - 6ms/step
Epoch 128/170
374/374 - 2s - loss: 0.0072 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 129/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 130/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 131/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 132/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 133/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 134/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 135/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 136/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 137/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 138/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 139/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 140/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 141/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 142/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 143/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 144/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 145/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 146/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 147/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 148/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 149/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 150/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 151/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 152/170
374/374 - 2s - loss: 0.0064 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 153/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 154/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 155/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 156/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 157/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 158/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 159/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 160/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 161/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 162/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 163/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 164/170
374/374 - 2s - loss: 0.0066 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 165/170
374/374 - 2s - loss: 0.0063 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 166/170
374/374 - 2s - loss: 0.0069 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 167/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 168/170
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 169/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 170/170
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.006076914258301258
  1/332 [..............................] - ETA: 1:17 23/332 [=>............................] - ETA: 0s   36/332 [==>...........................] - ETA: 0s 58/332 [====>.........................] - ETA: 0s 80/332 [======>.......................] - ETA: 0s102/332 [========>.....................] - ETA: 0s124/332 [==========>...................] - ETA: 0s146/332 [============>.................] - ETA: 0s168/332 [==============>...............] - ETA: 0s190/332 [================>.............] - ETA: 0s212/332 [==================>...........] - ETA: 0s234/332 [====================>.........] - ETA: 0s256/332 [======================>.......] - ETA: 0s278/332 [========================>.....] - ETA: 0s300/332 [==========================>...] - ETA: 0s322/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.12869733234722122
cosine 0.10069398866749421
MAE: 0.047741815
RMSE: 0.10189069
r2: 0.32651308819714325
RMSE zero-vector: 0.23411466903540806
['2.7custom_VAE', 'logcosh', 256, 170, 0.002, 0.6, 758, 0.006134835071861744, 0.006076914258301258, 0.12869733234722122, 0.10069398866749421, 0.04774181544780731, 0.1018906906247139, 0.32651308819714325, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 170 0.002 128 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1769)        7076        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1769)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4165381     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,093,562
Trainable params: 9,084,970
Non-trainable params: 8,592
__________________________________________________________________________________________________
Epoch 1/170
747/747 - 7s - loss: 0.0308 - val_loss: 0.0205 - 7s/epoch - 9ms/step
Epoch 2/170
747/747 - 4s - loss: 0.0156 - val_loss: 0.0169 - 4s/epoch - 5ms/step
Epoch 3/170
747/747 - 4s - loss: 0.0149 - val_loss: 0.0152 - 4s/epoch - 5ms/step
Epoch 4/170
747/747 - 4s - loss: 0.0141 - val_loss: 0.0194 - 4s/epoch - 5ms/step
Epoch 5/170
747/747 - 4s - loss: 0.0135 - val_loss: 0.0135 - 4s/epoch - 5ms/step
Epoch 6/170
747/747 - 4s - loss: 0.0131 - val_loss: 0.0130 - 4s/epoch - 5ms/step
Epoch 7/170
747/747 - 4s - loss: 0.0128 - val_loss: 0.0126 - 4s/epoch - 5ms/step
Epoch 8/170
747/747 - 4s - loss: 0.0125 - val_loss: 0.0122 - 4s/epoch - 5ms/step
Epoch 9/170
747/747 - 4s - loss: 0.0121 - val_loss: 0.0117 - 4s/epoch - 5ms/step
Epoch 10/170
747/747 - 4s - loss: 0.0117 - val_loss: 0.0115 - 4s/epoch - 5ms/step
Epoch 11/170
747/747 - 4s - loss: 0.0115 - val_loss: 0.0113 - 4s/epoch - 5ms/step
Epoch 12/170
747/747 - 4s - loss: 0.0114 - val_loss: 0.0117 - 4s/epoch - 5ms/step
Epoch 13/170
747/747 - 4s - loss: 0.0118 - val_loss: 0.0112 - 4s/epoch - 5ms/step
Epoch 14/170
747/747 - 4s - loss: 0.0113 - val_loss: 0.0115 - 4s/epoch - 5ms/step
Epoch 15/170
747/747 - 4s - loss: 0.0118 - val_loss: 0.0116 - 4s/epoch - 5ms/step
Epoch 16/170
747/747 - 4s - loss: 0.0120 - val_loss: 0.0112 - 4s/epoch - 5ms/step
Epoch 17/170
747/747 - 4s - loss: 0.0112 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 18/170
747/747 - 4s - loss: 0.0112 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 19/170
747/747 - 4s - loss: 0.0111 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 20/170
747/747 - 4s - loss: 0.0111 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 21/170
747/747 - 4s - loss: 0.0111 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 22/170
747/747 - 4s - loss: 0.0110 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 23/170
747/747 - 4s - loss: 0.0110 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 24/170
747/747 - 4s - loss: 0.0110 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 25/170
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 26/170
747/747 - 4s - loss: 0.0109 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 27/170
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 28/170
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 29/170
747/747 - 4s - loss: 0.0109 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 30/170
747/747 - 4s - loss: 0.0110 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 31/170
747/747 - 4s - loss: 0.0108 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 32/170
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 33/170
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 34/170
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 35/170
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 36/170
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 37/170
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 38/170
747/747 - 4s - loss: 0.0106 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 39/170
747/747 - 4s - loss: 0.0106 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 40/170
747/747 - 4s - loss: 0.0105 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 41/170
747/747 - 4s - loss: 0.0105 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 42/170
747/747 - 4s - loss: 0.0105 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 43/170
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 44/170
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 45/170
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 46/170
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 47/170
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 48/170
747/747 - 4s - loss: 0.0104 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 49/170
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 50/170
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 51/170
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 52/170
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 53/170
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 54/170
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 55/170
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 56/170
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 57/170
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 58/170
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 59/170
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 60/170
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 61/170
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 62/170
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 63/170
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 64/170
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 65/170
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 66/170
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 67/170
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 68/170
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 69/170
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 70/170
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 71/170
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 72/170
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 73/170
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 74/170
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 75/170
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 76/170
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 77/170
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 78/170
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 79/170
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 80/170
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 81/170
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 82/170
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 83/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 84/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 85/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 86/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 87/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 88/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 89/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 90/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 91/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 92/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 93/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 94/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 95/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 96/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 97/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 98/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 99/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 100/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 101/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 102/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 103/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 104/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 105/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 106/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 107/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 108/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 109/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 110/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 111/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 112/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 113/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 114/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 115/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 116/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 117/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 118/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 119/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 120/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 121/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 122/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 123/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 124/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 125/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 126/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 127/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 128/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 129/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 130/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 131/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 132/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 133/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 134/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 135/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 136/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 137/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 138/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 139/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 140/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 141/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 142/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 143/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 144/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 145/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 146/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 147/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 148/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 149/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 150/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 151/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 152/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 153/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 154/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 155/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 156/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 157/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 158/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 159/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 160/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 161/170
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 162/170
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 163/170
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 164/170
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 165/170
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 166/170
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 167/170
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 168/170
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 169/170
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 170/170
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009474346414208412
  1/332 [..............................] - ETA: 49s 38/332 [==>...........................] - ETA: 0s  76/332 [=====>........................] - ETA: 0s114/332 [=========>....................] - ETA: 0s152/332 [============>.................] - ETA: 0s191/332 [================>.............] - ETA: 0s227/332 [===================>..........] - ETA: 0s266/332 [=======================>......] - ETA: 0s305/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.0771659106701399
cosine 0.060574662874116114
MAE: 0.03647154
RMSE: 0.0798121
r2: 0.5867646081513825
RMSE zero-vector: 0.23411466903540806
['1.4custom_VAE', 'mse', 128, 170, 0.002, 0.6, 758, 0.009632350876927376, 0.009474346414208412, 0.0771659106701399, 0.060574662874116114, 0.03647153824567795, 0.07981210201978683, 0.5867646081513825, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.7 175 0.002 64 2] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3412)         4316180     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3412)        13648       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3412)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2587054     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2587054     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7495742     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,999,678
Trainable params: 16,984,514
Non-trainable params: 15,164
__________________________________________________________________________________________________
Epoch 1/175
1493/1493 - 10s - loss: 0.0187 - val_loss: 0.0112 - 10s/epoch - 7ms/step
Epoch 2/175
1493/1493 - 7s - loss: 0.0088 - val_loss: 0.0094 - 7s/epoch - 5ms/step
Epoch 3/175
1493/1493 - 7s - loss: 0.0075 - val_loss: 0.0071 - 7s/epoch - 5ms/step
Epoch 4/175
1493/1493 - 7s - loss: 0.0070 - val_loss: 0.0068 - 7s/epoch - 5ms/step
Epoch 5/175
1493/1493 - 7s - loss: 0.0068 - val_loss: 0.0067 - 7s/epoch - 5ms/step
Epoch 6/175
1493/1493 - 7s - loss: 0.0068 - val_loss: 0.0067 - 7s/epoch - 5ms/step
Epoch 7/175
1493/1493 - 7s - loss: 0.0067 - val_loss: 0.0067 - 7s/epoch - 5ms/step
Epoch 8/175
1493/1493 - 7s - loss: 0.0067 - val_loss: 0.0066 - 7s/epoch - 5ms/step
Epoch 9/175
1493/1493 - 7s - loss: 0.0066 - val_loss: 0.0065 - 7s/epoch - 5ms/step
Epoch 10/175
1493/1493 - 7s - loss: 0.0065 - val_loss: 0.0063 - 7s/epoch - 5ms/step
Epoch 11/175
1493/1493 - 7s - loss: 0.0064 - val_loss: 0.0063 - 7s/epoch - 5ms/step
Epoch 12/175
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0063 - 7s/epoch - 5ms/step
Epoch 13/175
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 14/175
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 15/175
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 16/175
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 17/175
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 18/175
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 19/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 20/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 21/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 22/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 23/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 24/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 25/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 26/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 27/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 28/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 29/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 30/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 31/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 32/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 33/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 34/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 35/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 36/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 37/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 38/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 39/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 40/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 41/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 42/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 43/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 44/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 45/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 46/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 47/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 48/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 49/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 50/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 51/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 52/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 53/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 54/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 55/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 56/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 57/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 58/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 59/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 60/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 61/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 62/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 63/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 64/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 65/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 66/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 67/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 68/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 69/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 70/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 71/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 72/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 73/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 74/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 75/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 76/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 77/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 78/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 79/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 80/175
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 81/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 82/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 83/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 84/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 85/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 86/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 87/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 88/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 89/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 90/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 91/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 92/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 93/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 94/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 95/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 96/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 97/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 98/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 99/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 100/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 101/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 102/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 103/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 104/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 105/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 106/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 107/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 108/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 109/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 110/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 111/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 112/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 113/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 114/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 115/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 116/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 117/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 118/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 119/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 120/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 121/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 122/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 123/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 124/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 125/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 126/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 127/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 128/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 129/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 130/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 131/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 132/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 133/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 134/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 135/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 136/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 137/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 138/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 139/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 140/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 141/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 142/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 143/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 144/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 145/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 146/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 147/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 148/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 149/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 150/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 151/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 152/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 153/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 154/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 155/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 156/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 157/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 158/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 159/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 160/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 161/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 162/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 163/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 164/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 165/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 166/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 167/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 168/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 169/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 170/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 171/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 172/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 173/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 174/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 175/175
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.006174253299832344
  1/332 [..............................] - ETA: 50s 38/332 [==>...........................] - ETA: 0s  77/332 [=====>........................] - ETA: 0s113/332 [=========>....................] - ETA: 0s152/332 [============>.................] - ETA: 0s190/332 [================>.............] - ETA: 0s228/332 [===================>..........] - ETA: 0s267/332 [=======================>......] - ETA: 0s305/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.12706350742411354
cosine 0.09946439597479047
MAE: 0.047792286
RMSE: 0.102585256
r2: 0.31729962918752125
RMSE zero-vector: 0.23411466903540806
['2.7custom_VAE', 'logcosh', 64, 175, 0.002, 0.6, 758, 0.006125014275312424, 0.006174253299832344, 0.12706350742411354, 0.09946439597479047, 0.047792285680770874, 0.10258525609970093, 0.31729962918752125, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.7 175 0.002 64 2] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3412)         4316180     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3412)        13648       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3412)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2587054     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2587054     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7495742     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,999,678
Trainable params: 16,984,514
Non-trainable params: 15,164
__________________________________________________________________________________________________
File geneticVAE_MMmp_gap_custom_VAE2.7_cr0.6_bs64_ep175_loss_logcosh_lr0.002_AutoEncoder.h5 exists in folder already, skiping this calculation.
  1/332 [..............................] - ETA: 54s 38/332 [==>...........................] - ETA: 0s  76/332 [=====>........................] - ETA: 0s113/332 [=========>....................] - ETA: 0s151/332 [============>.................] - ETA: 0s189/332 [================>.............] - ETA: 0s227/332 [===================>..........] - ETA: 0s265/332 [======================>.......] - ETA: 0s303/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.12706350742411354
cosine 0.09946439597479047
MAE: 0.047792286
RMSE: 0.102585256
r2: 0.31729962918752125
RMSE zero-vector: 0.23411466903540806
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
['2.7custom_VAE', 'logcosh', 64, 175, 0.002, 0.6, 758, '--', '--', 0.12706350742411354, 0.09946439597479047, 0.047792285680770874, 0.10258525609970093, 0.31729962918752125, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Saved GA instance to file: ./tmp//ga_instance_generation_2.pkl
[2.5 175 0.002 128 2] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2396038     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2396038     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         6984938     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 15,787,054
Trainable params: 15,772,898
Non-trainable params: 14,156
__________________________________________________________________________________________________
Epoch 1/175
747/747 - 7s - loss: 0.0219 - val_loss: 0.0265 - 7s/epoch - 9ms/step
Epoch 2/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0636 - 4s/epoch - 5ms/step
Epoch 3/175
747/747 - 4s - loss: 0.0084 - val_loss: 0.0113 - 4s/epoch - 5ms/step
Epoch 4/175
747/747 - 4s - loss: 0.0077 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 5/175
747/747 - 4s - loss: 0.0074 - val_loss: 0.0075 - 4s/epoch - 5ms/step
Epoch 6/175
747/747 - 4s - loss: 0.0071 - val_loss: 0.0076 - 4s/epoch - 5ms/step
Epoch 7/175
747/747 - 4s - loss: 0.0069 - val_loss: 0.0068 - 4s/epoch - 5ms/step
Epoch 8/175
747/747 - 4s - loss: 0.0067 - val_loss: 0.0065 - 4s/epoch - 5ms/step
Epoch 9/175
747/747 - 4s - loss: 0.0065 - val_loss: 0.0064 - 4s/epoch - 5ms/step
Epoch 10/175
747/747 - 4s - loss: 0.0064 - val_loss: 0.0063 - 4s/epoch - 5ms/step
Epoch 11/175
747/747 - 4s - loss: 0.0063 - val_loss: 0.0063 - 4s/epoch - 5ms/step
Epoch 12/175
747/747 - 4s - loss: 0.0063 - val_loss: 0.0064 - 4s/epoch - 5ms/step
Epoch 13/175
747/747 - 4s - loss: 0.0064 - val_loss: 0.0064 - 4s/epoch - 5ms/step
Epoch 14/175
747/747 - 4s - loss: 0.0064 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 15/175
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 16/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0063 - 4s/epoch - 5ms/step
Epoch 17/175
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 18/175
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 19/175
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 20/175
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 21/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 22/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 23/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 24/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 25/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 26/175
747/747 - 4s - loss: 0.0063 - val_loss: 0.0063 - 4s/epoch - 5ms/step
Epoch 27/175
747/747 - 4s - loss: 0.0064 - val_loss: 0.0063 - 4s/epoch - 5ms/step
Epoch 28/175
747/747 - 4s - loss: 0.0064 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 29/175
747/747 - 4s - loss: 0.0063 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 30/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 31/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 32/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 33/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 34/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 35/175
747/747 - 4s - loss: 0.0063 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 36/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 37/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 38/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 39/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 40/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 41/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 42/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 43/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 44/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 45/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 46/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 47/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 48/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 49/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 50/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 51/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 52/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 53/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 54/175
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 55/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 56/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 57/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 58/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 59/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 60/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 61/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 62/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 63/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 64/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 65/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 66/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 67/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 68/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 69/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 70/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 71/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 72/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 73/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 74/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 75/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 76/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 77/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 78/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 79/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 80/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 81/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 82/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 83/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 84/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 85/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 86/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 87/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 88/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 89/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 90/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 91/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 92/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 93/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 94/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 95/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 96/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 97/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 98/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 99/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 100/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 101/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 102/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 103/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 104/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 105/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 106/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 107/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 108/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 109/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 110/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 111/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 112/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 113/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 114/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 115/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 116/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 117/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 118/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 119/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 120/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 121/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 122/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 123/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 124/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 125/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 126/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 127/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 128/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 129/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 130/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 131/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 132/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 133/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 134/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 135/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 136/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 137/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 138/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 139/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 140/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 141/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 142/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 143/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 144/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 145/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 146/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 147/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 148/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 149/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 150/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 151/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 152/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 153/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 154/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 155/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 156/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 157/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 158/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 159/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 160/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 161/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 162/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 163/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 164/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 165/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 166/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 167/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 168/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 169/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 170/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 171/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 172/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 173/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 174/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 175/175
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.006009584292769432
  1/332 [..............................] - ETA: 1:21 22/332 [>.............................] - ETA: 0s   44/332 [==>...........................] - ETA: 0s 66/332 [====>.........................] - ETA: 0s 89/332 [=======>......................] - ETA: 0s112/332 [=========>....................] - ETA: 0s134/332 [===========>..................] - ETA: 0s156/332 [=============>................] - ETA: 0s178/332 [===============>..............] - ETA: 0s200/332 [=================>............] - ETA: 0s222/332 [===================>..........] - ETA: 0s244/332 [=====================>........] - ETA: 0s266/332 [=======================>......] - ETA: 0s289/332 [=========================>....] - ETA: 0s312/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.12430279878313209
cosine 0.09724072955304436
MAE: 0.046806715
RMSE: 0.100294136
r2: 0.3474538958824623
RMSE zero-vector: 0.23411466903540806
['2.5custom_VAE', 'logcosh', 128, 175, 0.002, 0.6, 758, 0.006063299719244242, 0.006009584292769432, 0.12430279878313209, 0.09724072955304436, 0.04680671542882919, 0.10029413551092148, 0.3474538958824623, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.6 170 0.002 256 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3286)         4156790     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3286)        13144       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3286)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7240340     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,393,366
Trainable params: 16,378,706
Non-trainable params: 14,660
__________________________________________________________________________________________________
Epoch 1/170
374/374 - 5s - loss: 0.0507 - val_loss: 0.1768 - 5s/epoch - 14ms/step
Epoch 2/170
374/374 - 2s - loss: 0.0203 - val_loss: 0.0184 - 2s/epoch - 6ms/step
Epoch 3/170
374/374 - 2s - loss: 0.0162 - val_loss: 0.0365 - 2s/epoch - 5ms/step
Epoch 4/170
374/374 - 2s - loss: 0.0156 - val_loss: 0.0306 - 2s/epoch - 6ms/step
Epoch 5/170
374/374 - 2s - loss: 0.0148 - val_loss: 0.0184 - 2s/epoch - 6ms/step
Epoch 6/170
374/374 - 2s - loss: 0.0144 - val_loss: 0.0277 - 2s/epoch - 6ms/step
Epoch 7/170
374/374 - 2s - loss: 0.0143 - val_loss: 0.0432 - 2s/epoch - 6ms/step
Epoch 8/170
374/374 - 2s - loss: 0.0140 - val_loss: 0.0160 - 2s/epoch - 6ms/step
Epoch 9/170
374/374 - 2s - loss: 0.0127 - val_loss: 0.0165 - 2s/epoch - 5ms/step
Epoch 10/170
374/374 - 2s - loss: 0.0123 - val_loss: 0.0180 - 2s/epoch - 6ms/step
Epoch 11/170
374/374 - 2s - loss: 0.0124 - val_loss: 0.1774 - 2s/epoch - 5ms/step
Epoch 12/170
374/374 - 2s - loss: 0.0192 - val_loss: 0.0127 - 2s/epoch - 6ms/step
Epoch 13/170
374/374 - 2s - loss: 0.0123 - val_loss: 0.0146 - 2s/epoch - 6ms/step
Epoch 14/170
374/374 - 2s - loss: 0.0126 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 15/170
374/374 - 2s - loss: 0.0117 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 16/170
374/374 - 2s - loss: 0.0116 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 17/170
374/374 - 2s - loss: 0.0116 - val_loss: 0.0130 - 2s/epoch - 6ms/step
Epoch 18/170
374/374 - 2s - loss: 0.0123 - val_loss: 0.0127 - 2s/epoch - 6ms/step
Epoch 19/170
374/374 - 2s - loss: 0.0117 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 20/170
374/374 - 2s - loss: 0.0114 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 21/170
374/374 - 2s - loss: 0.0119 - val_loss: 0.0121 - 2s/epoch - 6ms/step
Epoch 22/170
374/374 - 2s - loss: 0.0122 - val_loss: 0.0120 - 2s/epoch - 6ms/step
Epoch 23/170
374/374 - 2s - loss: 0.0154 - val_loss: 0.0133 - 2s/epoch - 6ms/step
Epoch 24/170
374/374 - 2s - loss: 0.0208 - val_loss: 0.0139 - 2s/epoch - 6ms/step
Epoch 25/170
374/374 - 2s - loss: 0.0154 - val_loss: 0.0134 - 2s/epoch - 6ms/step
Epoch 26/170
374/374 - 2s - loss: 0.0211 - val_loss: 0.0134 - 2s/epoch - 6ms/step
Epoch 27/170
374/374 - 2s - loss: 0.0200 - val_loss: 0.0158 - 2s/epoch - 6ms/step
Epoch 28/170
374/374 - 2s - loss: 0.0386 - val_loss: 0.0176 - 2s/epoch - 6ms/step
Epoch 29/170
374/374 - 2s - loss: 0.0210 - val_loss: 0.0138 - 2s/epoch - 6ms/step
Epoch 30/170
374/374 - 2s - loss: 0.0137 - val_loss: 0.0132 - 2s/epoch - 6ms/step
Epoch 31/170
374/374 - 2s - loss: 0.0132 - val_loss: 0.0127 - 2s/epoch - 6ms/step
Epoch 32/170
374/374 - 2s - loss: 0.0129 - val_loss: 0.0127 - 2s/epoch - 6ms/step
Epoch 33/170
374/374 - 2s - loss: 0.0134 - val_loss: 0.0142 - 2s/epoch - 6ms/step
Epoch 34/170
374/374 - 2s - loss: 0.0172 - val_loss: 0.0128 - 2s/epoch - 6ms/step
Epoch 35/170
374/374 - 2s - loss: 0.0141 - val_loss: 0.0189 - 2s/epoch - 6ms/step
Epoch 36/170
374/374 - 2s - loss: 0.0336 - val_loss: 0.0220 - 2s/epoch - 6ms/step
Epoch 37/170
374/374 - 2s - loss: 0.0192 - val_loss: 0.0143 - 2s/epoch - 6ms/step
Epoch 38/170
374/374 - 2s - loss: 0.0153 - val_loss: 0.0302 - 2s/epoch - 6ms/step
Epoch 39/170
374/374 - 2s - loss: 0.0339 - val_loss: 0.0145 - 2s/epoch - 6ms/step
Epoch 40/170
374/374 - 2s - loss: 0.0146 - val_loss: 0.0147 - 2s/epoch - 6ms/step
Epoch 41/170
374/374 - 2s - loss: 0.0162 - val_loss: 0.0150 - 2s/epoch - 6ms/step
Epoch 42/170
374/374 - 2s - loss: 0.0178 - val_loss: 0.0137 - 2s/epoch - 6ms/step
Epoch 43/170
374/374 - 2s - loss: 0.0136 - val_loss: 0.0130 - 2s/epoch - 6ms/step
Epoch 44/170
374/374 - 2s - loss: 0.0132 - val_loss: 0.0128 - 2s/epoch - 6ms/step
Epoch 45/170
374/374 - 2s - loss: 0.0129 - val_loss: 0.0126 - 2s/epoch - 6ms/step
Epoch 46/170
374/374 - 2s - loss: 0.0127 - val_loss: 0.0124 - 2s/epoch - 6ms/step
Epoch 47/170
374/374 - 2s - loss: 0.0126 - val_loss: 0.0134 - 2s/epoch - 6ms/step
Epoch 48/170
374/374 - 2s - loss: 0.0130 - val_loss: 0.0123 - 2s/epoch - 6ms/step
Epoch 49/170
374/374 - 2s - loss: 0.0124 - val_loss: 0.0121 - 2s/epoch - 6ms/step
Epoch 50/170
374/374 - 2s - loss: 0.0122 - val_loss: 0.0120 - 2s/epoch - 6ms/step
Epoch 51/170
374/374 - 2s - loss: 0.0121 - val_loss: 0.0124 - 2s/epoch - 6ms/step
Epoch 52/170
374/374 - 2s - loss: 0.0121 - val_loss: 0.0117 - 2s/epoch - 6ms/step
Epoch 53/170
374/374 - 2s - loss: 0.0118 - val_loss: 0.0118 - 2s/epoch - 6ms/step
Epoch 54/170
374/374 - 2s - loss: 0.0119 - val_loss: 0.0117 - 2s/epoch - 6ms/step
Epoch 55/170
374/374 - 2s - loss: 0.0117 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 56/170
374/374 - 2s - loss: 0.0116 - val_loss: 0.0133 - 2s/epoch - 6ms/step
Epoch 57/170
374/374 - 2s - loss: 0.0134 - val_loss: 0.0144 - 2s/epoch - 6ms/step
Epoch 58/170
374/374 - 2s - loss: 0.0124 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 59/170
374/374 - 2s - loss: 0.0116 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 60/170
374/374 - 2s - loss: 0.0115 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 61/170
374/374 - 2s - loss: 0.0115 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 62/170
374/374 - 2s - loss: 0.0115 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 63/170
374/374 - 2s - loss: 0.0113 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 64/170
374/374 - 2s - loss: 0.0115 - val_loss: 0.0257 - 2s/epoch - 6ms/step
Epoch 65/170
374/374 - 2s - loss: 0.0254 - val_loss: 0.0195 - 2s/epoch - 6ms/step
Epoch 66/170
374/374 - 2s - loss: 0.0338 - val_loss: 0.0149 - 2s/epoch - 6ms/step
Epoch 67/170
374/374 - 2s - loss: 0.0166 - val_loss: 0.0129 - 2s/epoch - 6ms/step
Epoch 68/170
374/374 - 2s - loss: 0.0128 - val_loss: 0.0122 - 2s/epoch - 6ms/step
Epoch 69/170
374/374 - 2s - loss: 0.0123 - val_loss: 0.0120 - 2s/epoch - 6ms/step
Epoch 70/170
374/374 - 2s - loss: 0.0121 - val_loss: 0.0118 - 2s/epoch - 6ms/step
Epoch 71/170
374/374 - 2s - loss: 0.0119 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 72/170
374/374 - 2s - loss: 0.0148 - val_loss: 0.0479 - 2s/epoch - 6ms/step
Epoch 73/170
374/374 - 2s - loss: 0.0684 - val_loss: 0.0170 - 2s/epoch - 6ms/step
Epoch 74/170
374/374 - 2s - loss: 0.0144 - val_loss: 0.0138 - 2s/epoch - 6ms/step
Epoch 75/170
374/374 - 2s - loss: 0.0149 - val_loss: 0.0129 - 2s/epoch - 6ms/step
Epoch 76/170
374/374 - 2s - loss: 0.0131 - val_loss: 0.0129 - 2s/epoch - 6ms/step
Epoch 77/170
374/374 - 2s - loss: 0.0128 - val_loss: 0.0123 - 2s/epoch - 6ms/step
Epoch 78/170
374/374 - 2s - loss: 0.0126 - val_loss: 0.0180 - 2s/epoch - 6ms/step
Epoch 79/170
374/374 - 2s - loss: 0.0168 - val_loss: 0.0127 - 2s/epoch - 6ms/step
Epoch 80/170
374/374 - 2s - loss: 0.0126 - val_loss: 0.0123 - 2s/epoch - 6ms/step
Epoch 81/170
374/374 - 2s - loss: 0.0123 - val_loss: 0.0120 - 2s/epoch - 6ms/step
Epoch 82/170
374/374 - 2s - loss: 0.0121 - val_loss: 0.0120 - 2s/epoch - 6ms/step
Epoch 83/170
374/374 - 2s - loss: 0.0120 - val_loss: 0.0118 - 2s/epoch - 6ms/step
Epoch 84/170
374/374 - 2s - loss: 0.0119 - val_loss: 0.0119 - 2s/epoch - 6ms/step
Epoch 85/170
374/374 - 2s - loss: 0.0118 - val_loss: 0.0117 - 2s/epoch - 6ms/step
Epoch 86/170
374/374 - 2s - loss: 0.0118 - val_loss: 0.0142 - 2s/epoch - 6ms/step
Epoch 87/170
374/374 - 2s - loss: 0.0135 - val_loss: 0.0181 - 2s/epoch - 6ms/step
Epoch 88/170
374/374 - 2s - loss: 0.0176 - val_loss: 0.0121 - 2s/epoch - 6ms/step
Epoch 89/170
374/374 - 2s - loss: 0.0120 - val_loss: 0.0117 - 2s/epoch - 6ms/step
Epoch 90/170
374/374 - 2s - loss: 0.0117 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 91/170
374/374 - 2s - loss: 0.0115 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 92/170
374/374 - 2s - loss: 0.0115 - val_loss: 0.0424 - 2s/epoch - 6ms/step
Epoch 93/170
374/374 - 2s - loss: 0.0198 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 94/170
374/374 - 2s - loss: 0.0116 - val_loss: 0.0120 - 2s/epoch - 6ms/step
Epoch 95/170
374/374 - 2s - loss: 0.0117 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 96/170
374/374 - 2s - loss: 0.0113 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 97/170
374/374 - 2s - loss: 0.0112 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 98/170
374/374 - 2s - loss: 0.0111 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 99/170
374/374 - 2s - loss: 0.0111 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 100/170
374/374 - 2s - loss: 0.0110 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 101/170
374/374 - 2s - loss: 0.0110 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 102/170
374/374 - 2s - loss: 0.0109 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 103/170
374/374 - 2s - loss: 0.0109 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 104/170
374/374 - 2s - loss: 0.0109 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 105/170
374/374 - 2s - loss: 0.0108 - val_loss: 0.0153 - 2s/epoch - 6ms/step
Epoch 106/170
374/374 - 2s - loss: 0.0127 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 107/170
374/374 - 2s - loss: 0.0108 - val_loss: 0.0151 - 2s/epoch - 6ms/step
Epoch 108/170
374/374 - 2s - loss: 0.0132 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 109/170
374/374 - 2s - loss: 0.0108 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 110/170
374/374 - 2s - loss: 0.0108 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 111/170
374/374 - 2s - loss: 0.0107 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 112/170
374/374 - 2s - loss: 0.0108 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 113/170
374/374 - 2s - loss: 0.0107 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 114/170
374/374 - 2s - loss: 0.0106 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 115/170
374/374 - 2s - loss: 0.0106 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 116/170
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 117/170
374/374 - 2s - loss: 0.0106 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 118/170
374/374 - 2s - loss: 0.0106 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 119/170
374/374 - 2s - loss: 0.0106 - val_loss: 0.0151 - 2s/epoch - 6ms/step
Epoch 120/170
374/374 - 2s - loss: 0.0134 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 121/170
374/374 - 2s - loss: 0.0107 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 122/170
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 123/170
374/374 - 2s - loss: 0.0119 - val_loss: 0.0139 - 2s/epoch - 6ms/step
Epoch 124/170
374/374 - 2s - loss: 0.0230 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 125/170
374/374 - 2s - loss: 0.0115 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 126/170
374/374 - 2s - loss: 0.0110 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 127/170
374/374 - 2s - loss: 0.0109 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 128/170
374/374 - 2s - loss: 0.0108 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 129/170
374/374 - 2s - loss: 0.0108 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 130/170
374/374 - 2s - loss: 0.0114 - val_loss: 0.0141 - 2s/epoch - 6ms/step
Epoch 131/170
374/374 - 2s - loss: 0.0144 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 132/170
374/374 - 2s - loss: 0.0109 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 133/170
374/374 - 2s - loss: 0.0108 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 134/170
374/374 - 2s - loss: 0.0107 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 135/170
374/374 - 2s - loss: 0.0107 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 136/170
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 137/170
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 138/170
374/374 - 2s - loss: 0.0108 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 139/170
374/374 - 2s - loss: 0.0109 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 140/170
374/374 - 2s - loss: 0.0106 - val_loss: 0.0131 - 2s/epoch - 6ms/step
Epoch 141/170
374/374 - 2s - loss: 0.0123 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 142/170
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 143/170
374/374 - 2s - loss: 0.0106 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 144/170
374/374 - 2s - loss: 0.0110 - val_loss: 0.0229 - 2s/epoch - 6ms/step
Epoch 145/170
374/374 - 2s - loss: 0.0474 - val_loss: 0.0136 - 2s/epoch - 6ms/step
Epoch 146/170
374/374 - 2s - loss: 0.0118 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 147/170
374/374 - 2s - loss: 0.0113 - val_loss: 0.0121 - 2s/epoch - 6ms/step
Epoch 148/170
374/374 - 2s - loss: 0.0135 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 149/170
374/374 - 2s - loss: 0.0111 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 150/170
374/374 - 2s - loss: 0.0110 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 151/170
374/374 - 2s - loss: 0.0109 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 152/170
374/374 - 2s - loss: 0.0108 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 153/170
374/374 - 2s - loss: 0.0108 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 154/170
374/374 - 2s - loss: 0.0108 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 155/170
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 156/170
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 157/170
374/374 - 2s - loss: 0.0106 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 158/170
374/374 - 2s - loss: 0.0106 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 159/170
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 160/170
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 161/170
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 162/170
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 163/170
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 164/170
374/374 - 2s - loss: 0.0104 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 165/170
374/374 - 2s - loss: 0.0104 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 166/170
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 167/170
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 168/170
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 169/170
374/374 - 2s - loss: 0.0103 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 170/170
374/374 - 2s - loss: 0.0103 - val_loss: 0.0106 - 2s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.010558011010289192
  1/332 [..............................] - ETA: 46s 38/332 [==>...........................] - ETA: 0s  77/332 [=====>........................] - ETA: 0s113/332 [=========>....................] - ETA: 0s152/332 [============>.................] - ETA: 0s184/332 [===============>..............] - ETA: 0s223/332 [===================>..........] - ETA: 0s262/332 [======================>.......] - ETA: 0s301/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.09023027219189997
cosine 0.07082647994536972
MAE: 0.039625578
RMSE: 0.08590711
r2: 0.52123946221042
RMSE zero-vector: 0.23411466903540806
['2.6custom_VAE', 'mse', 256, 170, 0.002, 0.6, 758, 0.010331719182431698, 0.010558011010289192, 0.09023027219189997, 0.07082647994536972, 0.03962557762861252, 0.08590710908174515, 0.52123946221042, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 175 0.0018 256 2] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2396038     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2396038     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         6984938     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 15,787,054
Trainable params: 15,772,898
Non-trainable params: 14,156
__________________________________________________________________________________________________
Epoch 1/175
374/374 - 5s - loss: 0.0314 - val_loss: 0.0135 - 5s/epoch - 13ms/step
Epoch 2/175
374/374 - 2s - loss: 0.0093 - val_loss: 0.0355 - 2s/epoch - 6ms/step
Epoch 3/175
374/374 - 2s - loss: 0.0158 - val_loss: 0.0092 - 2s/epoch - 6ms/step
Epoch 4/175
374/374 - 2s - loss: 0.0079 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 5/175
374/374 - 2s - loss: 0.0078 - val_loss: 0.0221 - 2s/epoch - 6ms/step
Epoch 6/175
374/374 - 2s - loss: 0.0079 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 7/175
374/374 - 2s - loss: 0.0078 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 8/175
374/374 - 2s - loss: 0.0076 - val_loss: 0.0092 - 2s/epoch - 6ms/step
Epoch 9/175
374/374 - 2s - loss: 0.0075 - val_loss: 0.0157 - 2s/epoch - 6ms/step
Epoch 10/175
374/374 - 2s - loss: 0.0079 - val_loss: 0.0076 - 2s/epoch - 6ms/step
Epoch 11/175
374/374 - 2s - loss: 0.0071 - val_loss: 0.0073 - 2s/epoch - 6ms/step
Epoch 12/175
374/374 - 2s - loss: 0.0070 - val_loss: 0.0077 - 2s/epoch - 6ms/step
Epoch 13/175
374/374 - 2s - loss: 0.0070 - val_loss: 0.0150 - 2s/epoch - 6ms/step
Epoch 14/175
374/374 - 2s - loss: 0.0072 - val_loss: 0.0070 - 2s/epoch - 6ms/step
Epoch 15/175
374/374 - 2s - loss: 0.0069 - val_loss: 0.0179 - 2s/epoch - 6ms/step
Epoch 16/175
374/374 - 2s - loss: 0.0092 - val_loss: 0.0070 - 2s/epoch - 6ms/step
Epoch 17/175
374/374 - 2s - loss: 0.0070 - val_loss: 0.0093 - 2s/epoch - 6ms/step
Epoch 18/175
374/374 - 2s - loss: 0.0114 - val_loss: 0.0073 - 2s/epoch - 6ms/step
Epoch 19/175
374/374 - 2s - loss: 0.0071 - val_loss: 0.0069 - 2s/epoch - 6ms/step
Epoch 20/175
374/374 - 2s - loss: 0.0069 - val_loss: 0.0068 - 2s/epoch - 6ms/step
Epoch 21/175
374/374 - 2s - loss: 0.0069 - val_loss: 0.0128 - 2s/epoch - 6ms/step
Epoch 22/175
374/374 - 2s - loss: 0.0126 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 23/175
374/374 - 2s - loss: 0.0078 - val_loss: 0.0070 - 2s/epoch - 6ms/step
Epoch 24/175
374/374 - 2s - loss: 0.0070 - val_loss: 0.0069 - 2s/epoch - 6ms/step
Epoch 25/175
374/374 - 2s - loss: 0.0069 - val_loss: 0.0068 - 2s/epoch - 6ms/step
Epoch 26/175
374/374 - 2s - loss: 0.0069 - val_loss: 0.0081 - 2s/epoch - 6ms/step
Epoch 27/175
374/374 - 2s - loss: 0.0073 - val_loss: 0.0068 - 2s/epoch - 6ms/step
Epoch 28/175
374/374 - 2s - loss: 0.0068 - val_loss: 0.0068 - 2s/epoch - 6ms/step
Epoch 29/175
374/374 - 2s - loss: 0.0068 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 30/175
374/374 - 2s - loss: 0.0068 - val_loss: 0.0085 - 2s/epoch - 6ms/step
Epoch 31/175
374/374 - 2s - loss: 0.0086 - val_loss: 0.0069 - 2s/epoch - 6ms/step
Epoch 32/175
374/374 - 2s - loss: 0.0069 - val_loss: 0.0068 - 2s/epoch - 6ms/step
Epoch 33/175
374/374 - 2s - loss: 0.0068 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 34/175
374/374 - 2s - loss: 0.0068 - val_loss: 0.0069 - 2s/epoch - 6ms/step
Epoch 35/175
374/374 - 2s - loss: 0.0069 - val_loss: 0.0068 - 2s/epoch - 6ms/step
Epoch 36/175
374/374 - 2s - loss: 0.0068 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 37/175
374/374 - 2s - loss: 0.0068 - val_loss: 0.0068 - 2s/epoch - 6ms/step
Epoch 38/175
374/374 - 2s - loss: 0.0068 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 39/175
374/374 - 2s - loss: 0.0066 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 40/175
374/374 - 2s - loss: 0.0065 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 41/175
374/374 - 2s - loss: 0.0064 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 42/175
374/374 - 2s - loss: 0.0064 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 43/175
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 44/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 45/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 46/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 47/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 48/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 49/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 50/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 51/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 52/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 53/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 54/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 55/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 56/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 57/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 58/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 59/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 60/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 61/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 62/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 63/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 64/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 65/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 66/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 67/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 68/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 69/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 70/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 71/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 72/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 73/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 74/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 75/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 76/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 77/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 78/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 79/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 80/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 81/175
374/374 - 2s - loss: 0.0065 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 82/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 83/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 84/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 85/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 86/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 87/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 88/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 89/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 90/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 91/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 92/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 93/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 94/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 95/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 96/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 97/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 98/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 99/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 100/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 101/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 102/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 103/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 104/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 105/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 106/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 107/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 108/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 109/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 110/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 111/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 112/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 113/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 114/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 115/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 116/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 117/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 118/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 119/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 120/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 121/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 122/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 123/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 124/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 125/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 126/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 127/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 128/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 129/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 130/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 131/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 132/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 133/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 134/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 135/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 136/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 137/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 138/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 139/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 140/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 141/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 142/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 143/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 144/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 145/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 146/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 147/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 148/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 149/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 150/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 151/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 152/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 153/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 154/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 155/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 156/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 157/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 158/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 159/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 160/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 161/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 162/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 163/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 164/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 165/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 166/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 167/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 168/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 169/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 170/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 171/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 172/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 173/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 174/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 175/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.006016773637384176
  1/332 [..............................] - ETA: 46s 38/332 [==>...........................] - ETA: 0s  77/332 [=====>........................] - ETA: 0s116/332 [=========>....................] - ETA: 0s155/332 [=============>................] - ETA: 0s194/332 [================>.............] - ETA: 0s226/332 [===================>..........] - ETA: 0s265/332 [======================>.......] - ETA: 0s303/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.12532544246977606
cosine 0.0980601662055888
MAE: 0.046867732
RMSE: 0.100684695
r2: 0.3423618031329261
RMSE zero-vector: 0.23411466903540806
['2.5custom_VAE', 'logcosh', 256, 175, 0.0018, 0.6, 758, 0.006069862749427557, 0.006016773637384176, 0.12532544246977606, 0.0980601662055888, 0.0468677319586277, 0.10068469494581223, 0.3423618031329261, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 170 0.0018 128 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1769)        7076        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1769)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4165381     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,093,562
Trainable params: 9,084,970
Non-trainable params: 8,592
__________________________________________________________________________________________________
Epoch 1/170
747/747 - 7s - loss: 0.0303 - val_loss: 0.0254 - 7s/epoch - 9ms/step
Epoch 2/170
747/747 - 4s - loss: 0.0162 - val_loss: 0.0254 - 4s/epoch - 5ms/step
Epoch 3/170
747/747 - 4s - loss: 0.0148 - val_loss: 0.0157 - 4s/epoch - 5ms/step
Epoch 4/170
747/747 - 4s - loss: 0.0141 - val_loss: 0.0169 - 4s/epoch - 5ms/step
Epoch 5/170
747/747 - 4s - loss: 0.0136 - val_loss: 0.0137 - 4s/epoch - 5ms/step
Epoch 6/170
747/747 - 4s - loss: 0.0131 - val_loss: 0.0132 - 4s/epoch - 5ms/step
Epoch 7/170
747/747 - 4s - loss: 0.0128 - val_loss: 0.0126 - 4s/epoch - 5ms/step
Epoch 8/170
747/747 - 4s - loss: 0.0125 - val_loss: 0.0123 - 4s/epoch - 5ms/step
Epoch 9/170
747/747 - 4s - loss: 0.0123 - val_loss: 0.0121 - 4s/epoch - 5ms/step
Epoch 10/170
747/747 - 4s - loss: 0.0120 - val_loss: 0.0117 - 4s/epoch - 5ms/step
Epoch 11/170
747/747 - 4s - loss: 0.0117 - val_loss: 0.0114 - 4s/epoch - 5ms/step
Epoch 12/170
747/747 - 4s - loss: 0.0115 - val_loss: 0.0115 - 4s/epoch - 5ms/step
Epoch 13/170
747/747 - 4s - loss: 0.0115 - val_loss: 0.0112 - 4s/epoch - 5ms/step
Epoch 14/170
747/747 - 4s - loss: 0.0113 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 15/170
747/747 - 4s - loss: 0.0112 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 16/170
747/747 - 4s - loss: 0.0113 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 17/170
747/747 - 4s - loss: 0.0111 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 18/170
747/747 - 4s - loss: 0.0112 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 19/170
747/747 - 4s - loss: 0.0113 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 20/170
747/747 - 4s - loss: 0.0111 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 21/170
747/747 - 4s - loss: 0.0111 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 22/170
747/747 - 4s - loss: 0.0111 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 23/170
747/747 - 4s - loss: 0.0110 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 24/170
747/747 - 4s - loss: 0.0112 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 25/170
747/747 - 4s - loss: 0.0112 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 26/170
747/747 - 4s - loss: 0.0110 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 27/170
747/747 - 4s - loss: 0.0113 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 28/170
747/747 - 4s - loss: 0.0109 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 29/170
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 30/170
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 31/170
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 32/170
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 33/170
747/747 - 4s - loss: 0.0108 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 34/170
747/747 - 4s - loss: 0.0108 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 35/170
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 36/170
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 37/170
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 38/170
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 39/170
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 40/170
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 41/170
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 42/170
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 43/170
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 44/170
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 45/170
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 46/170
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 47/170
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 48/170
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 49/170
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 50/170
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 51/170
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 52/170
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 53/170
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 54/170
747/747 - 4s - loss: 0.0106 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 55/170
747/747 - 4s - loss: 0.0105 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 56/170
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 57/170
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 58/170
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 59/170
747/747 - 4s - loss: 0.0104 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 60/170
747/747 - 4s - loss: 0.0104 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 61/170
747/747 - 4s - loss: 0.0104 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 62/170
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 63/170
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 64/170
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 65/170
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 66/170
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 67/170
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 68/170
747/747 - 4s - loss: 0.0102 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 69/170
747/747 - 4s - loss: 0.0102 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 70/170
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 71/170
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 72/170
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 73/170
747/747 - 4s - loss: 0.0102 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 74/170
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 75/170
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 76/170
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 77/170
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 78/170
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 79/170
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 80/170
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 81/170
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 82/170
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 83/170
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 84/170
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 85/170
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 86/170
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 87/170
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 88/170
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 89/170
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 90/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 91/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 92/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 93/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 94/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 95/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 96/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 97/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 98/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 99/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 100/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 101/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 102/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 103/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 104/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 105/170
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 106/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 107/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 108/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 109/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 110/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 111/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 112/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 113/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 114/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 115/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 116/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 117/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 118/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 119/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 120/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 121/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 122/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 123/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 124/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 125/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 126/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 127/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 128/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 129/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 130/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 131/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 132/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 133/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 134/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 135/170
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 136/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 137/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 138/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 139/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 140/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 141/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 142/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 143/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 144/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 145/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 146/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 147/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 148/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 149/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 150/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 151/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 152/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 153/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 154/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 155/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 156/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 157/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 158/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 159/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 160/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 161/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 162/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 163/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 164/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 165/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 166/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 167/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 168/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 169/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 170/170
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009508742019534111
  1/332 [..............................] - ETA: 48s 39/332 [==>...........................] - ETA: 0s  78/332 [======>.......................] - ETA: 0s117/332 [=========>....................] - ETA: 0s156/332 [=============>................] - ETA: 0s194/332 [================>.............] - ETA: 0s232/332 [===================>..........] - ETA: 0s270/332 [=======================>......] - ETA: 0s296/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.07745996924714686
cosine 0.06080671261379596
MAE: 0.036605533
RMSE: 0.0799624
r2: 0.5852067525380901
RMSE zero-vector: 0.23411466903540806
['1.4custom_VAE', 'mse', 128, 170, 0.0018, 0.6, 758, 0.009664739482104778, 0.009508742019534111, 0.07745996924714686, 0.06080671261379596, 0.036605533212423325, 0.07996240258216858, 0.5852067525380901, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.2999999999999998 180 0.002 128 2] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1643)         2078395     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1643)        6572        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1643)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3909979     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,487,250
Trainable params: 8,479,162
Non-trainable params: 8,088
__________________________________________________________________________________________________
Epoch 1/180
747/747 - 9s - loss: 0.0178 - val_loss: 0.0101 - 9s/epoch - 12ms/step
Epoch 2/180
747/747 - 4s - loss: 0.0082 - val_loss: 0.0090 - 4s/epoch - 5ms/step
Epoch 3/180
747/747 - 4s - loss: 0.0079 - val_loss: 0.0082 - 4s/epoch - 5ms/step
Epoch 4/180
747/747 - 4s - loss: 0.0076 - val_loss: 0.0083 - 4s/epoch - 5ms/step
Epoch 5/180
747/747 - 4s - loss: 0.0074 - val_loss: 0.0076 - 4s/epoch - 5ms/step
Epoch 6/180
747/747 - 4s - loss: 0.0072 - val_loss: 0.0075 - 4s/epoch - 5ms/step
Epoch 7/180
747/747 - 4s - loss: 0.0070 - val_loss: 0.0070 - 4s/epoch - 5ms/step
Epoch 8/180
747/747 - 4s - loss: 0.0069 - val_loss: 0.0068 - 4s/epoch - 5ms/step
Epoch 9/180
747/747 - 4s - loss: 0.0068 - val_loss: 0.0068 - 4s/epoch - 5ms/step
Epoch 10/180
747/747 - 4s - loss: 0.0068 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 11/180
747/747 - 4s - loss: 0.0068 - val_loss: 0.0070 - 4s/epoch - 5ms/step
Epoch 12/180
747/747 - 4s - loss: 0.0068 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 13/180
747/747 - 4s - loss: 0.0067 - val_loss: 0.0068 - 4s/epoch - 5ms/step
Epoch 14/180
747/747 - 4s - loss: 0.0068 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 15/180
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 16/180
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 17/180
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 18/180
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 19/180
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 20/180
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 21/180
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 22/180
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 23/180
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 24/180
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 25/180
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 26/180
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 27/180
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 28/180
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 29/180
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 30/180
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 31/180
747/747 - 4s - loss: 0.0066 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 32/180
747/747 - 4s - loss: 0.0066 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 33/180
747/747 - 4s - loss: 0.0066 - val_loss: 0.0064 - 4s/epoch - 5ms/step
Epoch 34/180
747/747 - 4s - loss: 0.0064 - val_loss: 0.0063 - 4s/epoch - 5ms/step
Epoch 35/180
747/747 - 4s - loss: 0.0063 - val_loss: 0.0063 - 4s/epoch - 5ms/step
Epoch 36/180
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 37/180
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 38/180
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 39/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 40/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 41/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 42/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 43/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 44/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 45/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 46/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 47/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 48/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 49/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 50/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 51/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 52/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 53/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 54/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 55/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 56/180
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 57/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 58/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 59/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 60/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 61/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 62/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 63/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 64/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 65/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 66/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 67/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 68/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 69/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 70/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 71/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 72/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 73/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 74/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 75/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 76/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 77/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 78/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 79/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 80/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 81/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 82/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 83/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 84/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 85/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 86/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 87/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 88/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 89/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 90/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 91/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 92/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 93/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 94/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 95/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 96/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 97/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 98/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 99/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 100/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 101/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 102/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 103/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 104/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 105/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 106/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 107/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 108/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 109/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 110/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 111/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 112/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 113/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 114/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 115/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 116/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 117/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 118/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 119/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 120/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 121/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 122/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 123/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 124/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 125/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 126/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 127/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 128/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 129/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 130/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 131/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 132/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 133/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 134/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 135/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 136/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 137/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 138/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 139/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 140/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 141/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 142/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 143/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 144/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 145/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 146/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 147/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 148/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 149/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 150/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 151/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 152/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 153/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 154/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 155/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 156/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 157/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 158/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 159/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 160/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 161/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 162/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 163/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 164/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 165/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 166/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 167/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 168/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 169/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 170/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 171/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 172/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 173/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 174/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 175/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 176/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 177/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 178/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 179/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 180/180
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.006025119684636593
  1/332 [..............................] - ETA: 1:19 23/332 [=>............................] - ETA: 0s   45/332 [===>..........................] - ETA: 0s 67/332 [=====>........................] - ETA: 0s 89/332 [=======>......................] - ETA: 0s112/332 [=========>....................] - ETA: 0s135/332 [===========>..................] - ETA: 0s158/332 [=============>................] - ETA: 0s179/332 [===============>..............] - ETA: 0s201/332 [=================>............] - ETA: 0s222/332 [===================>..........] - ETA: 0s245/332 [=====================>........] - ETA: 0s268/332 [=======================>......] - ETA: 0s290/332 [=========================>....] - ETA: 0s313/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.12394184670862143
cosine 0.09694578553364945
MAE: 0.04682656
RMSE: 0.100158304
r2: 0.3492203009386579
RMSE zero-vector: 0.23411466903540806
['1.2999999999999998custom_VAE', 'logcosh', 128, 180, 0.002, 0.6, 758, 0.006071715150028467, 0.006025119684636593, 0.12394184670862143, 0.09694578553364945, 0.0468265600502491, 0.10015830397605896, 0.3492203009386579, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.6 175 0.002 64 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3286)         4156790     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3286)        13144       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3286)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7240340     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,393,366
Trainable params: 16,378,706
Non-trainable params: 14,660
__________________________________________________________________________________________________
Epoch 1/175
1493/1493 - 10s - loss: 0.0320 - val_loss: 0.0197 - 10s/epoch - 7ms/step
Epoch 2/175
1493/1493 - 7s - loss: 0.0164 - val_loss: 0.0152 - 7s/epoch - 5ms/step
Epoch 3/175
1493/1493 - 7s - loss: 0.0137 - val_loss: 0.0130 - 7s/epoch - 5ms/step
Epoch 4/175
1493/1493 - 7s - loss: 0.0126 - val_loss: 0.0119 - 7s/epoch - 5ms/step
Epoch 5/175
1493/1493 - 7s - loss: 0.0120 - val_loss: 0.0116 - 7s/epoch - 5ms/step
Epoch 6/175
1493/1493 - 7s - loss: 0.0118 - val_loss: 0.0114 - 7s/epoch - 5ms/step
Epoch 7/175
1493/1493 - 7s - loss: 0.0116 - val_loss: 0.0114 - 7s/epoch - 5ms/step
Epoch 8/175
1493/1493 - 7s - loss: 0.0115 - val_loss: 0.0112 - 7s/epoch - 5ms/step
Epoch 9/175
1493/1493 - 7s - loss: 0.0114 - val_loss: 0.0111 - 7s/epoch - 5ms/step
Epoch 10/175
1493/1493 - 7s - loss: 0.0113 - val_loss: 0.0111 - 7s/epoch - 5ms/step
Epoch 11/175
1493/1493 - 7s - loss: 0.0112 - val_loss: 0.0110 - 7s/epoch - 5ms/step
Epoch 12/175
1493/1493 - 7s - loss: 0.0112 - val_loss: 0.0109 - 7s/epoch - 5ms/step
Epoch 13/175
1493/1493 - 7s - loss: 0.0112 - val_loss: 0.0109 - 7s/epoch - 5ms/step
Epoch 14/175
1493/1493 - 7s - loss: 0.0111 - val_loss: 0.0109 - 7s/epoch - 5ms/step
Epoch 15/175
1493/1493 - 7s - loss: 0.0111 - val_loss: 0.0109 - 7s/epoch - 5ms/step
Epoch 16/175
1493/1493 - 7s - loss: 0.0111 - val_loss: 0.0108 - 7s/epoch - 5ms/step
Epoch 17/175
1493/1493 - 7s - loss: 0.0111 - val_loss: 0.0108 - 7s/epoch - 5ms/step
Epoch 18/175
1493/1493 - 7s - loss: 0.0110 - val_loss: 0.0108 - 7s/epoch - 5ms/step
Epoch 19/175
1493/1493 - 7s - loss: 0.0110 - val_loss: 0.0108 - 7s/epoch - 5ms/step
Epoch 20/175
1493/1493 - 7s - loss: 0.0110 - val_loss: 0.0108 - 7s/epoch - 5ms/step
Epoch 21/175
1493/1493 - 7s - loss: 0.0110 - val_loss: 0.0108 - 7s/epoch - 5ms/step
Epoch 22/175
1493/1493 - 7s - loss: 0.0110 - val_loss: 0.0107 - 7s/epoch - 5ms/step
Epoch 23/175
1493/1493 - 7s - loss: 0.0110 - val_loss: 0.0107 - 7s/epoch - 5ms/step
Epoch 24/175
1493/1493 - 7s - loss: 0.0110 - val_loss: 0.0108 - 7s/epoch - 5ms/step
Epoch 25/175
1493/1493 - 7s - loss: 0.0109 - val_loss: 0.0107 - 7s/epoch - 5ms/step
Epoch 26/175
1493/1493 - 7s - loss: 0.0109 - val_loss: 0.0107 - 7s/epoch - 5ms/step
Epoch 27/175
1493/1493 - 7s - loss: 0.0109 - val_loss: 0.0106 - 7s/epoch - 5ms/step
Epoch 28/175
1493/1493 - 7s - loss: 0.0109 - val_loss: 0.0107 - 7s/epoch - 5ms/step
Epoch 29/175
1493/1493 - 7s - loss: 0.0109 - val_loss: 0.0107 - 7s/epoch - 5ms/step
Epoch 30/175
1493/1493 - 7s - loss: 0.0109 - val_loss: 0.0107 - 7s/epoch - 5ms/step
Epoch 31/175
1493/1493 - 7s - loss: 0.0109 - val_loss: 0.0107 - 7s/epoch - 5ms/step
Epoch 32/175
1493/1493 - 7s - loss: 0.0109 - val_loss: 0.0106 - 7s/epoch - 5ms/step
Epoch 33/175
1493/1493 - 7s - loss: 0.0108 - val_loss: 0.0106 - 7s/epoch - 5ms/step
Epoch 34/175
1493/1493 - 7s - loss: 0.0108 - val_loss: 0.0107 - 7s/epoch - 5ms/step
Epoch 35/175
1493/1493 - 7s - loss: 0.0108 - val_loss: 0.0107 - 7s/epoch - 5ms/step
Epoch 36/175
1493/1493 - 7s - loss: 0.0108 - val_loss: 0.0107 - 7s/epoch - 5ms/step
Epoch 37/175
1493/1493 - 7s - loss: 0.0108 - val_loss: 0.0107 - 7s/epoch - 5ms/step
Epoch 38/175
1493/1493 - 7s - loss: 0.0108 - val_loss: 0.0106 - 7s/epoch - 5ms/step
Epoch 39/175
1493/1493 - 7s - loss: 0.0108 - val_loss: 0.0107 - 7s/epoch - 5ms/step
Epoch 40/175
1493/1493 - 7s - loss: 0.0108 - val_loss: 0.0107 - 7s/epoch - 5ms/step
Epoch 41/175
1493/1493 - 7s - loss: 0.0108 - val_loss: 0.0107 - 7s/epoch - 5ms/step
Epoch 42/175
1493/1493 - 7s - loss: 0.0108 - val_loss: 0.0107 - 7s/epoch - 5ms/step
Epoch 43/175
1493/1493 - 7s - loss: 0.0108 - val_loss: 0.0106 - 7s/epoch - 5ms/step
Epoch 44/175
1493/1493 - 7s - loss: 0.0108 - val_loss: 0.0106 - 7s/epoch - 5ms/step
Epoch 45/175
1493/1493 - 7s - loss: 0.0108 - val_loss: 0.0106 - 7s/epoch - 5ms/step
Epoch 46/175
1493/1493 - 7s - loss: 0.0107 - val_loss: 0.0105 - 7s/epoch - 5ms/step
Epoch 47/175
1493/1493 - 7s - loss: 0.0107 - val_loss: 0.0105 - 7s/epoch - 5ms/step
Epoch 48/175
1493/1493 - 7s - loss: 0.0107 - val_loss: 0.0105 - 7s/epoch - 5ms/step
Epoch 49/175
1493/1493 - 7s - loss: 0.0106 - val_loss: 0.0105 - 7s/epoch - 5ms/step
Epoch 50/175
1493/1493 - 7s - loss: 0.0106 - val_loss: 0.0105 - 7s/epoch - 5ms/step
Epoch 51/175
1493/1493 - 7s - loss: 0.0106 - val_loss: 0.0104 - 7s/epoch - 5ms/step
Epoch 52/175
1493/1493 - 7s - loss: 0.0106 - val_loss: 0.0104 - 7s/epoch - 5ms/step
Epoch 53/175
1493/1493 - 7s - loss: 0.0106 - val_loss: 0.0104 - 7s/epoch - 5ms/step
Epoch 54/175
1493/1493 - 7s - loss: 0.0106 - val_loss: 0.0104 - 7s/epoch - 5ms/step
Epoch 55/175
1493/1493 - 7s - loss: 0.0105 - val_loss: 0.0104 - 7s/epoch - 5ms/step
Epoch 56/175
1493/1493 - 7s - loss: 0.0105 - val_loss: 0.0103 - 7s/epoch - 5ms/step
Epoch 57/175
1493/1493 - 7s - loss: 0.0105 - val_loss: 0.0103 - 7s/epoch - 5ms/step
Epoch 58/175
1493/1493 - 7s - loss: 0.0105 - val_loss: 0.0103 - 7s/epoch - 5ms/step
Epoch 59/175
1493/1493 - 7s - loss: 0.0105 - val_loss: 0.0103 - 7s/epoch - 5ms/step
Epoch 60/175
1493/1493 - 7s - loss: 0.0104 - val_loss: 0.0103 - 7s/epoch - 5ms/step
Epoch 61/175
1493/1493 - 7s - loss: 0.0104 - val_loss: 0.0102 - 7s/epoch - 5ms/step
Epoch 62/175
1493/1493 - 7s - loss: 0.0104 - val_loss: 0.0102 - 7s/epoch - 5ms/step
Epoch 63/175
1493/1493 - 7s - loss: 0.0104 - val_loss: 0.0102 - 7s/epoch - 5ms/step
Epoch 64/175
1493/1493 - 7s - loss: 0.0104 - val_loss: 0.0102 - 7s/epoch - 5ms/step
Epoch 65/175
1493/1493 - 7s - loss: 0.0104 - val_loss: 0.0102 - 7s/epoch - 5ms/step
Epoch 66/175
1493/1493 - 7s - loss: 0.0104 - val_loss: 0.0102 - 7s/epoch - 5ms/step
Epoch 67/175
1493/1493 - 7s - loss: 0.0104 - val_loss: 0.0101 - 7s/epoch - 5ms/step
Epoch 68/175
1493/1493 - 7s - loss: 0.0104 - val_loss: 0.0102 - 7s/epoch - 5ms/step
Epoch 69/175
1493/1493 - 7s - loss: 0.0103 - val_loss: 0.0102 - 7s/epoch - 5ms/step
Epoch 70/175
1493/1493 - 7s - loss: 0.0103 - val_loss: 0.0101 - 7s/epoch - 5ms/step
Epoch 71/175
1493/1493 - 7s - loss: 0.0103 - val_loss: 0.0102 - 7s/epoch - 5ms/step
Epoch 72/175
1493/1493 - 7s - loss: 0.0103 - val_loss: 0.0101 - 7s/epoch - 5ms/step
Epoch 73/175
1493/1493 - 7s - loss: 0.0103 - val_loss: 0.0102 - 7s/epoch - 5ms/step
Epoch 74/175
1493/1493 - 7s - loss: 0.0103 - val_loss: 0.0101 - 7s/epoch - 5ms/step
Epoch 75/175
1493/1493 - 7s - loss: 0.0103 - val_loss: 0.0101 - 7s/epoch - 5ms/step
Epoch 76/175
1493/1493 - 7s - loss: 0.0103 - val_loss: 0.0101 - 7s/epoch - 5ms/step
Epoch 77/175
1493/1493 - 7s - loss: 0.0103 - val_loss: 0.0101 - 7s/epoch - 5ms/step
Epoch 78/175
1493/1493 - 7s - loss: 0.0103 - val_loss: 0.0101 - 7s/epoch - 5ms/step
Epoch 79/175
1493/1493 - 7s - loss: 0.0103 - val_loss: 0.0101 - 7s/epoch - 5ms/step
Epoch 80/175
1493/1493 - 7s - loss: 0.0103 - val_loss: 0.0100 - 7s/epoch - 5ms/step
Epoch 81/175
1493/1493 - 7s - loss: 0.0102 - val_loss: 0.0101 - 7s/epoch - 5ms/step
Epoch 82/175
1493/1493 - 7s - loss: 0.0102 - val_loss: 0.0101 - 7s/epoch - 5ms/step
Epoch 83/175
1493/1493 - 7s - loss: 0.0102 - val_loss: 0.0101 - 7s/epoch - 5ms/step
Epoch 84/175
1493/1493 - 7s - loss: 0.0102 - val_loss: 0.0100 - 7s/epoch - 5ms/step
Epoch 85/175
1493/1493 - 7s - loss: 0.0102 - val_loss: 0.0100 - 7s/epoch - 5ms/step
Epoch 86/175
1493/1493 - 7s - loss: 0.0102 - val_loss: 0.0100 - 7s/epoch - 5ms/step
Epoch 87/175
1493/1493 - 7s - loss: 0.0102 - val_loss: 0.0100 - 7s/epoch - 5ms/step
Epoch 88/175
1493/1493 - 7s - loss: 0.0102 - val_loss: 0.0100 - 7s/epoch - 5ms/step
Epoch 89/175
1493/1493 - 7s - loss: 0.0102 - val_loss: 0.0100 - 7s/epoch - 5ms/step
Epoch 90/175
1493/1493 - 7s - loss: 0.0101 - val_loss: 0.0100 - 7s/epoch - 5ms/step
Epoch 91/175
1493/1493 - 7s - loss: 0.0101 - val_loss: 0.0099 - 7s/epoch - 5ms/step
Epoch 92/175
1493/1493 - 7s - loss: 0.0101 - val_loss: 0.0099 - 7s/epoch - 5ms/step
Epoch 93/175
1493/1493 - 7s - loss: 0.0101 - val_loss: 0.0100 - 7s/epoch - 5ms/step
Epoch 94/175
1493/1493 - 7s - loss: 0.0101 - val_loss: 0.0099 - 7s/epoch - 5ms/step
Epoch 95/175
1493/1493 - 7s - loss: 0.0101 - val_loss: 0.0099 - 7s/epoch - 5ms/step
Epoch 96/175
1493/1493 - 7s - loss: 0.0101 - val_loss: 0.0099 - 7s/epoch - 5ms/step
Epoch 97/175
1493/1493 - 7s - loss: 0.0101 - val_loss: 0.0099 - 7s/epoch - 5ms/step
Epoch 98/175
1493/1493 - 7s - loss: 0.0101 - val_loss: 0.0098 - 7s/epoch - 5ms/step
Epoch 99/175
1493/1493 - 7s - loss: 0.0100 - val_loss: 0.0099 - 7s/epoch - 5ms/step
Epoch 100/175
1493/1493 - 7s - loss: 0.0100 - val_loss: 0.0098 - 7s/epoch - 5ms/step
Epoch 101/175
1493/1493 - 7s - loss: 0.0100 - val_loss: 0.0098 - 7s/epoch - 5ms/step
Epoch 102/175
1493/1493 - 7s - loss: 0.0100 - val_loss: 0.0098 - 7s/epoch - 5ms/step
Epoch 103/175
1493/1493 - 7s - loss: 0.0100 - val_loss: 0.0098 - 7s/epoch - 5ms/step
Epoch 104/175
1493/1493 - 7s - loss: 0.0100 - val_loss: 0.0099 - 7s/epoch - 5ms/step
Epoch 105/175
1493/1493 - 7s - loss: 0.0100 - val_loss: 0.0098 - 7s/epoch - 5ms/step
Epoch 106/175
1493/1493 - 7s - loss: 0.0100 - val_loss: 0.0098 - 7s/epoch - 5ms/step
Epoch 107/175
1493/1493 - 7s - loss: 0.0100 - val_loss: 0.0097 - 7s/epoch - 5ms/step
Epoch 108/175
1493/1493 - 7s - loss: 0.0100 - val_loss: 0.0097 - 7s/epoch - 5ms/step
Epoch 109/175
1493/1493 - 7s - loss: 0.0100 - val_loss: 0.0098 - 7s/epoch - 5ms/step
Epoch 110/175
1493/1493 - 7s - loss: 0.0100 - val_loss: 0.0098 - 7s/epoch - 5ms/step
Epoch 111/175
1493/1493 - 7s - loss: 0.0100 - val_loss: 0.0097 - 7s/epoch - 5ms/step
Epoch 112/175
1493/1493 - 7s - loss: 0.0100 - val_loss: 0.0097 - 7s/epoch - 5ms/step
Epoch 113/175
1493/1493 - 7s - loss: 0.0100 - val_loss: 0.0097 - 7s/epoch - 5ms/step
Epoch 114/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 5ms/step
Epoch 115/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 5ms/step
Epoch 116/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 5ms/step
Epoch 117/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 5ms/step
Epoch 118/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 5ms/step
Epoch 119/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 5ms/step
Epoch 120/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 5ms/step
Epoch 121/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 5ms/step
Epoch 122/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 5ms/step
Epoch 123/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 5ms/step
Epoch 124/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 5ms/step
Epoch 125/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 5ms/step
Epoch 126/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 5ms/step
Epoch 127/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 5ms/step
Epoch 128/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 129/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 5ms/step
Epoch 130/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 131/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 132/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 133/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 134/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 135/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 136/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 137/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 138/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 5ms/step
Epoch 139/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 140/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 141/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 142/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 143/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 144/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 145/175
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 146/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 147/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 148/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 149/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 150/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 151/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 152/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 153/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 154/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 155/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 156/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 157/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 158/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0095 - 7s/epoch - 5ms/step
Epoch 159/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 160/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 161/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 162/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 163/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 164/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 165/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 166/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 167/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 168/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0095 - 7s/epoch - 5ms/step
Epoch 169/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 170/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 171/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 172/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 173/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 174/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 175/175
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009590797126293182
  1/332 [..............................] - ETA: 50s 38/332 [==>...........................] - ETA: 0s  76/332 [=====>........................] - ETA: 0s111/332 [=========>....................] - ETA: 0s149/332 [============>.................] - ETA: 0s181/332 [===============>..............] - ETA: 0s219/332 [==================>...........] - ETA: 0s258/332 [======================>.......] - ETA: 0s296/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.07970616760633148
cosine 0.06260363384098365
MAE: 0.037053447
RMSE: 0.08115428
r2: 0.5727491273526648
RMSE zero-vector: 0.23411466903540806
['2.6custom_VAE', 'mse', 64, 175, 0.002, 0.6, 758, 0.009780504740774632, 0.009590797126293182, 0.07970616760633148, 0.06260363384098365, 0.03705344721674919, 0.08115427941083908, 0.5727491273526648, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.6 175 0.0018 256 2] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3286)         4156790     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3286)        13144       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3286)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7240340     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,393,366
Trainable params: 16,378,706
Non-trainable params: 14,660
__________________________________________________________________________________________________
Epoch 1/175
374/374 - 5s - loss: 0.0317 - val_loss: 0.1930 - 5s/epoch - 14ms/step
Epoch 2/175
374/374 - 2s - loss: 0.0113 - val_loss: 0.0127 - 2s/epoch - 6ms/step
Epoch 3/175
374/374 - 2s - loss: 0.0091 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 4/175
374/374 - 2s - loss: 0.0157 - val_loss: 0.0169 - 2s/epoch - 6ms/step
Epoch 5/175
374/374 - 2s - loss: 0.0081 - val_loss: 0.0086 - 2s/epoch - 6ms/step
Epoch 6/175
374/374 - 2s - loss: 0.0077 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 7/175
374/374 - 2s - loss: 0.0076 - val_loss: 0.0087 - 2s/epoch - 6ms/step
Epoch 8/175
374/374 - 2s - loss: 0.0076 - val_loss: 0.0082 - 2s/epoch - 6ms/step
Epoch 9/175
374/374 - 2s - loss: 0.0073 - val_loss: 0.0091 - 2s/epoch - 6ms/step
Epoch 10/175
374/374 - 2s - loss: 0.0073 - val_loss: 0.0080 - 2s/epoch - 6ms/step
Epoch 11/175
374/374 - 2s - loss: 0.0071 - val_loss: 0.0087 - 2s/epoch - 6ms/step
Epoch 12/175
374/374 - 2s - loss: 0.0069 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 13/175
374/374 - 2s - loss: 0.0069 - val_loss: 0.0070 - 2s/epoch - 6ms/step
Epoch 14/175
374/374 - 2s - loss: 0.0065 - val_loss: 0.0069 - 2s/epoch - 6ms/step
Epoch 15/175
374/374 - 2s - loss: 0.0066 - val_loss: 0.0143 - 2s/epoch - 6ms/step
Epoch 16/175
374/374 - 2s - loss: 0.0066 - val_loss: 0.0069 - 2s/epoch - 6ms/step
Epoch 17/175
374/374 - 2s - loss: 0.0065 - val_loss: 0.0077 - 2s/epoch - 6ms/step
Epoch 18/175
374/374 - 2s - loss: 0.0070 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 19/175
374/374 - 2s - loss: 0.0068 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 20/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 21/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 22/175
374/374 - 2s - loss: 0.0064 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 23/175
374/374 - 2s - loss: 0.0065 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 24/175
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 25/175
374/374 - 2s - loss: 0.0067 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 26/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 27/175
374/374 - 2s - loss: 0.0066 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 28/175
374/374 - 2s - loss: 0.0069 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 29/175
374/374 - 2s - loss: 0.0078 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 30/175
374/374 - 2s - loss: 0.0067 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 31/175
374/374 - 2s - loss: 0.0082 - val_loss: 0.0071 - 2s/epoch - 6ms/step
Epoch 32/175
374/374 - 2s - loss: 0.0092 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 33/175
374/374 - 2s - loss: 0.0066 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 34/175
374/374 - 2s - loss: 0.0066 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 35/175
374/374 - 2s - loss: 0.0065 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 36/175
374/374 - 2s - loss: 0.0065 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 37/175
374/374 - 2s - loss: 0.0069 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 38/175
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 39/175
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 40/175
374/374 - 2s - loss: 0.0065 - val_loss: 0.0073 - 2s/epoch - 6ms/step
Epoch 41/175
374/374 - 2s - loss: 0.0079 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 42/175
374/374 - 2s - loss: 0.0065 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 43/175
374/374 - 2s - loss: 0.0064 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 44/175
374/374 - 2s - loss: 0.0066 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 45/175
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 46/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 47/175
374/374 - 2s - loss: 0.0067 - val_loss: 0.0089 - 2s/epoch - 6ms/step
Epoch 48/175
374/374 - 2s - loss: 0.0092 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 49/175
374/374 - 2s - loss: 0.0066 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 50/175
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 51/175
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 52/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 53/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 54/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 55/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0069 - 2s/epoch - 6ms/step
Epoch 56/175
374/374 - 2s - loss: 0.0069 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 57/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 58/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 59/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 60/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 61/175
374/374 - 2s - loss: 0.0066 - val_loss: 0.0079 - 2s/epoch - 6ms/step
Epoch 62/175
374/374 - 2s - loss: 0.0098 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 63/175
374/374 - 2s - loss: 0.0065 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 64/175
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 65/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 66/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0072 - 2s/epoch - 6ms/step
Epoch 67/175
374/374 - 2s - loss: 0.0074 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 68/175
374/374 - 2s - loss: 0.0064 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 69/175
374/374 - 2s - loss: 0.0066 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 70/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 71/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 72/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 73/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 74/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0070 - 2s/epoch - 6ms/step
Epoch 75/175
374/374 - 2s - loss: 0.0074 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 76/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 77/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 78/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 79/175
374/374 - 2s - loss: 0.0066 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 80/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 81/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 82/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0078 - 2s/epoch - 6ms/step
Epoch 83/175
374/374 - 2s - loss: 0.0090 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 84/175
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 85/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 86/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 87/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 88/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 89/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 90/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 91/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 92/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 93/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 94/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 95/175
374/374 - 2s - loss: 0.0064 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 96/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 97/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 98/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 99/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 100/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0073 - 2s/epoch - 6ms/step
Epoch 101/175
374/374 - 2s - loss: 0.0068 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 102/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 103/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 104/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 105/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 106/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 107/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 108/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 109/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 110/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 111/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 112/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 113/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 114/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 115/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 116/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 117/175
374/374 - 2s - loss: 0.0063 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 118/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 119/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 120/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 121/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 122/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 123/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 124/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 125/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 126/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 127/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 128/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 129/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 130/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 131/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 132/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 133/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 134/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 135/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 136/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 137/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 138/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 139/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 140/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 141/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 142/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 143/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 144/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 145/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 146/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 147/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 148/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 149/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 150/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 151/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 152/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 153/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 154/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 155/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 156/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 157/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 158/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 159/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 160/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 161/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 162/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 163/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 164/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 165/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 166/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 167/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 168/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 169/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 170/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 171/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 172/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 173/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 174/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 175/175
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.006045727990567684
  1/332 [..............................] - ETA: 1:17 17/332 [>.............................] - ETA: 1s   39/332 [==>...........................] - ETA: 0s 61/332 [====>.........................] - ETA: 0s 83/332 [======>.......................] - ETA: 0s105/332 [========>.....................] - ETA: 0s127/332 [==========>...................] - ETA: 0s149/332 [============>.................] - ETA: 0s171/332 [==============>...............] - ETA: 0s193/332 [================>.............] - ETA: 0s215/332 [==================>...........] - ETA: 0s232/332 [===================>..........] - ETA: 0s254/332 [=====================>........] - ETA: 0s274/332 [=======================>......] - ETA: 0s296/332 [=========================>....] - ETA: 0s319/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.12657243527873616
cosine 0.09904793594025506
MAE: 0.047246765
RMSE: 0.10110829
r2: 0.3368165979242468
RMSE zero-vector: 0.23411466903540806
['2.6custom_VAE', 'logcosh', 256, 175, 0.0018, 0.6, 758, 0.006084124557673931, 0.006045727990567684, 0.12657243527873616, 0.09904793594025506, 0.04724676534533501, 0.10110829025506973, 0.3368165979242468, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Saved GA instance to file: ./tmp//ga_instance_generation_3.pkl
[2.5 175 0.0018 128 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2396038     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2396038     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         6984938     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 15,787,054
Trainable params: 15,772,898
Non-trainable params: 14,156
__________________________________________________________________________________________________
Epoch 1/175
747/747 - 7s - loss: 0.0387 - val_loss: 0.0199 - 7s/epoch - 9ms/step
Epoch 2/175
747/747 - 4s - loss: 0.0171 - val_loss: 0.0178 - 4s/epoch - 5ms/step
Epoch 3/175
747/747 - 4s - loss: 0.0149 - val_loss: 0.0223 - 4s/epoch - 5ms/step
Epoch 4/175
747/747 - 4s - loss: 0.0142 - val_loss: 0.0226 - 4s/epoch - 5ms/step
Epoch 5/175
747/747 - 4s - loss: 0.0135 - val_loss: 0.0139 - 4s/epoch - 5ms/step
Epoch 6/175
747/747 - 4s - loss: 0.0130 - val_loss: 0.0398 - 4s/epoch - 5ms/step
Epoch 7/175
747/747 - 4s - loss: 0.0165 - val_loss: 0.0125 - 4s/epoch - 5ms/step
Epoch 8/175
747/747 - 4s - loss: 0.0126 - val_loss: 0.0124 - 4s/epoch - 5ms/step
Epoch 9/175
747/747 - 4s - loss: 0.0122 - val_loss: 0.0118 - 4s/epoch - 5ms/step
Epoch 10/175
747/747 - 4s - loss: 0.0118 - val_loss: 0.0116 - 4s/epoch - 5ms/step
Epoch 11/175
747/747 - 4s - loss: 0.0120 - val_loss: 0.0114 - 4s/epoch - 5ms/step
Epoch 12/175
747/747 - 4s - loss: 0.0115 - val_loss: 0.0113 - 4s/epoch - 5ms/step
Epoch 13/175
747/747 - 4s - loss: 0.0114 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 14/175
747/747 - 4s - loss: 0.0113 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 15/175
747/747 - 4s - loss: 0.0113 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 16/175
747/747 - 4s - loss: 0.0113 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 17/175
747/747 - 4s - loss: 0.0116 - val_loss: 0.0119 - 4s/epoch - 5ms/step
Epoch 18/175
747/747 - 4s - loss: 0.0126 - val_loss: 0.0120 - 4s/epoch - 5ms/step
Epoch 19/175
747/747 - 4s - loss: 0.0137 - val_loss: 0.0115 - 4s/epoch - 5ms/step
Epoch 20/175
747/747 - 4s - loss: 0.0116 - val_loss: 0.0113 - 4s/epoch - 5ms/step
Epoch 21/175
747/747 - 4s - loss: 0.0115 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 22/175
747/747 - 4s - loss: 0.0113 - val_loss: 0.0119 - 4s/epoch - 5ms/step
Epoch 23/175
747/747 - 4s - loss: 0.0122 - val_loss: 0.0125 - 4s/epoch - 5ms/step
Epoch 24/175
747/747 - 4s - loss: 0.0137 - val_loss: 0.0115 - 4s/epoch - 5ms/step
Epoch 25/175
747/747 - 4s - loss: 0.0114 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 26/175
747/747 - 4s - loss: 0.0113 - val_loss: 0.0112 - 4s/epoch - 5ms/step
Epoch 27/175
747/747 - 4s - loss: 0.0112 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 28/175
747/747 - 4s - loss: 0.0112 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 29/175
747/747 - 4s - loss: 0.0111 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 30/175
747/747 - 4s - loss: 0.0111 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 31/175
747/747 - 4s - loss: 0.0111 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 32/175
747/747 - 4s - loss: 0.0110 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 33/175
747/747 - 4s - loss: 0.0110 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 34/175
747/747 - 4s - loss: 0.0110 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 35/175
747/747 - 4s - loss: 0.0110 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 36/175
747/747 - 4s - loss: 0.0110 - val_loss: 0.0114 - 4s/epoch - 5ms/step
Epoch 37/175
747/747 - 4s - loss: 0.0115 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 38/175
747/747 - 4s - loss: 0.0110 - val_loss: 0.0113 - 4s/epoch - 5ms/step
Epoch 39/175
747/747 - 4s - loss: 0.0116 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 40/175
747/747 - 4s - loss: 0.0110 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 41/175
747/747 - 4s - loss: 0.0109 - val_loss: 0.0117 - 4s/epoch - 5ms/step
Epoch 42/175
747/747 - 4s - loss: 0.0116 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 43/175
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 44/175
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 45/175
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 46/175
747/747 - 4s - loss: 0.0108 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 47/175
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 48/175
747/747 - 4s - loss: 0.0108 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 49/175
747/747 - 4s - loss: 0.0108 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 50/175
747/747 - 4s - loss: 0.0108 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 51/175
747/747 - 4s - loss: 0.0109 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 52/175
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 53/175
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 54/175
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 55/175
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 56/175
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 57/175
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 58/175
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 59/175
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 60/175
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 61/175
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 62/175
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 63/175
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 64/175
747/747 - 4s - loss: 0.0106 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 65/175
747/747 - 4s - loss: 0.0106 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 66/175
747/747 - 4s - loss: 0.0106 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 67/175
747/747 - 4s - loss: 0.0105 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 68/175
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 69/175
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 70/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 71/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 72/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 73/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 74/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 75/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 76/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 77/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 78/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 79/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 80/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 81/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 82/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 83/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 84/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 85/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 86/175
747/747 - 4s - loss: 0.0102 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 87/175
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 88/175
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 89/175
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 90/175
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 91/175
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 92/175
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 93/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 94/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 95/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 96/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 97/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 98/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 99/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 100/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 101/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 102/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 103/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 104/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 105/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 106/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 107/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 108/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 109/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 110/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 111/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 112/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 113/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 114/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 115/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 116/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 117/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 118/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 119/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 120/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 121/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 122/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 123/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 124/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 125/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 126/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 127/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 128/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 129/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 130/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 131/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 132/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 133/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 134/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 135/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 136/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 137/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 138/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 139/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 140/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 141/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 142/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 143/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 144/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 145/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 146/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 147/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 148/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 149/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 150/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 151/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 152/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 153/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 154/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 155/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 156/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 157/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 158/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 159/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 160/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 161/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 162/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 163/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 164/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 165/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 166/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 167/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 168/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 169/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 170/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 171/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 172/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 173/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 174/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 175/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009635328315198421
  1/332 [..............................] - ETA: 1:22 22/332 [>.............................] - ETA: 0s   44/332 [==>...........................] - ETA: 0s 67/332 [=====>........................] - ETA: 0s 88/332 [======>.......................] - ETA: 0s110/332 [========>.....................] - ETA: 0s132/332 [==========>...................] - ETA: 0s154/332 [============>.................] - ETA: 0s176/332 [==============>...............] - ETA: 0s198/332 [================>.............] - ETA: 0s220/332 [==================>...........] - ETA: 0s242/332 [====================>.........] - ETA: 0s264/332 [======================>.......] - ETA: 0s286/332 [========================>.....] - ETA: 0s308/332 [==========================>...] - ETA: 0s330/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.08137115132368335
cosine 0.06386710496677617
MAE: 0.037385326
RMSE: 0.08182915
r2: 0.5656135445617763
RMSE zero-vector: 0.23411466903540806
['2.5custom_VAE', 'mse', 128, 175, 0.0018, 0.6, 758, 0.009824104607105255, 0.009635328315198421, 0.08137115132368335, 0.06386710496677617, 0.037385325878858566, 0.08182915300130844, 0.5656135445617763, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 165 0.002 128 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1769)        7076        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1769)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4165381     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,093,562
Trainable params: 9,084,970
Non-trainable params: 8,592
__________________________________________________________________________________________________
Epoch 1/165
747/747 - 7s - loss: 0.0309 - val_loss: 0.0183 - 7s/epoch - 9ms/step
Epoch 2/165
747/747 - 4s - loss: 0.0159 - val_loss: 0.0171 - 4s/epoch - 5ms/step
Epoch 3/165
747/747 - 4s - loss: 0.0149 - val_loss: 0.0187 - 4s/epoch - 5ms/step
Epoch 4/165
747/747 - 4s - loss: 0.0142 - val_loss: 0.0153 - 4s/epoch - 5ms/step
Epoch 5/165
747/747 - 4s - loss: 0.0136 - val_loss: 0.0136 - 4s/epoch - 5ms/step
Epoch 6/165
747/747 - 4s - loss: 0.0131 - val_loss: 0.0131 - 4s/epoch - 5ms/step
Epoch 7/165
747/747 - 4s - loss: 0.0128 - val_loss: 0.0126 - 4s/epoch - 5ms/step
Epoch 8/165
747/747 - 4s - loss: 0.0125 - val_loss: 0.0126 - 4s/epoch - 5ms/step
Epoch 9/165
747/747 - 4s - loss: 0.0124 - val_loss: 0.0122 - 4s/epoch - 5ms/step
Epoch 10/165
747/747 - 4s - loss: 0.0123 - val_loss: 0.0126 - 4s/epoch - 5ms/step
Epoch 11/165
747/747 - 4s - loss: 0.0123 - val_loss: 0.0120 - 4s/epoch - 5ms/step
Epoch 12/165
747/747 - 4s - loss: 0.0120 - val_loss: 0.0117 - 4s/epoch - 5ms/step
Epoch 13/165
747/747 - 4s - loss: 0.0117 - val_loss: 0.0114 - 4s/epoch - 5ms/step
Epoch 14/165
747/747 - 4s - loss: 0.0115 - val_loss: 0.0115 - 4s/epoch - 5ms/step
Epoch 15/165
747/747 - 4s - loss: 0.0114 - val_loss: 0.0112 - 4s/epoch - 5ms/step
Epoch 16/165
747/747 - 4s - loss: 0.0113 - val_loss: 0.0112 - 4s/epoch - 5ms/step
Epoch 17/165
747/747 - 4s - loss: 0.0119 - val_loss: 0.0121 - 4s/epoch - 5ms/step
Epoch 18/165
747/747 - 4s - loss: 0.0121 - val_loss: 0.0114 - 4s/epoch - 5ms/step
Epoch 19/165
747/747 - 4s - loss: 0.0118 - val_loss: 0.0116 - 4s/epoch - 5ms/step
Epoch 20/165
747/747 - 4s - loss: 0.0121 - val_loss: 0.0112 - 4s/epoch - 5ms/step
Epoch 21/165
747/747 - 4s - loss: 0.0113 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 22/165
747/747 - 4s - loss: 0.0113 - val_loss: 0.0114 - 4s/epoch - 5ms/step
Epoch 23/165
747/747 - 4s - loss: 0.0119 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 24/165
747/747 - 4s - loss: 0.0112 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 25/165
747/747 - 4s - loss: 0.0111 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 26/165
747/747 - 4s - loss: 0.0111 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 27/165
747/747 - 4s - loss: 0.0111 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 28/165
747/747 - 4s - loss: 0.0113 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 29/165
747/747 - 4s - loss: 0.0111 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 30/165
747/747 - 4s - loss: 0.0110 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 31/165
747/747 - 4s - loss: 0.0110 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 32/165
747/747 - 4s - loss: 0.0109 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 33/165
747/747 - 4s - loss: 0.0109 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 34/165
747/747 - 4s - loss: 0.0109 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 35/165
747/747 - 4s - loss: 0.0110 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 36/165
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 37/165
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 38/165
747/747 - 4s - loss: 0.0108 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 39/165
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 40/165
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 41/165
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 42/165
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 43/165
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 44/165
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 45/165
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 46/165
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 47/165
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 48/165
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 49/165
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 50/165
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 51/165
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 52/165
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 53/165
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 54/165
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 55/165
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 56/165
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 57/165
747/747 - 4s - loss: 0.0106 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 58/165
747/747 - 4s - loss: 0.0106 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 59/165
747/747 - 4s - loss: 0.0106 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 60/165
747/747 - 4s - loss: 0.0106 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 61/165
747/747 - 4s - loss: 0.0106 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 62/165
747/747 - 4s - loss: 0.0106 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 63/165
747/747 - 4s - loss: 0.0106 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 64/165
747/747 - 4s - loss: 0.0106 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 65/165
747/747 - 4s - loss: 0.0105 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 66/165
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 67/165
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 68/165
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 69/165
747/747 - 4s - loss: 0.0104 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 70/165
747/747 - 4s - loss: 0.0104 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 71/165
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 72/165
747/747 - 4s - loss: 0.0104 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 73/165
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 74/165
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 75/165
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 76/165
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 77/165
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 78/165
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 79/165
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 80/165
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 81/165
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 82/165
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 83/165
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 84/165
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 85/165
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 86/165
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 87/165
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 88/165
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 89/165
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 90/165
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 91/165
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 92/165
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 93/165
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 94/165
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 95/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 96/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 97/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 98/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 99/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 100/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 101/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 102/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 103/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 104/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 105/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 106/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 107/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 108/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 109/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 110/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 111/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 112/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 113/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 114/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 115/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 116/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 117/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 118/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 119/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 120/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 121/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 122/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 123/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 124/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 125/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 126/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 127/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 128/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 129/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 130/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 131/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 132/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 133/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 134/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 135/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 136/165
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 137/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 138/165
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 139/165
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 140/165
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 141/165
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 142/165
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 143/165
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 144/165
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 145/165
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 146/165
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 147/165
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 148/165
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 149/165
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 150/165
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 151/165
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 152/165
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 153/165
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 154/165
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 155/165
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 156/165
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 157/165
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 158/165
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 159/165
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 160/165
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 161/165
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 162/165
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 163/165
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 164/165
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 165/165
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009383282624185085
  1/332 [..............................] - ETA: 51s 37/332 [==>...........................] - ETA: 0s  75/332 [=====>........................] - ETA: 0s111/332 [=========>....................] - ETA: 0s150/332 [============>.................] - ETA: 0s189/332 [================>.............] - ETA: 0s227/332 [===================>..........] - ETA: 0s266/332 [=======================>......] - ETA: 0s304/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.07563960830804944
cosine 0.05936647475866655
MAE: 0.035945453
RMSE: 0.07905183
r2: 0.5946013155051795
RMSE zero-vector: 0.23411466903540806
['1.4custom_VAE', 'mse', 128, 165, 0.002, 0.6, 758, 0.009552143514156342, 0.009383282624185085, 0.07563960830804944, 0.05936647475866655, 0.035945452749729156, 0.07905182987451553, 0.5946013155051795, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.6 170 0.002 64 2] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3286)         4156790     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3286)        13144       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3286)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2491546     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7240340     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 16,393,366
Trainable params: 16,378,706
Non-trainable params: 14,660
__________________________________________________________________________________________________
Epoch 1/170
1493/1493 - 10s - loss: 0.0185 - val_loss: 0.0119 - 10s/epoch - 7ms/step
Epoch 2/170
1493/1493 - 7s - loss: 0.0089 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 3/170
1493/1493 - 7s - loss: 0.0074 - val_loss: 0.0071 - 7s/epoch - 5ms/step
Epoch 4/170
1493/1493 - 7s - loss: 0.0069 - val_loss: 0.0066 - 7s/epoch - 5ms/step
Epoch 5/170
1493/1493 - 7s - loss: 0.0065 - val_loss: 0.0064 - 7s/epoch - 5ms/step
Epoch 6/170
1493/1493 - 7s - loss: 0.0064 - val_loss: 0.0063 - 7s/epoch - 5ms/step
Epoch 7/170
1493/1493 - 7s - loss: 0.0064 - val_loss: 0.0063 - 7s/epoch - 5ms/step
Epoch 8/170
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 9/170
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 10/170
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 11/170
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 12/170
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 13/170
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 14/170
1493/1493 - 7s - loss: 0.0063 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 15/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 16/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 17/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 18/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 19/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 20/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 21/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 22/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 23/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 24/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 25/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 26/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 27/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 28/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 29/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 30/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 31/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 32/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 33/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 34/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 35/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 36/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 37/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 38/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 39/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 40/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 41/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 42/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 43/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 44/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 45/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 46/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 47/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 48/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 49/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 50/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 51/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 52/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 53/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 54/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 55/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 56/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 57/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 58/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 59/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 60/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 61/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 62/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 63/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 64/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 65/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 66/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 67/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 68/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 69/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 70/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 71/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 72/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 73/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 74/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 75/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 76/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 77/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 78/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 79/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 80/170
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 81/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 82/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 83/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 84/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 85/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 86/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 87/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 88/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 89/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 90/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 91/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 92/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 93/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 94/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 95/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 96/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 97/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 98/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 99/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 100/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 101/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 102/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 103/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 104/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 105/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 106/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 107/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 108/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 109/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 110/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 111/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 112/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 113/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 114/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 115/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 116/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 117/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 118/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 119/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 120/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 121/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 122/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 123/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 124/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 125/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 126/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 127/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 128/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 129/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 130/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 131/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 132/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 133/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 134/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 135/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 136/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 137/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 138/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 139/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 140/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 141/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 142/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 143/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 144/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 145/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 146/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 147/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 148/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 149/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 150/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 151/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 152/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 153/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 154/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 155/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 156/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 157/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 158/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 159/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 160/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 161/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 162/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 163/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 164/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 165/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 166/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 167/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0062 - 7s/epoch - 5ms/step
Epoch 168/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 169/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
Epoch 170/170
1493/1493 - 7s - loss: 0.0061 - val_loss: 0.0061 - 7s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0061337766237556934
  1/332 [..............................] - ETA: 53s 38/332 [==>...........................] - ETA: 0s  76/332 [=====>........................] - ETA: 0s114/332 [=========>....................] - ETA: 0s152/332 [============>.................] - ETA: 0s190/332 [================>.............] - ETA: 0s228/332 [===================>..........] - ETA: 0s266/332 [=======================>......] - ETA: 0s303/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.12601578635536767
cosine 0.09861813880758738
MAE: 0.047416795
RMSE: 0.10170769
r2: 0.32893014154794337
RMSE zero-vector: 0.23411466903540806
['2.6custom_VAE', 'logcosh', 64, 170, 0.002, 0.6, 758, 0.006123387720435858, 0.0061337766237556934, 0.12601578635536767, 0.09861813880758738, 0.047416795045137405, 0.10170768946409225, 0.32893014154794337, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 165 0.002 128 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1769)        7076        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1769)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4165381     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,093,562
Trainable params: 9,084,970
Non-trainable params: 8,592
__________________________________________________________________________________________________
File geneticVAE_MMmp_gap_custom_VAE1.4_cr0.6_bs128_ep165_loss_mse_lr0.002_AutoEncoder.h5 exists in folder already, skiping this calculation.
  1/332 [..............................] - ETA: 50s 36/332 [==>...........................] - ETA: 0s  74/332 [=====>........................] - ETA: 0s111/332 [=========>....................] - ETA: 0s149/332 [============>.................] - ETA: 0s187/332 [===============>..............] - ETA: 0s225/332 [===================>..........] - ETA: 0s263/332 [======================>.......] - ETA: 0s301/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.07563960830804944
cosine 0.05936647475866655
MAE: 0.035945453
RMSE: 0.07905183
r2: 0.5946013155051795
RMSE zero-vector: 0.23411466903540806
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
['1.4custom_VAE', 'mse', 128, 165, 0.002, 0.6, 758, '--', '--', 0.07563960830804944, 0.05936647475866655, 0.035945452749729156, 0.07905182987451553, 0.5946013155051795, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 175 0.002 128 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1769)        7076        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1769)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4165381     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,093,562
Trainable params: 9,084,970
Non-trainable params: 8,592
__________________________________________________________________________________________________
Epoch 1/175
747/747 - 7s - loss: 0.0305 - val_loss: 0.0335 - 7s/epoch - 9ms/step
Epoch 2/175
747/747 - 4s - loss: 0.0160 - val_loss: 0.0218 - 4s/epoch - 5ms/step
Epoch 3/175
747/747 - 4s - loss: 0.0148 - val_loss: 0.0156 - 4s/epoch - 5ms/step
Epoch 4/175
747/747 - 4s - loss: 0.0141 - val_loss: 0.0145 - 4s/epoch - 5ms/step
Epoch 5/175
747/747 - 4s - loss: 0.0134 - val_loss: 0.0139 - 4s/epoch - 5ms/step
Epoch 6/175
747/747 - 4s - loss: 0.0130 - val_loss: 0.0131 - 4s/epoch - 5ms/step
Epoch 7/175
747/747 - 4s - loss: 0.0127 - val_loss: 0.0126 - 4s/epoch - 5ms/step
Epoch 8/175
747/747 - 4s - loss: 0.0125 - val_loss: 0.0123 - 4s/epoch - 5ms/step
Epoch 9/175
747/747 - 4s - loss: 0.0124 - val_loss: 0.0121 - 4s/epoch - 5ms/step
Epoch 10/175
747/747 - 4s - loss: 0.0122 - val_loss: 0.0120 - 4s/epoch - 5ms/step
Epoch 11/175
747/747 - 4s - loss: 0.0122 - val_loss: 0.0120 - 4s/epoch - 5ms/step
Epoch 12/175
747/747 - 4s - loss: 0.0121 - val_loss: 0.0120 - 4s/epoch - 5ms/step
Epoch 13/175
747/747 - 4s - loss: 0.0120 - val_loss: 0.0118 - 4s/epoch - 5ms/step
Epoch 14/175
747/747 - 4s - loss: 0.0119 - val_loss: 0.0117 - 4s/epoch - 5ms/step
Epoch 15/175
747/747 - 4s - loss: 0.0117 - val_loss: 0.0115 - 4s/epoch - 5ms/step
Epoch 16/175
747/747 - 4s - loss: 0.0116 - val_loss: 0.0113 - 4s/epoch - 5ms/step
Epoch 17/175
747/747 - 4s - loss: 0.0113 - val_loss: 0.0112 - 4s/epoch - 5ms/step
Epoch 18/175
747/747 - 4s - loss: 0.0113 - val_loss: 0.0112 - 4s/epoch - 5ms/step
Epoch 19/175
747/747 - 4s - loss: 0.0113 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 20/175
747/747 - 4s - loss: 0.0112 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 21/175
747/747 - 4s - loss: 0.0111 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 22/175
747/747 - 4s - loss: 0.0111 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 23/175
747/747 - 4s - loss: 0.0111 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 24/175
747/747 - 4s - loss: 0.0112 - val_loss: 0.0112 - 4s/epoch - 5ms/step
Epoch 25/175
747/747 - 4s - loss: 0.0114 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 26/175
747/747 - 4s - loss: 0.0110 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 27/175
747/747 - 4s - loss: 0.0110 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 28/175
747/747 - 4s - loss: 0.0110 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 29/175
747/747 - 4s - loss: 0.0109 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 30/175
747/747 - 4s - loss: 0.0110 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 31/175
747/747 - 4s - loss: 0.0113 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 32/175
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 33/175
747/747 - 4s - loss: 0.0109 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 34/175
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 35/175
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 36/175
747/747 - 4s - loss: 0.0109 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 37/175
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 38/175
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 39/175
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 40/175
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 41/175
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 42/175
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 43/175
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 44/175
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 45/175
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 46/175
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 47/175
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 48/175
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 49/175
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 50/175
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 51/175
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 52/175
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 53/175
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 54/175
747/747 - 4s - loss: 0.0106 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 55/175
747/747 - 4s - loss: 0.0106 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 56/175
747/747 - 4s - loss: 0.0105 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 57/175
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 58/175
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 59/175
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 60/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 61/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 62/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 63/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 64/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 65/175
747/747 - 4s - loss: 0.0102 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 66/175
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 67/175
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 68/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 69/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 70/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 71/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 72/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 73/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 74/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 75/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 76/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 77/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 78/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 79/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 80/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 81/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 82/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 83/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 84/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 85/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 86/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 87/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 88/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 89/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 90/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 91/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 92/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 93/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 94/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 95/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 96/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 97/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 98/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 99/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 100/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 101/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 102/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 103/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 104/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 105/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 106/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 107/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 108/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 109/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 110/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 111/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 112/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 113/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 114/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 115/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 116/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 117/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 118/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 119/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 120/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 121/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 122/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 123/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 124/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 125/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 126/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 127/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 128/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 129/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 130/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 131/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 132/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 133/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 134/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 135/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 136/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 137/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 138/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 139/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 140/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 141/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 142/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 143/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 144/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 145/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 146/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 147/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 148/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 149/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 150/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 151/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 152/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 153/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 154/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 155/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 156/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 157/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 158/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 159/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 160/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 161/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 162/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 163/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 164/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 165/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 166/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 167/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 168/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 169/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 170/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 171/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 172/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 173/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 174/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 175/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009448765777051449
  1/332 [..............................] - ETA: 1:23 22/332 [>.............................] - ETA: 0s   44/332 [==>...........................] - ETA: 0s 66/332 [====>.........................] - ETA: 0s 89/332 [=======>......................] - ETA: 0s112/332 [=========>....................] - ETA: 0s133/332 [===========>..................] - ETA: 0s156/332 [=============>................] - ETA: 0s178/332 [===============>..............] - ETA: 0s200/332 [=================>............] - ETA: 0s222/332 [===================>..........] - ETA: 0s244/332 [=====================>........] - ETA: 0s266/332 [=======================>......] - ETA: 0s288/332 [=========================>....] - ETA: 0s310/332 [===========================>..] - ETA: 0s332/332 [==============================] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.07728438915686925
cosine 0.06064518238075535
MAE: 0.036352377
RMSE: 0.079898655
r2: 0.5858677713560447
RMSE zero-vector: 0.23411466903540806
['1.4custom_VAE', 'mse', 128, 175, 0.002, 0.6, 758, 0.00962852779775858, 0.009448765777051449, 0.07728438915686925, 0.06064518238075535, 0.03635237738490105, 0.0798986554145813, 0.5858677713560447, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Saved GA instance to file: ./tmp//ga_instance_generation_4.pkl
[1.2999999999999998 165 0.002 128 2] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1643)         2078395     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1643)        6572        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1643)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3909979     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,487,250
Trainable params: 8,479,162
Non-trainable params: 8,088
__________________________________________________________________________________________________
Epoch 1/165
747/747 - 7s - loss: 0.0179 - val_loss: 0.0098 - 7s/epoch - 9ms/step
Epoch 2/165
747/747 - 4s - loss: 0.0082 - val_loss: 0.0087 - 4s/epoch - 5ms/step
Epoch 3/165
747/747 - 4s - loss: 0.0079 - val_loss: 0.0080 - 4s/epoch - 5ms/step
Epoch 4/165
747/747 - 4s - loss: 0.0076 - val_loss: 0.0079 - 4s/epoch - 5ms/step
Epoch 5/165
747/747 - 4s - loss: 0.0073 - val_loss: 0.0086 - 4s/epoch - 5ms/step
Epoch 6/165
747/747 - 4s - loss: 0.0071 - val_loss: 0.0072 - 4s/epoch - 5ms/step
Epoch 7/165
747/747 - 4s - loss: 0.0069 - val_loss: 0.0070 - 4s/epoch - 5ms/step
Epoch 8/165
747/747 - 4s - loss: 0.0069 - val_loss: 0.0068 - 4s/epoch - 5ms/step
Epoch 9/165
747/747 - 4s - loss: 0.0068 - val_loss: 0.0068 - 4s/epoch - 5ms/step
Epoch 10/165
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 11/165
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 12/165
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 13/165
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 14/165
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 15/165
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 16/165
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 17/165
747/747 - 4s - loss: 0.0066 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 18/165
747/747 - 4s - loss: 0.0066 - val_loss: 0.0064 - 4s/epoch - 5ms/step
Epoch 19/165
747/747 - 4s - loss: 0.0064 - val_loss: 0.0063 - 4s/epoch - 5ms/step
Epoch 20/165
747/747 - 4s - loss: 0.0064 - val_loss: 0.0063 - 4s/epoch - 5ms/step
Epoch 21/165
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 22/165
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 23/165
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 24/165
747/747 - 4s - loss: 0.0064 - val_loss: 0.0063 - 4s/epoch - 5ms/step
Epoch 25/165
747/747 - 4s - loss: 0.0064 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 26/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 27/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 28/165
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 29/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 30/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 31/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 32/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 33/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 34/165
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 35/165
747/747 - 4s - loss: 0.0063 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 36/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 37/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 38/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 39/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 40/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 41/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 42/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 43/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 44/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 45/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 46/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 47/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 48/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 49/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 50/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 51/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 52/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 53/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 54/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 55/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 56/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 57/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 58/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 59/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 60/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 61/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 62/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 63/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 64/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 65/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 66/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 67/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 68/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 69/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 70/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 71/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 72/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 73/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 74/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 75/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 76/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 77/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 78/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 79/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 80/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 81/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 82/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 83/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 84/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 85/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 86/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 87/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 88/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 89/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 90/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 91/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 92/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 93/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 94/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 95/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 96/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 97/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 98/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 99/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 100/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 101/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 102/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 103/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 104/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 105/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 106/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 107/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 108/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 109/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 110/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 111/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 112/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 113/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 114/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 115/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 116/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 117/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 118/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 119/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 120/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 121/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 122/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 123/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 124/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 125/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 126/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 127/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 128/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 129/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 130/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 131/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 132/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 133/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 134/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 135/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 136/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 137/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 138/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 139/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 140/165
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 141/165
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 142/165
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 143/165
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 144/165
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 145/165
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 146/165
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 147/165
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 148/165
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 149/165
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 150/165
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 151/165
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 152/165
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 153/165
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 154/165
747/747 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 155/165
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 156/165
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 157/165
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 158/165
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 159/165
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 160/165
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 161/165
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 162/165
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 163/165
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 164/165
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
Epoch 165/165
747/747 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.005927316844463348
  1/332 [..............................] - ETA: 54s 38/332 [==>...........................] - ETA: 0s  77/332 [=====>........................] - ETA: 0s115/332 [=========>....................] - ETA: 0s153/332 [============>.................] - ETA: 0s192/332 [================>.............] - ETA: 0s231/332 [===================>..........] - ETA: 0s268/332 [=======================>......] - ETA: 0s306/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.11941258753644625
cosine 0.09339666843998445
MAE: 0.04559724
RMSE: 0.09830562
r2: 0.3730736158082467
RMSE zero-vector: 0.23411466903540806
['1.2999999999999998custom_VAE', 'logcosh', 128, 165, 0.002, 0.6, 758, 0.0059901936911046505, 0.005927316844463348, 0.11941258753644625, 0.09339666843998445, 0.04559724032878876, 0.09830562025308609, 0.3730736158082467, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 160 0.002 128 2] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1769)        7076        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1769)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4165381     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,093,562
Trainable params: 9,084,970
Non-trainable params: 8,592
__________________________________________________________________________________________________
Epoch 1/160
747/747 - 7s - loss: 0.0185 - val_loss: 0.0090 - 7s/epoch - 9ms/step
Epoch 2/160
747/747 - 4s - loss: 0.0083 - val_loss: 0.0085 - 4s/epoch - 5ms/step
Epoch 3/160
747/747 - 4s - loss: 0.0079 - val_loss: 0.0089 - 4s/epoch - 5ms/step
Epoch 4/160
747/747 - 4s - loss: 0.0076 - val_loss: 0.0082 - 4s/epoch - 5ms/step
Epoch 5/160
747/747 - 4s - loss: 0.0074 - val_loss: 0.0073 - 4s/epoch - 5ms/step
Epoch 6/160
747/747 - 4s - loss: 0.0072 - val_loss: 0.0079 - 4s/epoch - 5ms/step
Epoch 7/160
747/747 - 4s - loss: 0.0070 - val_loss: 0.0070 - 4s/epoch - 5ms/step
Epoch 8/160
747/747 - 4s - loss: 0.0069 - val_loss: 0.0068 - 4s/epoch - 5ms/step
Epoch 9/160
747/747 - 4s - loss: 0.0068 - val_loss: 0.0068 - 4s/epoch - 5ms/step
Epoch 10/160
747/747 - 4s - loss: 0.0068 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 11/160
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 12/160
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 13/160
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 14/160
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 15/160
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 16/160
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 17/160
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 18/160
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 19/160
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 20/160
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 21/160
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 22/160
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 23/160
747/747 - 4s - loss: 0.0066 - val_loss: 0.0065 - 4s/epoch - 5ms/step
Epoch 24/160
747/747 - 4s - loss: 0.0064 - val_loss: 0.0063 - 4s/epoch - 5ms/step
Epoch 25/160
747/747 - 4s - loss: 0.0063 - val_loss: 0.0063 - 4s/epoch - 5ms/step
Epoch 26/160
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 27/160
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 28/160
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 29/160
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 30/160
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 31/160
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 32/160
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 33/160
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 34/160
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 35/160
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 36/160
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 37/160
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 38/160
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 39/160
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 40/160
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 41/160
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 42/160
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 43/160
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 44/160
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 45/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 46/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 47/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 48/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 49/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 50/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 51/160
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 52/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 53/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 54/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 55/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 56/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 57/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 58/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 59/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 60/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 61/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 62/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 63/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 64/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 65/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 66/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 67/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 68/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 69/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 70/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 71/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 72/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 73/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 74/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 75/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 76/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 77/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 78/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 79/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 80/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 81/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 82/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 83/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 84/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 85/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 86/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 87/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 88/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 89/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 90/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 91/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 92/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 93/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 94/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 95/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 96/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 97/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 98/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 99/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 100/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 101/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 102/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 103/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 104/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 105/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 106/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 107/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 108/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 109/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 110/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 111/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 112/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 113/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 114/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 115/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 116/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 117/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 118/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 119/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 120/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 121/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 122/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 123/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 124/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 125/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 126/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 127/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 128/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 129/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 130/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 131/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 132/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 133/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 134/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 135/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 136/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 137/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 138/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 139/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 140/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 141/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 142/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 143/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 144/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 145/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 146/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 147/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 148/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 149/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 150/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 151/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 152/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 153/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 154/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 155/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 156/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 157/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 158/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 159/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 160/160
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.006026121322065592
  1/332 [..............................] - ETA: 52s 38/332 [==>...........................] - ETA: 0s  76/332 [=====>........................] - ETA: 0s114/332 [=========>....................] - ETA: 0s152/332 [============>.................] - ETA: 0s190/332 [================>.............] - ETA: 0s229/332 [===================>..........] - ETA: 0s267/332 [=======================>......] - ETA: 0s306/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.12436429100849024
cosine 0.0972281856061466
MAE: 0.04675188
RMSE: 0.100359075
r2: 0.3466085710597066
RMSE zero-vector: 0.23411466903540806
['1.4custom_VAE', 'logcosh', 128, 160, 0.002, 0.6, 758, 0.006078753154724836, 0.006026121322065592, 0.12436429100849024, 0.0972281856061466, 0.046751879155635834, 0.10035907477140427, 0.3466085710597066, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 175 0.002 128 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Epoch 1/175
747/747 - 7s - loss: 0.0320 - val_loss: 0.0196 - 7s/epoch - 9ms/step
Epoch 2/175
747/747 - 4s - loss: 0.0159 - val_loss: 0.0168 - 4s/epoch - 5ms/step
Epoch 3/175
747/747 - 4s - loss: 0.0149 - val_loss: 0.0164 - 4s/epoch - 5ms/step
Epoch 4/175
747/747 - 4s - loss: 0.0142 - val_loss: 0.0160 - 4s/epoch - 5ms/step
Epoch 5/175
747/747 - 4s - loss: 0.0135 - val_loss: 0.0147 - 4s/epoch - 5ms/step
Epoch 6/175
747/747 - 4s - loss: 0.0131 - val_loss: 0.0142 - 4s/epoch - 5ms/step
Epoch 7/175
747/747 - 4s - loss: 0.0127 - val_loss: 0.0126 - 4s/epoch - 5ms/step
Epoch 8/175
747/747 - 4s - loss: 0.0125 - val_loss: 0.0123 - 4s/epoch - 5ms/step
Epoch 9/175
747/747 - 4s - loss: 0.0124 - val_loss: 0.0122 - 4s/epoch - 5ms/step
Epoch 10/175
747/747 - 4s - loss: 0.0123 - val_loss: 0.0121 - 4s/epoch - 5ms/step
Epoch 11/175
747/747 - 4s - loss: 0.0122 - val_loss: 0.0121 - 4s/epoch - 5ms/step
Epoch 12/175
747/747 - 4s - loss: 0.0121 - val_loss: 0.0120 - 4s/epoch - 5ms/step
Epoch 13/175
747/747 - 4s - loss: 0.0120 - val_loss: 0.0119 - 4s/epoch - 5ms/step
Epoch 14/175
747/747 - 4s - loss: 0.0120 - val_loss: 0.0117 - 4s/epoch - 5ms/step
Epoch 15/175
747/747 - 4s - loss: 0.0118 - val_loss: 0.0117 - 4s/epoch - 5ms/step
Epoch 16/175
747/747 - 4s - loss: 0.0117 - val_loss: 0.0113 - 4s/epoch - 5ms/step
Epoch 17/175
747/747 - 4s - loss: 0.0114 - val_loss: 0.0112 - 4s/epoch - 5ms/step
Epoch 18/175
747/747 - 4s - loss: 0.0113 - val_loss: 0.0113 - 4s/epoch - 5ms/step
Epoch 19/175
747/747 - 4s - loss: 0.0113 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 20/175
747/747 - 4s - loss: 0.0113 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 21/175
747/747 - 4s - loss: 0.0112 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 22/175
747/747 - 4s - loss: 0.0112 - val_loss: 0.0112 - 4s/epoch - 5ms/step
Epoch 23/175
747/747 - 4s - loss: 0.0112 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 24/175
747/747 - 4s - loss: 0.0111 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 25/175
747/747 - 4s - loss: 0.0110 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 26/175
747/747 - 4s - loss: 0.0110 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 27/175
747/747 - 4s - loss: 0.0112 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 28/175
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 29/175
747/747 - 4s - loss: 0.0109 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 30/175
747/747 - 4s - loss: 0.0110 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 31/175
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 32/175
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 33/175
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 34/175
747/747 - 4s - loss: 0.0106 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 35/175
747/747 - 4s - loss: 0.0106 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 36/175
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 37/175
747/747 - 4s - loss: 0.0105 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 38/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 39/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 40/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 41/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 42/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 43/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 44/175
747/747 - 4s - loss: 0.0104 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 45/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 46/175
747/747 - 4s - loss: 0.0102 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 47/175
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 48/175
747/747 - 4s - loss: 0.0102 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 49/175
747/747 - 4s - loss: 0.0103 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 50/175
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 51/175
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 52/175
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 53/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 54/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 55/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 56/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 57/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 58/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 59/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 60/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 61/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 62/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 63/175
747/747 - 4s - loss: 0.0101 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 64/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 65/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 66/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 67/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 68/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 69/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 70/175
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 71/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 72/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 73/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 74/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 75/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 76/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 77/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 78/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 79/175
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 80/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 81/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 82/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 83/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 84/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 85/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 86/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 87/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 88/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 89/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 90/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 91/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 92/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 93/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 94/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 95/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 96/175
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 97/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 98/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 99/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 100/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 101/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 102/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 103/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 104/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 105/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 106/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 107/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 108/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 109/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 110/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 111/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 112/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 113/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 114/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 115/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 116/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 117/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 118/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 119/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 120/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 121/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 122/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 123/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 124/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 125/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 126/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 127/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 128/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 129/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 130/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 131/175
747/747 - 4s - loss: 0.0097 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 132/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 133/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 134/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 135/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 136/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 137/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 138/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 139/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 140/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 141/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 142/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 143/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 144/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 145/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 146/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 147/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 148/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 149/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 150/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 151/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 152/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 153/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 154/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 155/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 156/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 157/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 158/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 159/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 160/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 161/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 162/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 163/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 164/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 165/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 166/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 167/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 168/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 169/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 170/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 171/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 172/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 173/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 174/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 175/175
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009383579716086388
  1/332 [..............................] - ETA: 1:22 22/332 [>.............................] - ETA: 0s   44/332 [==>...........................] - ETA: 0s 65/332 [====>.........................] - ETA: 0s 88/332 [======>.......................] - ETA: 0s110/332 [========>.....................] - ETA: 0s132/332 [==========>...................] - ETA: 0s154/332 [============>.................] - ETA: 0s176/332 [==============>...............] - ETA: 0s198/332 [================>.............] - ETA: 0s220/332 [==================>...........] - ETA: 0s242/332 [====================>.........] - ETA: 0s264/332 [======================>.......] - ETA: 0s286/332 [========================>.....] - ETA: 0s308/332 [==========================>...] - ETA: 0s330/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.07606930297206493
cosine 0.05970918812641233
MAE: 0.036070824
RMSE: 0.079261705
r2: 0.5924444002925915
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'mse', 128, 175, 0.002, 0.6, 758, 0.009568626061081886, 0.009383579716086388, 0.07606930297206493, 0.05970918812641233, 0.036070823669433594, 0.07926170527935028, 0.5924444002925915, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 175 0.002 128 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
File geneticVAE_MMmp_gap_custom_VAE1.5_cr0.6_bs128_ep175_loss_mse_lr0.002_AutoEncoder.h5 exists in folder already, skiping this calculation.
  1/332 [..............................] - ETA: 49s 38/332 [==>...........................] - ETA: 0s  76/332 [=====>........................] - ETA: 0s114/332 [=========>....................] - ETA: 0s153/332 [============>.................] - ETA: 0s192/332 [================>.............] - ETA: 0s231/332 [===================>..........] - ETA: 0s269/332 [=======================>......] - ETA: 0s308/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.07606930297206493
cosine 0.05970918812641233
MAE: 0.036070824
RMSE: 0.079261705
r2: 0.5924444002925915
RMSE zero-vector: 0.23411466903540806
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
['1.5custom_VAE', 'mse', 128, 175, 0.002, 0.6, 758, '--', '--', 0.07606930297206493, 0.05970918812641233, 0.036070823669433594, 0.07926170527935028, 0.5924444002925915, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 160 0.002 128 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1769)        7076        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1769)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4165381     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,093,562
Trainable params: 9,084,970
Non-trainable params: 8,592
__________________________________________________________________________________________________
Epoch 1/160
747/747 - 7s - loss: 0.0303 - val_loss: 0.0204 - 7s/epoch - 9ms/step
Epoch 2/160
747/747 - 4s - loss: 0.0161 - val_loss: 0.0162 - 4s/epoch - 5ms/step
Epoch 3/160
747/747 - 4s - loss: 0.0149 - val_loss: 0.0156 - 4s/epoch - 5ms/step
Epoch 4/160
747/747 - 4s - loss: 0.0141 - val_loss: 0.0153 - 4s/epoch - 5ms/step
Epoch 5/160
747/747 - 4s - loss: 0.0135 - val_loss: 0.0133 - 4s/epoch - 5ms/step
Epoch 6/160
747/747 - 4s - loss: 0.0130 - val_loss: 0.0131 - 4s/epoch - 5ms/step
Epoch 7/160
747/747 - 4s - loss: 0.0127 - val_loss: 0.0125 - 4s/epoch - 5ms/step
Epoch 8/160
747/747 - 4s - loss: 0.0125 - val_loss: 0.0124 - 4s/epoch - 5ms/step
Epoch 9/160
747/747 - 4s - loss: 0.0124 - val_loss: 0.0122 - 4s/epoch - 5ms/step
Epoch 10/160
747/747 - 4s - loss: 0.0123 - val_loss: 0.0120 - 4s/epoch - 5ms/step
Epoch 11/160
747/747 - 4s - loss: 0.0122 - val_loss: 0.0120 - 4s/epoch - 5ms/step
Epoch 12/160
747/747 - 4s - loss: 0.0121 - val_loss: 0.0118 - 4s/epoch - 5ms/step
Epoch 13/160
747/747 - 4s - loss: 0.0119 - val_loss: 0.0116 - 4s/epoch - 5ms/step
Epoch 14/160
747/747 - 4s - loss: 0.0116 - val_loss: 0.0113 - 4s/epoch - 5ms/step
Epoch 15/160
747/747 - 4s - loss: 0.0114 - val_loss: 0.0114 - 4s/epoch - 5ms/step
Epoch 16/160
747/747 - 4s - loss: 0.0115 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 17/160
747/747 - 4s - loss: 0.0112 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 18/160
747/747 - 4s - loss: 0.0112 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 19/160
747/747 - 4s - loss: 0.0112 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 20/160
747/747 - 4s - loss: 0.0111 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 21/160
747/747 - 4s - loss: 0.0111 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 22/160
747/747 - 4s - loss: 0.0111 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 23/160
747/747 - 4s - loss: 0.0113 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 24/160
747/747 - 4s - loss: 0.0110 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 25/160
747/747 - 4s - loss: 0.0111 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 26/160
747/747 - 4s - loss: 0.0110 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 27/160
747/747 - 4s - loss: 0.0112 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 28/160
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 29/160
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 30/160
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 31/160
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 32/160
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 33/160
747/747 - 4s - loss: 0.0108 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 34/160
747/747 - 4s - loss: 0.0108 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 35/160
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 36/160
747/747 - 4s - loss: 0.0108 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 37/160
747/747 - 4s - loss: 0.0110 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 38/160
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 39/160
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 40/160
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 41/160
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 42/160
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 43/160
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 44/160
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 45/160
747/747 - 4s - loss: 0.0106 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 46/160
747/747 - 4s - loss: 0.0106 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 47/160
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 48/160
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 49/160
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 50/160
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 51/160
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 52/160
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 53/160
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 54/160
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 55/160
747/747 - 4s - loss: 0.0102 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 56/160
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 57/160
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 58/160
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 59/160
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 60/160
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 61/160
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 62/160
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 63/160
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 64/160
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 65/160
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 66/160
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 67/160
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 68/160
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 69/160
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 70/160
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 71/160
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 72/160
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 73/160
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 74/160
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 75/160
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 76/160
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 77/160
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 78/160
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 79/160
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 80/160
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 81/160
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 82/160
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 83/160
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 84/160
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 85/160
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 86/160
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 87/160
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 88/160
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 89/160
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 90/160
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 91/160
747/747 - 4s - loss: 0.0099 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 92/160
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 93/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 94/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 95/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 96/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 97/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 98/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 99/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 100/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 101/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 102/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 103/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 104/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 105/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 106/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 107/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 108/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 109/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 110/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 111/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 112/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 113/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 114/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 115/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 116/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 117/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 118/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 119/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 120/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 121/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 122/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 123/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 124/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 125/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 126/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 127/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 128/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 129/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 130/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 131/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 132/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 133/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 134/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 135/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 136/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 137/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 138/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 139/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 140/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 141/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 142/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 143/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 144/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 145/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 146/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 147/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 148/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 149/160
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 150/160
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 151/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 152/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 153/160
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 154/160
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 155/160
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 156/160
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 157/160
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 158/160
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 159/160
747/747 - 4s - loss: 0.0096 - val_loss: 0.0094 - 4s/epoch - 5ms/step
Epoch 160/160
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009485973976552486
  1/332 [..............................] - ETA: 1:23 23/332 [=>............................] - ETA: 0s   46/332 [===>..........................] - ETA: 0s 68/332 [=====>........................] - ETA: 0s 90/332 [=======>......................] - ETA: 0s113/332 [=========>....................] - ETA: 0s135/332 [===========>..................] - ETA: 0s157/332 [=============>................] - ETA: 0s179/332 [===============>..............] - ETA: 0s200/332 [=================>............] - ETA: 0s222/332 [===================>..........] - ETA: 0s244/332 [=====================>........] - ETA: 0s266/332 [=======================>......] - ETA: 0s288/332 [=========================>....] - ETA: 0s310/332 [===========================>..] - ETA: 0s332/332 [==============================] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.07720601282904295
cosine 0.06057986289071803
MAE: 0.036221117
RMSE: 0.079792306
r2: 0.5869694077602733
RMSE zero-vector: 0.23411466903540806
['1.4custom_VAE', 'mse', 128, 160, 0.002, 0.6, 758, 0.009628918021917343, 0.009485973976552486, 0.07720601282904295, 0.06057986289071803, 0.03622111678123474, 0.07979230582714081, 0.5869694077602733, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.2999999999999998 160 0.002 128 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1643)         2078395     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1643)        6572        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1643)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3909979     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,487,250
Trainable params: 8,479,162
Non-trainable params: 8,088
__________________________________________________________________________________________________
Epoch 1/160
747/747 - 7s - loss: 0.0299 - val_loss: 0.0183 - 7s/epoch - 9ms/step
Epoch 2/160
747/747 - 4s - loss: 0.0157 - val_loss: 0.0167 - 4s/epoch - 5ms/step
Epoch 3/160
747/747 - 4s - loss: 0.0148 - val_loss: 0.0151 - 4s/epoch - 5ms/step
Epoch 4/160
747/747 - 4s - loss: 0.0140 - val_loss: 0.0152 - 4s/epoch - 5ms/step
Epoch 5/160
747/747 - 4s - loss: 0.0135 - val_loss: 0.0134 - 4s/epoch - 5ms/step
Epoch 6/160
747/747 - 4s - loss: 0.0130 - val_loss: 0.0130 - 4s/epoch - 5ms/step
Epoch 7/160
747/747 - 4s - loss: 0.0127 - val_loss: 0.0125 - 4s/epoch - 5ms/step
Epoch 8/160
747/747 - 4s - loss: 0.0125 - val_loss: 0.0123 - 4s/epoch - 5ms/step
Epoch 9/160
747/747 - 4s - loss: 0.0123 - val_loss: 0.0121 - 4s/epoch - 5ms/step
Epoch 10/160
747/747 - 4s - loss: 0.0120 - val_loss: 0.0118 - 4s/epoch - 5ms/step
Epoch 11/160
747/747 - 4s - loss: 0.0117 - val_loss: 0.0114 - 4s/epoch - 5ms/step
Epoch 12/160
747/747 - 4s - loss: 0.0114 - val_loss: 0.0113 - 4s/epoch - 5ms/step
Epoch 13/160
747/747 - 4s - loss: 0.0114 - val_loss: 0.0112 - 4s/epoch - 5ms/step
Epoch 14/160
747/747 - 4s - loss: 0.0113 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 15/160
747/747 - 4s - loss: 0.0112 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 16/160
747/747 - 4s - loss: 0.0112 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 17/160
747/747 - 4s - loss: 0.0111 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 18/160
747/747 - 4s - loss: 0.0111 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 19/160
747/747 - 4s - loss: 0.0111 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 20/160
747/747 - 4s - loss: 0.0110 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 21/160
747/747 - 4s - loss: 0.0110 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 22/160
747/747 - 4s - loss: 0.0110 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 23/160
747/747 - 4s - loss: 0.0111 - val_loss: 0.0112 - 4s/epoch - 5ms/step
Epoch 24/160
747/747 - 4s - loss: 0.0110 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 25/160
747/747 - 4s - loss: 0.0109 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 26/160
747/747 - 4s - loss: 0.0109 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 27/160
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 28/160
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 29/160
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 30/160
747/747 - 4s - loss: 0.0108 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 31/160
747/747 - 4s - loss: 0.0108 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 32/160
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 33/160
747/747 - 4s - loss: 0.0108 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 34/160
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 35/160
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 36/160
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 37/160
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 38/160
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 39/160
747/747 - 4s - loss: 0.0106 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 40/160
747/747 - 4s - loss: 0.0106 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 41/160
747/747 - 4s - loss: 0.0106 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 42/160
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 43/160
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 44/160
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 45/160
747/747 - 4s - loss: 0.0104 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 46/160
747/747 - 4s - loss: 0.0104 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 47/160
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 48/160
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 49/160
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 50/160
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 51/160
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 52/160
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 53/160
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 54/160
747/747 - 4s - loss: 0.0102 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 55/160
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 56/160
747/747 - 4s - loss: 0.0102 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 57/160
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 58/160
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 59/160
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 60/160
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 61/160
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 62/160
747/747 - 4s - loss: 0.0102 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 63/160
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 64/160
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 65/160
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 66/160
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 67/160
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 68/160
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 69/160
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 70/160
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 71/160
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 72/160
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 73/160
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 74/160
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 75/160
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 76/160
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 77/160
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 78/160
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 79/160
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 80/160
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 81/160
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 82/160
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 83/160
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 84/160
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 85/160
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 86/160
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 87/160
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 88/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 89/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 90/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 91/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 92/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 93/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 94/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 95/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 96/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 97/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 98/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 99/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 100/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 101/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 102/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 103/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 104/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 105/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 106/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 107/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 108/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 109/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 110/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 111/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 112/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 113/160
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 114/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 115/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 116/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 117/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 118/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 119/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 120/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 121/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 122/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 123/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 124/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 125/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 126/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 127/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 128/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 129/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 130/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 131/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 132/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 133/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 134/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 135/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 136/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 137/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 138/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 139/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 140/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 141/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 142/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 143/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 144/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 145/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 146/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 147/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 148/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 149/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 150/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 151/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 152/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 153/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 154/160
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 155/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 156/160
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 157/160
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 158/160
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 159/160
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 160/160
747/747 - 4s - loss: 0.0096 - val_loss: 0.0095 - 4s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.00947178341448307
  1/332 [..............................] - ETA: 51s 38/332 [==>...........................] - ETA: 0s  77/332 [=====>........................] - ETA: 0s116/332 [=========>....................] - ETA: 0s155/332 [=============>................] - ETA: 0s191/332 [================>.............] - ETA: 0s229/332 [===================>..........] - ETA: 0s268/332 [=======================>......] - ETA: 0s307/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.07746271385793366
cosine 0.060779340893782806
MAE: 0.036306027
RMSE: 0.07995484
r2: 0.5852851819418864
RMSE zero-vector: 0.23411466903540806
['1.2999999999999998custom_VAE', 'mse', 128, 160, 0.002, 0.6, 758, 0.009643001481890678, 0.00947178341448307, 0.07746271385793366, 0.060779340893782806, 0.036306027323007584, 0.0799548402428627, 0.5852851819418864, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Saved GA instance to file: ./tmp//ga_instance_generation_5.pkl
[1.4 180 0.0022 256 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1769)        7076        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1769)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4165381     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,093,562
Trainable params: 9,084,970
Non-trainable params: 8,592
__________________________________________________________________________________________________
Epoch 1/180
374/374 - 5s - loss: 0.0392 - val_loss: 0.0191 - 5s/epoch - 14ms/step
Epoch 2/180
374/374 - 2s - loss: 0.0166 - val_loss: 0.0189 - 2s/epoch - 6ms/step
Epoch 3/180
374/374 - 2s - loss: 0.0153 - val_loss: 0.0184 - 2s/epoch - 6ms/step
Epoch 4/180
374/374 - 2s - loss: 0.0147 - val_loss: 0.0259 - 2s/epoch - 6ms/step
Epoch 5/180
374/374 - 2s - loss: 0.0145 - val_loss: 0.0167 - 2s/epoch - 6ms/step
Epoch 6/180
374/374 - 2s - loss: 0.0140 - val_loss: 0.0171 - 2s/epoch - 6ms/step
Epoch 7/180
374/374 - 2s - loss: 0.0138 - val_loss: 0.0166 - 2s/epoch - 6ms/step
Epoch 8/180
374/374 - 2s - loss: 0.0135 - val_loss: 0.0159 - 2s/epoch - 6ms/step
Epoch 9/180
374/374 - 2s - loss: 0.0133 - val_loss: 0.0147 - 2s/epoch - 6ms/step
Epoch 10/180
374/374 - 2s - loss: 0.0131 - val_loss: 0.0155 - 2s/epoch - 6ms/step
Epoch 11/180
374/374 - 2s - loss: 0.0129 - val_loss: 0.0134 - 2s/epoch - 6ms/step
Epoch 12/180
374/374 - 2s - loss: 0.0127 - val_loss: 0.0129 - 2s/epoch - 6ms/step
Epoch 13/180
374/374 - 2s - loss: 0.0125 - val_loss: 0.0130 - 2s/epoch - 6ms/step
Epoch 14/180
374/374 - 2s - loss: 0.0125 - val_loss: 0.0134 - 2s/epoch - 6ms/step
Epoch 15/180
374/374 - 2s - loss: 0.0125 - val_loss: 0.0129 - 2s/epoch - 6ms/step
Epoch 16/180
374/374 - 2s - loss: 0.0123 - val_loss: 0.0123 - 2s/epoch - 6ms/step
Epoch 17/180
374/374 - 2s - loss: 0.0122 - val_loss: 0.0121 - 2s/epoch - 6ms/step
Epoch 18/180
374/374 - 2s - loss: 0.0121 - val_loss: 0.0121 - 2s/epoch - 6ms/step
Epoch 19/180
374/374 - 2s - loss: 0.0121 - val_loss: 0.0119 - 2s/epoch - 6ms/step
Epoch 20/180
374/374 - 2s - loss: 0.0119 - val_loss: 0.0118 - 2s/epoch - 6ms/step
Epoch 21/180
374/374 - 2s - loss: 0.0117 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 22/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0115 - 2s/epoch - 6ms/step
Epoch 23/180
374/374 - 2s - loss: 0.0116 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 24/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 25/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 26/180
374/374 - 2s - loss: 0.0113 - val_loss: 0.0117 - 2s/epoch - 6ms/step
Epoch 27/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 28/180
374/374 - 2s - loss: 0.0112 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 29/180
374/374 - 2s - loss: 0.0116 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 30/180
374/374 - 2s - loss: 0.0112 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 31/180
374/374 - 2s - loss: 0.0120 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 32/180
374/374 - 2s - loss: 0.0111 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 33/180
374/374 - 2s - loss: 0.0111 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 34/180
374/374 - 2s - loss: 0.0111 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 35/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 36/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 37/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 38/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 39/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 40/180
374/374 - 2s - loss: 0.0113 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 41/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 42/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 43/180
374/374 - 2s - loss: 0.0111 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 44/180
374/374 - 2s - loss: 0.0120 - val_loss: 0.0117 - 2s/epoch - 6ms/step
Epoch 45/180
374/374 - 2s - loss: 0.0120 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 46/180
374/374 - 2s - loss: 0.0113 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 47/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 48/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 49/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 50/180
374/374 - 2s - loss: 0.0114 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 51/180
374/374 - 2s - loss: 0.0125 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 52/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 53/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 54/180
374/374 - 2s - loss: 0.0109 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 55/180
374/374 - 2s - loss: 0.0109 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 56/180
374/374 - 2s - loss: 0.0121 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 57/180
374/374 - 2s - loss: 0.0109 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 58/180
374/374 - 2s - loss: 0.0109 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 59/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 60/180
374/374 - 2s - loss: 0.0109 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 61/180
374/374 - 2s - loss: 0.0108 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 62/180
374/374 - 2s - loss: 0.0108 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 63/180
374/374 - 2s - loss: 0.0108 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 64/180
374/374 - 2s - loss: 0.0108 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 65/180
374/374 - 2s - loss: 0.0108 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 66/180
374/374 - 2s - loss: 0.0115 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 67/180
374/374 - 2s - loss: 0.0109 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 68/180
374/374 - 2s - loss: 0.0108 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 69/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 70/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 71/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 72/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 73/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 74/180
374/374 - 2s - loss: 0.0110 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 75/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 76/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 77/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 78/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0108 - 2s/epoch - 6ms/step
Epoch 79/180
374/374 - 2s - loss: 0.0111 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 80/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 81/180
374/374 - 2s - loss: 0.0111 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 82/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 83/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 84/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 85/180
374/374 - 2s - loss: 0.0109 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 86/180
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 87/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 88/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 89/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 90/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 91/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 92/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 93/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 94/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 95/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 96/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 97/180
374/374 - 2s - loss: 0.0106 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 98/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 99/180
374/374 - 2s - loss: 0.0105 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 100/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 101/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 102/180
374/374 - 2s - loss: 0.0104 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 103/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 104/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 105/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 106/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 107/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 108/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 109/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 110/180
374/374 - 2s - loss: 0.0103 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 111/180
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 112/180
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 113/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 114/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 115/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 116/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 117/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 118/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 119/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 120/180
374/374 - 2s - loss: 0.0101 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 121/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 122/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 123/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 124/180
374/374 - 2s - loss: 0.0100 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 125/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 126/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 127/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 128/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 129/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 130/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 131/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0098 - 2s/epoch - 6ms/step
Epoch 132/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 133/180
374/374 - 2s - loss: 0.0099 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 134/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 135/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 136/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 137/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 138/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 139/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 140/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 141/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 142/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0097 - 2s/epoch - 6ms/step
Epoch 143/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 144/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 145/180
374/374 - 2s - loss: 0.0098 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 146/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 147/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 148/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 149/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 150/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 151/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 152/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 153/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 154/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 155/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 156/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 157/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 158/180
374/374 - 2s - loss: 0.0097 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 159/180
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 160/180
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 161/180
374/374 - 2s - loss: 0.0096 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 162/180
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 163/180
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 164/180
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 165/180
374/374 - 2s - loss: 0.0096 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 166/180
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 167/180
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 168/180
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 169/180
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 170/180
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 171/180
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 172/180
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 173/180
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 174/180
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 175/180
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 176/180
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 177/180
374/374 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 178/180
374/374 - 2s - loss: 0.0096 - val_loss: 0.0095 - 2s/epoch - 6ms/step
Epoch 179/180
374/374 - 2s - loss: 0.0096 - val_loss: 0.0094 - 2s/epoch - 6ms/step
Epoch 180/180
374/374 - 2s - loss: 0.0095 - val_loss: 0.0094 - 2s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009432274848222733
  1/332 [..............................] - ETA: 46s 38/332 [==>...........................] - ETA: 0s  77/332 [=====>........................] - ETA: 0s116/332 [=========>....................] - ETA: 0s155/332 [=============>................] - ETA: 0s194/332 [================>.............] - ETA: 0s233/332 [====================>.........] - ETA: 0s272/332 [=======================>......] - ETA: 0s311/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.07688778696691949
cosine 0.06035683673932514
MAE: 0.036469504
RMSE: 0.07961009
r2: 0.5888539448650573
RMSE zero-vector: 0.23411466903540806
['1.4custom_VAE', 'mse', 256, 180, 0.0022, 0.6, 758, 0.009549335576593876, 0.009432274848222733, 0.07688778696691949, 0.06035683673932514, 0.03646950423717499, 0.07961008697748184, 0.5888539448650573, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.2999999999999998 165 0.0022 128 2] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1643)         2078395     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1643)        6572        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1643)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3909979     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,487,250
Trainable params: 8,479,162
Non-trainable params: 8,088
__________________________________________________________________________________________________
Epoch 1/165
747/747 - 7s - loss: 0.0178 - val_loss: 0.0098 - 7s/epoch - 9ms/step
Epoch 2/165
747/747 - 4s - loss: 0.0083 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 3/165
747/747 - 4s - loss: 0.0079 - val_loss: 0.0091 - 4s/epoch - 5ms/step
Epoch 4/165
747/747 - 4s - loss: 0.0076 - val_loss: 0.0132 - 4s/epoch - 5ms/step
Epoch 5/165
747/747 - 4s - loss: 0.0073 - val_loss: 0.0075 - 4s/epoch - 5ms/step
Epoch 6/165
747/747 - 4s - loss: 0.0071 - val_loss: 0.0072 - 4s/epoch - 5ms/step
Epoch 7/165
747/747 - 4s - loss: 0.0070 - val_loss: 0.0070 - 4s/epoch - 5ms/step
Epoch 8/165
747/747 - 4s - loss: 0.0069 - val_loss: 0.0069 - 4s/epoch - 5ms/step
Epoch 9/165
747/747 - 4s - loss: 0.0068 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 10/165
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 11/165
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 12/165
747/747 - 4s - loss: 0.0065 - val_loss: 0.0064 - 4s/epoch - 5ms/step
Epoch 13/165
747/747 - 4s - loss: 0.0064 - val_loss: 0.0064 - 4s/epoch - 5ms/step
Epoch 14/165
747/747 - 4s - loss: 0.0064 - val_loss: 0.0063 - 4s/epoch - 5ms/step
Epoch 15/165
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 16/165
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 17/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 18/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 19/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 20/165
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 21/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 22/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 23/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 24/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 25/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 26/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 27/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 28/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 29/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 30/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 31/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 32/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 33/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 34/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 35/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 36/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 37/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 38/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 39/165
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 40/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 41/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 42/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 43/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 44/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 45/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 46/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 47/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 48/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 49/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 50/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 51/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 52/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 53/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 54/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 55/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 56/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 57/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 58/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 59/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 60/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 61/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 62/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 63/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 64/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 65/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 66/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 67/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 68/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 69/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 70/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 71/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 72/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 73/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 74/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 75/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 76/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 77/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 78/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 79/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 80/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 81/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 82/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 83/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 84/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 85/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 86/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 87/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 88/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 89/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 90/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 91/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 92/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 93/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 94/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 95/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 96/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 97/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 98/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 99/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 100/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 101/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 102/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 103/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 104/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 105/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 106/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 107/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 108/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 109/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 110/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 111/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 112/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 113/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 114/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 115/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 116/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 117/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 118/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 119/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 120/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 121/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 122/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 123/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 124/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 125/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 126/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 127/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 128/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 129/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 130/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 131/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 132/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 133/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 134/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 135/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 136/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 137/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 138/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 139/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 140/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 141/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 142/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 143/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 144/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 145/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 146/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 147/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 148/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 149/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 150/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 151/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 152/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 153/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 154/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 155/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 156/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 157/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 158/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 159/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 160/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 161/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 162/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 163/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 164/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 165/165
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.006021867971867323
  1/332 [..............................] - ETA: 49s 38/332 [==>...........................] - ETA: 0s  76/332 [=====>........................] - ETA: 0s112/332 [=========>....................] - ETA: 0s150/332 [============>.................] - ETA: 0s188/332 [===============>..............] - ETA: 0s226/332 [===================>..........] - ETA: 0s264/332 [======================>.......] - ETA: 0s302/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.12380439105390649
cosine 0.09683219867744762
MAE: 0.04661877
RMSE: 0.10012077
r2: 0.3497079854903779
RMSE zero-vector: 0.23411466903540806
['1.2999999999999998custom_VAE', 'logcosh', 128, 165, 0.0022, 0.6, 758, 0.0060676150023937225, 0.006021867971867323, 0.12380439105390649, 0.09683219867744762, 0.04661877080798149, 0.10012076795101166, 0.3497079854903779, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.1999999999999997 165 0.002 128 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1516)         1917740     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1516)        6064        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1516)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1149886     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1149886     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3652550     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,876,126
Trainable params: 7,868,546
Non-trainable params: 7,580
__________________________________________________________________________________________________
Epoch 1/165
747/747 - 10s - loss: 0.0296 - val_loss: 0.0244 - 10s/epoch - 14ms/step
Epoch 2/165
747/747 - 4s - loss: 0.0155 - val_loss: 0.0167 - 4s/epoch - 5ms/step
Epoch 3/165
747/747 - 4s - loss: 0.0146 - val_loss: 0.0148 - 4s/epoch - 5ms/step
Epoch 4/165
747/747 - 4s - loss: 0.0140 - val_loss: 0.0143 - 4s/epoch - 5ms/step
Epoch 5/165
747/747 - 4s - loss: 0.0134 - val_loss: 0.0140 - 4s/epoch - 5ms/step
Epoch 6/165
747/747 - 4s - loss: 0.0130 - val_loss: 0.0129 - 4s/epoch - 5ms/step
Epoch 7/165
747/747 - 4s - loss: 0.0127 - val_loss: 0.0126 - 4s/epoch - 5ms/step
Epoch 8/165
747/747 - 4s - loss: 0.0125 - val_loss: 0.0124 - 4s/epoch - 5ms/step
Epoch 9/165
747/747 - 4s - loss: 0.0123 - val_loss: 0.0119 - 4s/epoch - 5ms/step
Epoch 10/165
747/747 - 4s - loss: 0.0119 - val_loss: 0.0120 - 4s/epoch - 5ms/step
Epoch 11/165
747/747 - 4s - loss: 0.0118 - val_loss: 0.0115 - 4s/epoch - 5ms/step
Epoch 12/165
747/747 - 4s - loss: 0.0115 - val_loss: 0.0112 - 4s/epoch - 5ms/step
Epoch 13/165
747/747 - 4s - loss: 0.0114 - val_loss: 0.0112 - 4s/epoch - 5ms/step
Epoch 14/165
747/747 - 4s - loss: 0.0113 - val_loss: 0.0111 - 4s/epoch - 5ms/step
Epoch 15/165
747/747 - 4s - loss: 0.0112 - val_loss: 0.0112 - 4s/epoch - 5ms/step
Epoch 16/165
747/747 - 4s - loss: 0.0113 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 17/165
747/747 - 4s - loss: 0.0111 - val_loss: 0.0110 - 4s/epoch - 5ms/step
Epoch 18/165
747/747 - 4s - loss: 0.0111 - val_loss: 0.0109 - 4s/epoch - 5ms/step
Epoch 19/165
747/747 - 4s - loss: 0.0111 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 20/165
747/747 - 4s - loss: 0.0110 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 21/165
747/747 - 4s - loss: 0.0110 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 22/165
747/747 - 4s - loss: 0.0110 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 23/165
747/747 - 4s - loss: 0.0109 - val_loss: 0.0108 - 4s/epoch - 5ms/step
Epoch 24/165
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 25/165
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 26/165
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 27/165
747/747 - 4s - loss: 0.0109 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 28/165
747/747 - 4s - loss: 0.0108 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 29/165
747/747 - 4s - loss: 0.0108 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 30/165
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 31/165
747/747 - 4s - loss: 0.0108 - val_loss: 0.0107 - 4s/epoch - 5ms/step
Epoch 32/165
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 33/165
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 34/165
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 35/165
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 36/165
747/747 - 4s - loss: 0.0108 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 37/165
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 38/165
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 39/165
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 40/165
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 41/165
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 42/165
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 43/165
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 44/165
747/747 - 4s - loss: 0.0107 - val_loss: 0.0106 - 4s/epoch - 5ms/step
Epoch 45/165
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 46/165
747/747 - 4s - loss: 0.0106 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 47/165
747/747 - 4s - loss: 0.0107 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 48/165
747/747 - 4s - loss: 0.0106 - val_loss: 0.0105 - 4s/epoch - 5ms/step
Epoch 49/165
747/747 - 4s - loss: 0.0106 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 50/165
747/747 - 4s - loss: 0.0106 - val_loss: 0.0104 - 4s/epoch - 5ms/step
Epoch 51/165
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 52/165
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 53/165
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 54/165
747/747 - 4s - loss: 0.0105 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 55/165
747/747 - 4s - loss: 0.0104 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 56/165
747/747 - 4s - loss: 0.0104 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 57/165
747/747 - 4s - loss: 0.0104 - val_loss: 0.0103 - 4s/epoch - 5ms/step
Epoch 58/165
747/747 - 4s - loss: 0.0104 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 59/165
747/747 - 4s - loss: 0.0103 - val_loss: 0.0102 - 4s/epoch - 5ms/step
Epoch 60/165
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 61/165
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 62/165
747/747 - 4s - loss: 0.0103 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 63/165
747/747 - 4s - loss: 0.0102 - val_loss: 0.0101 - 4s/epoch - 5ms/step
Epoch 64/165
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 65/165
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 66/165
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 67/165
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 68/165
747/747 - 4s - loss: 0.0102 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 69/165
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 70/165
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 71/165
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 72/165
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 73/165
747/747 - 4s - loss: 0.0101 - val_loss: 0.0100 - 4s/epoch - 5ms/step
Epoch 74/165
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 75/165
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 76/165
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 77/165
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 78/165
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 79/165
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 80/165
747/747 - 4s - loss: 0.0101 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 81/165
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 82/165
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 83/165
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 84/165
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 85/165
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 86/165
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 87/165
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 88/165
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 89/165
747/747 - 4s - loss: 0.0100 - val_loss: 0.0099 - 4s/epoch - 5ms/step
Epoch 90/165
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 91/165
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 92/165
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 93/165
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 94/165
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 95/165
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 96/165
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 97/165
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 98/165
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 99/165
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 100/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 101/165
747/747 - 4s - loss: 0.0100 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 102/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 103/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 104/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 105/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 106/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 107/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 108/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 109/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 110/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 111/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 112/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 113/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 114/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 115/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 116/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0098 - 4s/epoch - 5ms/step
Epoch 117/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 118/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 119/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 120/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 121/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 122/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 123/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 124/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 125/165
747/747 - 4s - loss: 0.0099 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 126/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 127/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 128/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0097 - 4s/epoch - 5ms/step
Epoch 129/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 130/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 131/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 132/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 133/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 134/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 135/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 136/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 137/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 138/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 139/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 140/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 141/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 142/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 143/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 144/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 145/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 146/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 147/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 148/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 149/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 150/165
747/747 - 4s - loss: 0.0098 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 151/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 152/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 153/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 154/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 155/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 156/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 157/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0096 - 4s/epoch - 5ms/step
Epoch 158/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 159/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 160/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 161/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 162/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 163/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 164/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
Epoch 165/165
747/747 - 4s - loss: 0.0097 - val_loss: 0.0095 - 4s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009497689083218575
  1/332 [..............................] - ETA: 1:22 23/332 [=>............................] - ETA: 0s   45/332 [===>..........................] - ETA: 0s 67/332 [=====>........................] - ETA: 0s 90/332 [=======>......................] - ETA: 0s112/332 [=========>....................] - ETA: 0s134/332 [===========>..................] - ETA: 0s156/332 [=============>................] - ETA: 0s178/332 [===============>..............] - ETA: 0s201/332 [=================>............] - ETA: 0s223/332 [===================>..........] - ETA: 0s243/332 [====================>.........] - ETA: 0s265/332 [======================>.......] - ETA: 0s287/332 [========================>.....] - ETA: 0s309/332 [==========================>...] - ETA: 0s332/332 [==============================] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.07830581127406515
cosine 0.06145746211173442
MAE: 0.036585506
RMSE: 0.080336645
r2: 0.5813162850525445
RMSE zero-vector: 0.23411466903540806
['1.1999999999999997custom_VAE', 'mse', 128, 165, 0.002, 0.6, 758, 0.009691199287772179, 0.009497689083218575, 0.07830581127406515, 0.06145746211173442, 0.03658550605177879, 0.08033664524555206, 0.5813162850525445, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.2999999999999998 160 0.002 64 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1643)         2078395     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1643)        6572        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1643)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3909979     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,487,250
Trainable params: 8,479,162
Non-trainable params: 8,088
__________________________________________________________________________________________________
Epoch 1/160
2023-02-20 02:39:47.336239: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 53654272 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 29884416/42314694656
2023-02-20 02:39:47.336691: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     40077099008
InUse:                     40931303398
MaxInUse:                  40931303398
NumAllocs:                   532964453
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-20 02:39:47.336778: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-20 02:39:47.336786: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 164
2023-02-20 02:39:47.336790: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 150
2023-02-20 02:39:47.336795: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-20 02:39:47.336800: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 650
2023-02-20 02:39:47.336804: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 114
2023-02-20 02:39:47.336807: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 22
2023-02-20 02:39:47.336812: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 110
2023-02-20 02:39:47.336816: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 208
2023-02-20 02:39:47.336820: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 98
2023-02-20 02:39:47.336824: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 22
2023-02-20 02:39:47.336829: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 110
2023-02-20 02:39:47.336833: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 154
2023-02-20 02:39:47.336837: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 98
2023-02-20 02:39:47.336850: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14156, 22
2023-02-20 02:39:47.336855: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 114
2023-02-20 02:39:47.336859: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 9
2023-02-20 02:39:47.336863: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 45
2023-02-20 02:39:47.336867: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 84
2023-02-20 02:39:47.336872: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 39
2023-02-20 02:39:47.336875: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 9
2023-02-20 02:39:47.336880: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 6
2023-02-20 02:39:47.336883: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 30
2023-02-20 02:39:47.336888: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 56
2023-02-20 02:39:47.336892: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 45
2023-02-20 02:39:47.336896: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 26
2023-02-20 02:39:47.336900: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 63
2023-02-20 02:39:47.336904: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 6
2023-02-20 02:39:47.336907: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 39
2023-02-20 02:39:47.336911: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10730248, 9
2023-02-20 02:39:47.336915: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 30
2023-02-20 02:39:47.336919: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 42
2023-02-20 02:39:47.336925: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 26
2023-02-20 02:39:47.336929: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17893184, 6
2023-02-20 02:39:47.336933: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 45
2023-02-20 02:39:47.336937: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 68
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.2999999999999998custom_VAE', 'mse', 64, 160, 0.002, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.2999999999999998 160 0.002 64 1]) is not valid.
[1.2999999999999998 160 0.002 128 2] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1643)         2078395     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1643)        6572        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1643)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3909979     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,487,250
Trainable params: 8,479,162
Non-trainable params: 8,088
__________________________________________________________________________________________________
2023-02-20 02:39:50.380349: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 29884416/42314694656
2023-02-20 02:39:50.380445: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     40077099008
InUse:                     40911598066
MaxInUse:                  40931303398
NumAllocs:                   532964509
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-20 02:39:50.380522: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-20 02:39:50.380529: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 153
2023-02-20 02:39:50.380533: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 148
2023-02-20 02:39:50.380537: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-20 02:39:50.380541: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 657
2023-02-20 02:39:50.380545: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 115
2023-02-20 02:39:50.380549: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 22
2023-02-20 02:39:50.380553: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 120
2023-02-20 02:39:50.380556: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 208
2023-02-20 02:39:50.380560: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 98
2023-02-20 02:39:50.380564: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 22
2023-02-20 02:39:50.380568: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 110
2023-02-20 02:39:50.380571: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 154
2023-02-20 02:39:50.380575: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 98
2023-02-20 02:39:50.380579: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14156, 22
2023-02-20 02:39:50.380583: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 115
2023-02-20 02:39:50.380587: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 9
2023-02-20 02:39:50.380591: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 48
2023-02-20 02:39:50.380594: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 84
2023-02-20 02:39:50.380598: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 39
2023-02-20 02:39:50.380602: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 9
2023-02-20 02:39:50.380606: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 6
2023-02-20 02:39:50.380610: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 32
2023-02-20 02:39:50.380614: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 56
2023-02-20 02:39:50.380617: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 45
2023-02-20 02:39:50.380621: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 26
2023-02-20 02:39:50.380625: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 63
2023-02-20 02:39:50.380629: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 6
2023-02-20 02:39:50.380643: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 39
2023-02-20 02:39:50.380647: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10730248, 9
2023-02-20 02:39:50.380651: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 30
2023-02-20 02:39:50.380655: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 42
2023-02-20 02:39:50.380658: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 26
2023-02-20 02:39:50.380662: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17893184, 6
2023-02-20 02:39:50.380666: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 44
2023-02-20 02:39:50.380670: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 68
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.2999999999999998custom_VAE', 'logcosh', 128, 160, 0.002, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.2999999999999998 160 0.002 128 2]) is not valid.
[1.2999999999999998 160 0.002 128 2] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1643)         2078395     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1643)        6572        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1643)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3909979     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,487,250
Trainable params: 8,479,162
Non-trainable params: 8,088
__________________________________________________________________________________________________
2023-02-20 02:39:53.492134: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 29884416/42314694656
2023-02-20 02:39:53.492231: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     40077099008
InUse:                     40911598066
MaxInUse:                  40936519114
NumAllocs:                   532964565
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-20 02:39:53.492307: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-20 02:39:53.492314: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 153
2023-02-20 02:39:53.492319: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 148
2023-02-20 02:39:53.492324: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-20 02:39:53.492328: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 657
2023-02-20 02:39:53.492332: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 115
2023-02-20 02:39:53.492337: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 22
2023-02-20 02:39:53.492341: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 120
2023-02-20 02:39:53.492346: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 208
2023-02-20 02:39:53.492349: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 98
2023-02-20 02:39:53.492354: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 22
2023-02-20 02:39:53.492359: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 110
2023-02-20 02:39:53.492363: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 154
2023-02-20 02:39:53.492368: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 98
2023-02-20 02:39:53.492372: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14156, 22
2023-02-20 02:39:53.492376: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 115
2023-02-20 02:39:53.492380: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 9
2023-02-20 02:39:53.492384: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 48
2023-02-20 02:39:53.492389: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 84
2023-02-20 02:39:53.492393: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 39
2023-02-20 02:39:53.492398: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 9
2023-02-20 02:39:53.492401: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 6
2023-02-20 02:39:53.492406: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 32
2023-02-20 02:39:53.492410: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 56
2023-02-20 02:39:53.492414: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 45
2023-02-20 02:39:53.492428: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 26
2023-02-20 02:39:53.492433: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 63
2023-02-20 02:39:53.492438: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 6
2023-02-20 02:39:53.492442: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 39
2023-02-20 02:39:53.492446: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10730248, 9
2023-02-20 02:39:53.492450: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 30
2023-02-20 02:39:53.492454: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 42
2023-02-20 02:39:53.492458: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 26
2023-02-20 02:39:53.492463: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17893184, 6
2023-02-20 02:39:53.492467: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 44
2023-02-20 02:39:53.492471: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 68
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.2999999999999998custom_VAE', 'logcosh', 128, 160, 0.002, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.2999999999999998 160 0.002 128 2]) is not valid.
[1.2999999999999998 155 0.002 128 2] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1643)         2078395     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1643)        6572        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1643)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1246152     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3909979     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,487,250
Trainable params: 8,479,162
Non-trainable params: 8,088
__________________________________________________________________________________________________
2023-02-20 02:39:56.542426: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 29884416/42314694656
2023-02-20 02:39:56.542523: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     40077099008
InUse:                     40945547066
MaxInUse:                  40962156042
NumAllocs:                   532964621
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-20 02:39:56.542600: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-20 02:39:56.542606: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 153
2023-02-20 02:39:56.542610: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 148
2023-02-20 02:39:56.542614: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-20 02:39:56.542618: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 664
2023-02-20 02:39:56.542622: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 116
2023-02-20 02:39:56.542625: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 22
2023-02-20 02:39:56.542629: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 130
2023-02-20 02:39:56.542633: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 208
2023-02-20 02:39:56.542637: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 98
2023-02-20 02:39:56.542641: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 22
2023-02-20 02:39:56.542644: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 110
2023-02-20 02:39:56.542648: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 154
2023-02-20 02:39:56.542652: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 98
2023-02-20 02:39:56.542656: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14156, 22
2023-02-20 02:39:56.542660: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 116
2023-02-20 02:39:56.542663: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 9
2023-02-20 02:39:56.542667: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 51
2023-02-20 02:39:56.542671: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 84
2023-02-20 02:39:56.542675: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 39
2023-02-20 02:39:56.542679: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 9
2023-02-20 02:39:56.542682: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 6
2023-02-20 02:39:56.542698: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 34
2023-02-20 02:39:56.542702: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 56
2023-02-20 02:39:56.542706: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 45
2023-02-20 02:39:56.542710: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 26
2023-02-20 02:39:56.542713: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 63
2023-02-20 02:39:56.542717: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 6
2023-02-20 02:39:56.542721: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 39
2023-02-20 02:39:56.542725: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10730248, 9
2023-02-20 02:39:56.542728: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 30
2023-02-20 02:39:56.542732: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 42
2023-02-20 02:39:56.542736: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 26
2023-02-20 02:39:56.542740: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17893184, 6
2023-02-20 02:39:56.542744: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 44
2023-02-20 02:39:56.542747: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 68
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.2999999999999998custom_VAE', 'logcosh', 128, 155, 0.002, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.2999999999999998 155 0.002 128 2]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_6.pkl
[1.0999999999999996 165 0.002 128 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1390)         1758350     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1390)        5560        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1390)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1054378     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1054378     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3397148     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 7,269,814
Trainable params: 7,262,738
Non-trainable params: 7,076
__________________________________________________________________________________________________
2023-02-20 02:40:03.641404: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 29884416/42314694656
2023-02-20 02:40:03.641512: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     40077099008
InUse:                     40940677330
MaxInUse:                  40962156042
NumAllocs:                   532964679
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-20 02:40:03.641607: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-20 02:40:03.641614: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 155
2023-02-20 02:40:03.641619: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 148
2023-02-20 02:40:03.641624: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-20 02:40:03.641627: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 664
2023-02-20 02:40:03.641631: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 116
2023-02-20 02:40:03.641635: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 10
2023-02-20 02:40:03.641639: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 22
2023-02-20 02:40:03.641643: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 120
2023-02-20 02:40:03.641647: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 208
2023-02-20 02:40:03.641651: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 98
2023-02-20 02:40:03.641655: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 22
2023-02-20 02:40:03.641658: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 110
2023-02-20 02:40:03.641662: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 154
2023-02-20 02:40:03.641666: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 98
2023-02-20 02:40:03.641670: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14156, 22
2023-02-20 02:40:03.641674: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 116
2023-02-20 02:40:03.641678: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4214480, 3
2023-02-20 02:40:03.641692: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 9
2023-02-20 02:40:03.641697: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 48
2023-02-20 02:40:03.641701: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 84
2023-02-20 02:40:03.641705: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 39
2023-02-20 02:40:03.641708: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 9
2023-02-20 02:40:03.641712: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 2
2023-02-20 02:40:03.641716: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 6
2023-02-20 02:40:03.641720: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 32
2023-02-20 02:40:03.641724: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 56
2023-02-20 02:40:03.641727: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 45
2023-02-20 02:40:03.641731: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 26
2023-02-20 02:40:03.641735: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 63
2023-02-20 02:40:03.641739: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 6
2023-02-20 02:40:03.641743: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 39
2023-02-20 02:40:03.641746: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10730248, 9
2023-02-20 02:40:03.641750: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 30
2023-02-20 02:40:03.641754: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 42
2023-02-20 02:40:03.641758: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 26
2023-02-20 02:40:03.641761: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17893184, 6
2023-02-20 02:40:03.641765: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 44
2023-02-20 02:40:03.641769: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 68
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['1.0999999999999996custom_VAE', 'mse', 128, 165, 0.002, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([1.0999999999999996 165 0.002 128 1]) is not valid.
[1.0999999999999996 165 0.0018 128 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
2023-02-20 02:40:05.818828: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 7027840 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 29884416/42314694656
2023-02-20 02:40:05.818892: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     40077099008
InUse:                     40962723706
MaxInUse:                  40971124866
NumAllocs:                   532964728
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-20 02:40:05.818973: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-20 02:40:05.818993: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 153
2023-02-20 02:40:05.818998: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 149
2023-02-20 02:40:05.819003: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16, 1
2023-02-20 02:40:05.819007: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-20 02:40:05.819011: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 671
2023-02-20 02:40:05.819016: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 116
2023-02-20 02:40:05.819020: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5560, 20
2023-02-20 02:40:05.819025: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6064, 22
2023-02-20 02:40:05.819029: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6572, 120
2023-02-20 02:40:05.819034: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 208
2023-02-20 02:40:05.819039: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 98
2023-02-20 02:40:05.819042: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 22
2023-02-20 02:40:05.819047: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 110
2023-02-20 02:40:05.819051: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13144, 154
2023-02-20 02:40:05.819055: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13648, 98
2023-02-20 02:40:05.819060: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 14156, 22
2023-02-20 02:40:05.819065: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 117
2023-02-20 02:40:05.819069: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4214480, 6
2023-02-20 02:40:05.819073: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4596512, 9
2023-02-20 02:40:05.819077: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4981576, 48
2023-02-20 02:40:05.819081: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 84
2023-02-20 02:40:05.819086: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 39
2023-02-20 02:40:05.819090: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 9
2023-02-20 02:40:05.819094: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7027840, 3
2023-02-20 02:40:05.819098: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 6
2023-02-20 02:40:05.819103: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8307008, 32
2023-02-20 02:40:05.819107: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 56
2023-02-20 02:40:05.819111: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9581120, 45
2023-02-20 02:40:05.819115: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 26
2023-02-20 02:40:05.819119: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9963152, 63
2023-02-20 02:40:05.819123: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 6
2023-02-20 02:40:05.819127: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10345184, 39
2023-02-20 02:40:05.819131: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10730248, 9
2023-02-20 02:40:05.819136: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 30
2023-02-20 02:40:05.819140: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16614016, 42
2023-02-20 02:40:05.819146: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17251072, 26
2023-02-20 02:40:05.819151: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 17893184, 6
2023-02-20 02:40:05.819155: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 44
2023-02-20 02:40:05.819159: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 68
2023-02-20 02:40:05.819181: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at stateless_random_ops_v2.cc:67 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[1390,1264] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator gpu_async_0
Traceback (most recent call last):
  File "genetic.py", line 44, in <module>
    ga=continue_run_ga('tmp/ga_instance_generation_4.pkl')
  File "/auto/globalscratch/users/r/g/rgouvea/GeneticAlgorithmTest/GeneticVAE_MMmpgap/cr_0.6/../../genetic_hypertune.py", line 249, in continue_run_ga
    ga_instance.run()
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py", line 1409, in run
    self.last_generation_fitness = self.cal_pop_fitness()
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py", line 1235, in cal_pop_fitness
    fitness = self.fitness_func(sol, sol_idx)
  File "/auto/globalscratch/users/r/g/rgouvea/GeneticAlgorithmTest/GeneticVAE_MMmpgap/cr_0.6/../../genetic_hypertune.py", line 73, in fitness_func
    results_dict=train_autoencoder(prefix_name = prefix_name, 
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/autoencoder_tools-0.0.2-py3.8.egg/autoencoder_tools/autoencoder_setup.py", line 352, in train_autoencoder
    model = create_autoencoder(n_inputs=n_inputs, layers_structure=layers_structure, 
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/autoencoder_tools-0.0.2-py3.8.egg/autoencoder_tools/autoencoder_setup.py", line 585, in create_autoencoder
    decoder_output = Dense(n_inputs, activation='linear', name='outputlayer')(d)
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/utils/traceback_utils.py", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/backend.py", line 2100, in random_uniform
    return tf.random.stateless_uniform(
tensorflow.python.framework.errors_impl.ResourceExhaustedError: {{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[1390,1264] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator gpu_async_0 [Op:StatelessRandomUniformV2]
Mon Feb 20 02:40:43 CET 2023
done
