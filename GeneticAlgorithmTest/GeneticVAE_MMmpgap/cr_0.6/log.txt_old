start
Tue Feb 14 15:43:54 CET 2023
2023-02-14 15:44:00.870955: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-14 15:45:01,407 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7f510ab1fdc0> object, created with modnet version 0.1.12
        AtomicOrbitals|HOMO_character  ...  BondFractions|B - B bond frac.
id                                     ...                                
0                                 3.0  ...                             0.0
1                                 3.0  ...                             0.0
2                                 2.0  ...                             0.0
3                                 2.0  ...                             0.0
4                                 2.0  ...                             0.0
...                               ...  ...                             ...
106108                            3.0  ...                             0.0
106109                            2.0  ...                             0.0
106110                            3.0  ...                             0.0
106111                            3.0  ...                             0.0
106112                            1.0  ...                             0.0

[106113 rows x 1336 columns]
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:486: UserWarning: The percentage of genes to mutate (mutation_percent_genes=10) resutled in selecting (0) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).
If you do not want to mutate any gene, please set mutation_type=None.
  if not self.suppress_warnings: warnings.warn("The percentage of genes to mutate (mutation_percent_genes={mutation_percent}) resutled in selecting ({mutation_num}) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).\nIf you do not want to mutate any gene, please set mutation_type=None.".format(mutation_percent=mutation_percent_genes, mutation_num=mutation_num_genes))
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:820: UserWarning: Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.")
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/pygad/pygad.py:828: UserWarning: Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.")
[[1.5 145 0.0012 64 1]
 [1.5 150 0.001 32 1]
 [1.5 70 0.0005 16 0]
 [2.5 50 0.0005 64 2]
 [2.0 110 0.0005 8 0]
 [2.0 170 0.0005 16 0]
 [2.0 10 0.001 16 1]
 [1.5 90 0.001 32 1]
 [0.5 10 0.001 64 0]
 [1.5 30 0.0005 8 1]]
[1.5 145 0.0012 64 1] 0
Shape of dataset to encode: (106113, 1264)
WARNING:tensorflow:From /home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/layers/normalization/batch_normalization.py:561: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.
2023-02-14 15:45:05.428123: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-14 15:45:10.168502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22291 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:23:00.0, compute capability: 8.6
2023-02-14 15:45:10.170085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22291 MB memory:  -> device: 1, name: GeForce RTX 3090, pci bus id: 0000:41:00.0, compute capability: 8.6
2023-02-14 15:45:10.229461: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled
2023-02-14 15:45:10.890864: W tensorflow/c/c_api.cc:291] Operation '{name:'training/Adam/dense_dec1/kernel/v/Assign' id:1079 op device:{requested: '', assigned: ''} def:{{{node training/Adam/dense_dec1/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/dense_dec1/kernel/v, training/Adam/dense_dec1/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-02-14 15:45:13.341708: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 15:45:22.161945: W tensorflow/c/c_api.cc:291] Operation '{name:'loss/mul' id:452 op device:{requested: '', assigned: ''} def:{{{node loss/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss/mul/x, loss/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 17s - loss: 0.0107 - val_loss: 0.0053 - 17s/epoch - 181us/sample
Epoch 2/145
95501/95501 - 8s - loss: 0.0041 - val_loss: 0.0033 - 8s/epoch - 87us/sample
Epoch 3/145
95501/95501 - 8s - loss: 0.0036 - val_loss: 0.0041 - 8s/epoch - 87us/sample
Epoch 4/145
95501/95501 - 8s - loss: 0.0035 - val_loss: 0.0038 - 8s/epoch - 86us/sample
Epoch 5/145
95501/95501 - 8s - loss: 0.0029 - val_loss: 0.0021 - 8s/epoch - 86us/sample
Epoch 6/145
95501/95501 - 8s - loss: 0.0023 - val_loss: 0.0019 - 8s/epoch - 86us/sample
Epoch 7/145
95501/95501 - 8s - loss: 0.0020 - val_loss: 0.0017 - 8s/epoch - 85us/sample
Epoch 8/145
95501/95501 - 8s - loss: 0.0018 - val_loss: 0.0015 - 8s/epoch - 86us/sample
Epoch 9/145
95501/95501 - 8s - loss: 0.0016 - val_loss: 0.0016 - 8s/epoch - 85us/sample
Epoch 10/145
95501/95501 - 8s - loss: 0.0015 - val_loss: 0.0013 - 8s/epoch - 85us/sample
Epoch 11/145
95501/95501 - 8s - loss: 0.0014 - val_loss: 0.0012 - 8s/epoch - 85us/sample
Epoch 12/145
95501/95501 - 8s - loss: 0.0013 - val_loss: 0.0011 - 8s/epoch - 85us/sample
Epoch 13/145
95501/95501 - 8s - loss: 0.0012 - val_loss: 0.0027 - 8s/epoch - 86us/sample
Epoch 14/145
95501/95501 - 8s - loss: 0.0016 - val_loss: 0.0012 - 8s/epoch - 85us/sample
Epoch 15/145
95501/95501 - 8s - loss: 0.0012 - val_loss: 0.0014 - 8s/epoch - 85us/sample
Epoch 16/145
95501/95501 - 8s - loss: 0.0012 - val_loss: 0.0010 - 8s/epoch - 85us/sample
Epoch 17/145
95501/95501 - 8s - loss: 0.0011 - val_loss: 9.9522e-04 - 8s/epoch - 84us/sample
Epoch 18/145
95501/95501 - 8s - loss: 0.0011 - val_loss: 9.8865e-04 - 8s/epoch - 84us/sample
Epoch 19/145
95501/95501 - 8s - loss: 0.0011 - val_loss: 9.5968e-04 - 8s/epoch - 84us/sample
Epoch 20/145
95501/95501 - 8s - loss: 0.0010 - val_loss: 9.9132e-04 - 8s/epoch - 85us/sample
Epoch 21/145
95501/95501 - 8s - loss: 0.0010 - val_loss: 0.0011 - 8s/epoch - 84us/sample
Epoch 22/145
95501/95501 - 8s - loss: 0.0011 - val_loss: 9.0436e-04 - 8s/epoch - 85us/sample
Epoch 23/145
95501/95501 - 8s - loss: 9.7713e-04 - val_loss: 0.0010 - 8s/epoch - 84us/sample
Epoch 24/145
95501/95501 - 8s - loss: 0.0010 - val_loss: 8.8991e-04 - 8s/epoch - 84us/sample
Epoch 25/145
95501/95501 - 8s - loss: 9.4676e-04 - val_loss: 0.0010 - 8s/epoch - 84us/sample
Epoch 26/145
95501/95501 - 8s - loss: 0.0010 - val_loss: 8.9477e-04 - 8s/epoch - 84us/sample
Epoch 27/145
95501/95501 - 8s - loss: 9.3876e-04 - val_loss: 8.8244e-04 - 8s/epoch - 84us/sample
Epoch 28/145
95501/95501 - 8s - loss: 9.2082e-04 - val_loss: 0.0010 - 8s/epoch - 85us/sample
Epoch 29/145
95501/95501 - 8s - loss: 0.0010 - val_loss: 8.3987e-04 - 8s/epoch - 84us/sample
Epoch 30/145
95501/95501 - 8s - loss: 9.0430e-04 - val_loss: 8.8765e-04 - 8s/epoch - 84us/sample
Epoch 31/145
95501/95501 - 8s - loss: 8.9459e-04 - val_loss: 8.4267e-04 - 8s/epoch - 84us/sample
Epoch 32/145
95501/95501 - 8s - loss: 8.8522e-04 - val_loss: 8.2994e-04 - 8s/epoch - 84us/sample
Epoch 33/145
95501/95501 - 8s - loss: 8.7749e-04 - val_loss: 8.1872e-04 - 8s/epoch - 84us/sample
Epoch 34/145
95501/95501 - 8s - loss: 8.7196e-04 - val_loss: 9.7454e-04 - 8s/epoch - 84us/sample
Epoch 35/145
95501/95501 - 8s - loss: 9.5795e-04 - val_loss: 8.5564e-04 - 8s/epoch - 84us/sample
Epoch 36/145
95501/95501 - 8s - loss: 8.8319e-04 - val_loss: 8.0373e-04 - 8s/epoch - 84us/sample
Epoch 37/145
95501/95501 - 8s - loss: 8.5252e-04 - val_loss: 8.1084e-04 - 8s/epoch - 84us/sample
Epoch 38/145
95501/95501 - 8s - loss: 8.5833e-04 - val_loss: 0.0011 - 8s/epoch - 84us/sample
Epoch 39/145
95501/95501 - 8s - loss: 0.0011 - val_loss: 0.0013 - 8s/epoch - 84us/sample
Epoch 40/145
95501/95501 - 8s - loss: 0.0011 - val_loss: 8.6436e-04 - 8s/epoch - 84us/sample
Epoch 41/145
95501/95501 - 8s - loss: 8.7572e-04 - val_loss: 8.8221e-04 - 8s/epoch - 84us/sample
Epoch 42/145
95501/95501 - 8s - loss: 9.6065e-04 - val_loss: 9.0633e-04 - 8s/epoch - 84us/sample
Epoch 43/145
95501/95501 - 8s - loss: 8.6985e-04 - val_loss: 7.8851e-04 - 8s/epoch - 84us/sample
Epoch 44/145
95501/95501 - 8s - loss: 8.4033e-04 - val_loss: 8.5279e-04 - 8s/epoch - 84us/sample
Epoch 45/145
95501/95501 - 8s - loss: 9.2370e-04 - val_loss: 7.8518e-04 - 8s/epoch - 84us/sample
Epoch 46/145
95501/95501 - 8s - loss: 8.3447e-04 - val_loss: 7.8103e-04 - 8s/epoch - 85us/sample
Epoch 47/145
95501/95501 - 8s - loss: 8.3012e-04 - val_loss: 7.6215e-04 - 8s/epoch - 84us/sample
Epoch 48/145
95501/95501 - 8s - loss: 8.2298e-04 - val_loss: 7.6332e-04 - 8s/epoch - 84us/sample
Epoch 49/145
95501/95501 - 8s - loss: 8.1507e-04 - val_loss: 8.5230e-04 - 8s/epoch - 84us/sample
Epoch 50/145
95501/95501 - 8s - loss: 8.6044e-04 - val_loss: 7.6090e-04 - 8s/epoch - 84us/sample
Epoch 51/145
95501/95501 - 8s - loss: 8.1320e-04 - val_loss: 7.5393e-04 - 8s/epoch - 84us/sample
Epoch 52/145
95501/95501 - 8s - loss: 8.1528e-04 - val_loss: 7.6609e-04 - 8s/epoch - 84us/sample
Epoch 53/145
95501/95501 - 8s - loss: 8.0158e-04 - val_loss: 7.8303e-04 - 8s/epoch - 84us/sample
Epoch 54/145
95501/95501 - 8s - loss: 8.2770e-04 - val_loss: 7.7763e-04 - 8s/epoch - 84us/sample
Epoch 55/145
95501/95501 - 8s - loss: 8.2674e-04 - val_loss: 7.5285e-04 - 8s/epoch - 84us/sample
Epoch 56/145
95501/95501 - 8s - loss: 7.9678e-04 - val_loss: 7.4167e-04 - 8s/epoch - 85us/sample
Epoch 57/145
95501/95501 - 8s - loss: 7.8905e-04 - val_loss: 7.3277e-04 - 8s/epoch - 84us/sample
Epoch 58/145
95501/95501 - 8s - loss: 7.8394e-04 - val_loss: 7.3876e-04 - 8s/epoch - 84us/sample
Epoch 59/145
95501/95501 - 8s - loss: 8.0782e-04 - val_loss: 7.6452e-04 - 8s/epoch - 84us/sample
Epoch 60/145
95501/95501 - 8s - loss: 7.9590e-04 - val_loss: 7.4586e-04 - 8s/epoch - 83us/sample
Epoch 61/145
95501/95501 - 8s - loss: 8.0203e-04 - val_loss: 7.5679e-04 - 8s/epoch - 84us/sample
Epoch 62/145
95501/95501 - 9s - loss: 7.9926e-04 - val_loss: 7.3537e-04 - 9s/epoch - 91us/sample
Epoch 63/145
95501/95501 - 9s - loss: 7.7691e-04 - val_loss: 7.2947e-04 - 9s/epoch - 99us/sample
Epoch 64/145
95501/95501 - 10s - loss: 7.7408e-04 - val_loss: 7.2840e-04 - 10s/epoch - 100us/sample
Epoch 65/145
95501/95501 - 9s - loss: 7.6669e-04 - val_loss: 7.7347e-04 - 9s/epoch - 99us/sample
Epoch 66/145
95501/95501 - 10s - loss: 8.4831e-04 - val_loss: 7.3172e-04 - 10s/epoch - 103us/sample
Epoch 67/145
95501/95501 - 9s - loss: 7.7252e-04 - val_loss: 9.7260e-04 - 9s/epoch - 99us/sample
Epoch 68/145
95501/95501 - 9s - loss: 9.3321e-04 - val_loss: 0.0011 - 9s/epoch - 99us/sample
Epoch 69/145
95501/95501 - 9s - loss: 9.2941e-04 - val_loss: 7.5409e-04 - 9s/epoch - 99us/sample
Epoch 70/145
95501/95501 - 10s - loss: 7.8775e-04 - val_loss: 7.3101e-04 - 10s/epoch - 100us/sample
Epoch 71/145
95501/95501 - 9s - loss: 7.7336e-04 - val_loss: 7.8327e-04 - 9s/epoch - 99us/sample
Epoch 72/145
95501/95501 - 10s - loss: 8.1522e-04 - val_loss: 7.1552e-04 - 10s/epoch - 100us/sample
Epoch 73/145
95501/95501 - 9s - loss: 7.6691e-04 - val_loss: 7.2731e-04 - 9s/epoch - 99us/sample
Epoch 74/145
95501/95501 - 10s - loss: 8.0967e-04 - val_loss: 0.0019 - 10s/epoch - 100us/sample
Epoch 75/145
95501/95501 - 9s - loss: 8.3717e-04 - val_loss: 7.2647e-04 - 9s/epoch - 99us/sample
Epoch 76/145
95501/95501 - 10s - loss: 7.6579e-04 - val_loss: 7.2172e-04 - 10s/epoch - 100us/sample
Epoch 77/145
95501/95501 - 9s - loss: 7.9436e-04 - val_loss: 7.1090e-04 - 9s/epoch - 99us/sample
Epoch 78/145
95501/95501 - 9s - loss: 7.5739e-04 - val_loss: 7.1140e-04 - 9s/epoch - 99us/sample
Epoch 79/145
95501/95501 - 9s - loss: 7.5160e-04 - val_loss: 7.6669e-04 - 9s/epoch - 99us/sample
Epoch 80/145
95501/95501 - 10s - loss: 7.9677e-04 - val_loss: 7.0210e-04 - 10s/epoch - 100us/sample
Epoch 81/145
95501/95501 - 9s - loss: 7.4662e-04 - val_loss: 7.1313e-04 - 9s/epoch - 99us/sample
Epoch 82/145
95501/95501 - 10s - loss: 8.0517e-04 - val_loss: 7.1330e-04 - 10s/epoch - 100us/sample
Epoch 83/145
95501/95501 - 9s - loss: 7.5006e-04 - val_loss: 7.0606e-04 - 9s/epoch - 99us/sample
Epoch 84/145
95501/95501 - 10s - loss: 7.5276e-04 - val_loss: 7.5486e-04 - 10s/epoch - 100us/sample
Epoch 85/145
95501/95501 - 9s - loss: 7.6560e-04 - val_loss: 6.9869e-04 - 9s/epoch - 99us/sample
Epoch 86/145
95501/95501 - 10s - loss: 7.4040e-04 - val_loss: 6.8884e-04 - 10s/epoch - 100us/sample
Epoch 87/145
95501/95501 - 9s - loss: 7.3975e-04 - val_loss: 6.9639e-04 - 9s/epoch - 99us/sample
Epoch 88/145
95501/95501 - 10s - loss: 7.4074e-04 - val_loss: 8.1412e-04 - 10s/epoch - 99us/sample
Epoch 89/145
95501/95501 - 10s - loss: 7.8698e-04 - val_loss: 6.9897e-04 - 10s/epoch - 100us/sample
Epoch 90/145
95501/95501 - 10s - loss: 7.4236e-04 - val_loss: 0.0015 - 10s/epoch - 100us/sample
Epoch 91/145
95501/95501 - 9s - loss: 7.8527e-04 - val_loss: 6.9994e-04 - 9s/epoch - 99us/sample
Epoch 92/145
95501/95501 - 10s - loss: 7.3756e-04 - val_loss: 7.4360e-04 - 10s/epoch - 100us/sample
Epoch 93/145
95501/95501 - 9s - loss: 7.7261e-04 - val_loss: 8.4037e-04 - 9s/epoch - 99us/sample
Epoch 94/145
95501/95501 - 10s - loss: 8.5372e-04 - val_loss: 7.3731e-04 - 10s/epoch - 100us/sample
Epoch 95/145
95501/95501 - 9s - loss: 7.7649e-04 - val_loss: 8.6006e-04 - 9s/epoch - 99us/sample
Epoch 96/145
95501/95501 - 9s - loss: 8.7739e-04 - val_loss: 7.1276e-04 - 9s/epoch - 99us/sample
Epoch 97/145
95501/95501 - 9s - loss: 7.4972e-04 - val_loss: 6.9421e-04 - 9s/epoch - 99us/sample
Epoch 98/145
95501/95501 - 10s - loss: 7.5113e-04 - val_loss: 6.9496e-04 - 10s/epoch - 100us/sample
Epoch 99/145
95501/95501 - 9s - loss: 7.3515e-04 - val_loss: 6.9209e-04 - 9s/epoch - 99us/sample
Epoch 100/145
95501/95501 - 10s - loss: 7.3385e-04 - val_loss: 7.2417e-04 - 10s/epoch - 100us/sample
Epoch 101/145
95501/95501 - 9s - loss: 7.4242e-04 - val_loss: 7.1013e-04 - 9s/epoch - 99us/sample
Epoch 102/145
95501/95501 - 10s - loss: 7.4294e-04 - val_loss: 6.9203e-04 - 10s/epoch - 100us/sample
Epoch 103/145
95501/95501 - 9s - loss: 7.3737e-04 - val_loss: 6.8940e-04 - 9s/epoch - 99us/sample
Epoch 104/145
95501/95501 - 10s - loss: 7.2510e-04 - val_loss: 6.8785e-04 - 10s/epoch - 99us/sample
Epoch 105/145
95501/95501 - 9s - loss: 7.2301e-04 - val_loss: 7.4083e-04 - 9s/epoch - 99us/sample
Epoch 106/145
95501/95501 - 9s - loss: 7.9008e-04 - val_loss: 7.7541e-04 - 9s/epoch - 99us/sample
Epoch 107/145
95501/95501 - 9s - loss: 7.9525e-04 - val_loss: 9.2780e-04 - 9s/epoch - 99us/sample
Epoch 108/145
95501/95501 - 10s - loss: 8.4911e-04 - val_loss: 6.9835e-04 - 10s/epoch - 100us/sample
Epoch 109/145
95501/95501 - 9s - loss: 7.3871e-04 - val_loss: 7.5975e-04 - 9s/epoch - 99us/sample
Epoch 110/145
95501/95501 - 10s - loss: 8.0434e-04 - val_loss: 8.0406e-04 - 10s/epoch - 100us/sample
Epoch 111/145
95501/95501 - 9s - loss: 7.3992e-04 - val_loss: 7.6792e-04 - 9s/epoch - 99us/sample
Epoch 112/145
95501/95501 - 10s - loss: 8.2397e-04 - val_loss: 7.0515e-04 - 10s/epoch - 100us/sample
Epoch 113/145
95501/95501 - 9s - loss: 7.3527e-04 - val_loss: 7.2416e-04 - 9s/epoch - 99us/sample
Epoch 114/145
95501/95501 - 9s - loss: 7.8034e-04 - val_loss: 6.9627e-04 - 9s/epoch - 99us/sample
Epoch 115/145
95501/95501 - 9s - loss: 7.3003e-04 - val_loss: 6.9950e-04 - 9s/epoch - 99us/sample
Epoch 116/145
95501/95501 - 10s - loss: 7.3410e-04 - val_loss: 6.8644e-04 - 10s/epoch - 100us/sample
Epoch 117/145
95501/95501 - 10s - loss: 7.2034e-04 - val_loss: 7.4475e-04 - 10s/epoch - 100us/sample
Epoch 118/145
95501/95501 - 10s - loss: 7.8452e-04 - val_loss: 7.0132e-04 - 10s/epoch - 100us/sample
Epoch 119/145
95501/95501 - 9s - loss: 7.3373e-04 - val_loss: 6.9410e-04 - 9s/epoch - 99us/sample
Epoch 120/145
95501/95501 - 10s - loss: 7.3364e-04 - val_loss: 6.8210e-04 - 10s/epoch - 100us/sample
Epoch 121/145
95501/95501 - 9s - loss: 7.3334e-04 - val_loss: 0.0025 - 9s/epoch - 99us/sample
Epoch 122/145
95501/95501 - 10s - loss: 7.8888e-04 - val_loss: 6.7835e-04 - 10s/epoch - 100us/sample
Epoch 123/145
95501/95501 - 9s - loss: 7.1678e-04 - val_loss: 7.1963e-04 - 9s/epoch - 99us/sample
Epoch 124/145
95501/95501 - 10s - loss: 7.3734e-04 - val_loss: 6.8670e-04 - 10s/epoch - 100us/sample
Epoch 125/145
95501/95501 - 10s - loss: 7.2197e-04 - val_loss: 6.7915e-04 - 10s/epoch - 100us/sample
Epoch 126/145
95501/95501 - 10s - loss: 7.1858e-04 - val_loss: 6.7133e-04 - 10s/epoch - 100us/sample
Epoch 127/145
95501/95501 - 9s - loss: 7.2078e-04 - val_loss: 7.7766e-04 - 9s/epoch - 99us/sample
Epoch 128/145
95501/95501 - 10s - loss: 7.7435e-04 - val_loss: 7.1189e-04 - 10s/epoch - 100us/sample
Epoch 129/145
95501/95501 - 9s - loss: 7.6883e-04 - val_loss: 6.7642e-04 - 9s/epoch - 99us/sample
Epoch 130/145
95501/95501 - 10s - loss: 7.1487e-04 - val_loss: 6.6908e-04 - 10s/epoch - 100us/sample
Epoch 131/145
95501/95501 - 9s - loss: 7.1165e-04 - val_loss: 7.0800e-04 - 9s/epoch - 99us/sample
Epoch 132/145
95501/95501 - 9s - loss: 7.3889e-04 - val_loss: 6.8382e-04 - 9s/epoch - 99us/sample
Epoch 133/145
95501/95501 - 9s - loss: 7.1398e-04 - val_loss: 7.9716e-04 - 9s/epoch - 99us/sample
Epoch 134/145
95501/95501 - 9s - loss: 8.1912e-04 - val_loss: 7.1292e-04 - 9s/epoch - 99us/sample
Epoch 135/145
95501/95501 - 9s - loss: 7.3862e-04 - val_loss: 6.8100e-04 - 9s/epoch - 99us/sample
Epoch 136/145
95501/95501 - 10s - loss: 7.1863e-04 - val_loss: 7.5658e-04 - 10s/epoch - 100us/sample
Epoch 137/145
95501/95501 - 9s - loss: 7.6742e-04 - val_loss: 7.0827e-04 - 9s/epoch - 99us/sample
Epoch 138/145
95501/95501 - 10s - loss: 7.5478e-04 - val_loss: 6.6766e-04 - 10s/epoch - 99us/sample
Epoch 139/145
95501/95501 - 9s - loss: 7.0987e-04 - val_loss: 6.8164e-04 - 9s/epoch - 99us/sample
Epoch 140/145
95501/95501 - 10s - loss: 7.1217e-04 - val_loss: 6.6582e-04 - 10s/epoch - 100us/sample
Epoch 141/145
95501/95501 - 9s - loss: 7.0705e-04 - val_loss: 6.7667e-04 - 9s/epoch - 99us/sample
Epoch 142/145
95501/95501 - 10s - loss: 7.0308e-04 - val_loss: 6.6592e-04 - 10s/epoch - 100us/sample
Epoch 143/145
95501/95501 - 9s - loss: 7.0726e-04 - val_loss: 6.6433e-04 - 9s/epoch - 99us/sample
Epoch 144/145
95501/95501 - 10s - loss: 7.0156e-04 - val_loss: 6.9435e-04 - 10s/epoch - 100us/sample
Epoch 145/145
95501/95501 - 10s - loss: 7.1989e-04 - val_loss: 6.6945e-04 - 10s/epoch - 100us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0006694460165131552
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 16:06:44.008795: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model/outputlayer/BiasAdd' id:423 op device:{requested: '', assigned: ''} def:{{{node decoder_model/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model/outputlayer/MatMul, decoder_model/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.004459966129956495
cosine 0.003517421076893582
MAE: 0.010385033
RMSE: 0.019629098
r2: 0.9750063462941515
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'mse', 64, 145, 0.0012, 0.6, 758, 0.0007198874706225832, 0.0006694460165131552, 0.004459966129956495, 0.003517421076893582, 0.010385032743215561, 0.0196290984749794, 0.9750063462941515, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 150 0.001 32 1] 1
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_3 (BatchNo  (None, 1896)        7584        ['dense_enc0[0][0]']             
 rmalization)                                                                                     
                                                                                                  
 re_lu_3 (ReLU)                 (None, 1896)         0           ['batch_normalization_3[0][0]']  
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu_3[0][0]']                
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu_3[0][0]']                
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/150
2023-02-14 16:06:51.607024: W tensorflow/c/c_api.cc:291] Operation '{name:'training_2/Adam/beta_1/Assign' id:2150 op device:{requested: '', assigned: ''} def:{{{node training_2/Adam/beta_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_2/Adam/beta_1, training_2/Adam/beta_1/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 16:07:08.092640: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_1/mul' id:1719 op device:{requested: '', assigned: ''} def:{{{node loss_1/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_1/mul/x, loss_1/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 17s - loss: 0.0095 - val_loss: 0.0047 - 17s/epoch - 183us/sample
Epoch 2/150
95501/95501 - 16s - loss: 0.0061 - val_loss: 0.0235 - 16s/epoch - 172us/sample
Epoch 3/150
95501/95501 - 16s - loss: 0.0034 - val_loss: 0.0027 - 16s/epoch - 171us/sample
Epoch 4/150
95501/95501 - 16s - loss: 0.0029 - val_loss: 0.0024 - 16s/epoch - 172us/sample
Epoch 5/150
95501/95501 - 16s - loss: 0.0023 - val_loss: 0.0018 - 16s/epoch - 173us/sample
Epoch 6/150
95501/95501 - 16s - loss: 0.0019 - val_loss: 0.0017 - 16s/epoch - 172us/sample
Epoch 7/150
95501/95501 - 16s - loss: 0.0018 - val_loss: 0.0015 - 16s/epoch - 171us/sample
Epoch 8/150
95501/95501 - 16s - loss: 0.0016 - val_loss: 0.0014 - 16s/epoch - 172us/sample
Epoch 9/150
95501/95501 - 16s - loss: 0.0015 - val_loss: 0.0013 - 16s/epoch - 171us/sample
Epoch 10/150
95501/95501 - 16s - loss: 0.0014 - val_loss: 0.0012 - 16s/epoch - 173us/sample
Epoch 11/150
95501/95501 - 16s - loss: 0.0013 - val_loss: 0.0012 - 16s/epoch - 172us/sample
Epoch 12/150
95501/95501 - 16s - loss: 0.0013 - val_loss: 0.0011 - 16s/epoch - 172us/sample
Epoch 13/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0011 - 16s/epoch - 172us/sample
Epoch 14/150
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0011 - 17s/epoch - 173us/sample
Epoch 15/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0010 - 16s/epoch - 172us/sample
Epoch 16/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0010 - 16s/epoch - 172us/sample
Epoch 17/150
95501/95501 - 16s - loss: 0.0012 - val_loss: 0.0010 - 16s/epoch - 172us/sample
Epoch 18/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.6811e-04 - 16s/epoch - 173us/sample
Epoch 19/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.5188e-04 - 16s/epoch - 172us/sample
Epoch 20/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.4930e-04 - 16s/epoch - 172us/sample
Epoch 21/150
95501/95501 - 16s - loss: 0.0011 - val_loss: 9.6237e-04 - 16s/epoch - 171us/sample
Epoch 22/150
95501/95501 - 16s - loss: 0.0010 - val_loss: 9.2612e-04 - 16s/epoch - 172us/sample
Epoch 23/150
95501/95501 - 16s - loss: 0.0010 - val_loss: 9.2273e-04 - 16s/epoch - 172us/sample
Epoch 24/150
95501/95501 - 16s - loss: 0.0010 - val_loss: 9.5242e-04 - 16s/epoch - 172us/sample
Epoch 25/150
95501/95501 - 16s - loss: 0.0010 - val_loss: 8.9788e-04 - 16s/epoch - 171us/sample
Epoch 26/150
95501/95501 - 16s - loss: 0.0010 - val_loss: 9.0989e-04 - 16s/epoch - 172us/sample
Epoch 27/150
95501/95501 - 16s - loss: 9.9590e-04 - val_loss: 9.0376e-04 - 16s/epoch - 172us/sample
Epoch 28/150
95501/95501 - 16s - loss: 0.0010 - val_loss: 8.7465e-04 - 16s/epoch - 172us/sample
Epoch 29/150
95501/95501 - 16s - loss: 9.8817e-04 - val_loss: 8.7030e-04 - 16s/epoch - 171us/sample
Epoch 30/150
95501/95501 - 16s - loss: 9.8274e-04 - val_loss: 8.6862e-04 - 16s/epoch - 172us/sample
Epoch 31/150
95501/95501 - 16s - loss: 9.6828e-04 - val_loss: 8.5372e-04 - 16s/epoch - 171us/sample
Epoch 32/150
95501/95501 - 16s - loss: 9.6870e-04 - val_loss: 8.4826e-04 - 16s/epoch - 173us/sample
Epoch 33/150
95501/95501 - 16s - loss: 9.5886e-04 - val_loss: 9.0106e-04 - 16s/epoch - 172us/sample
Epoch 34/150
95501/95501 - 16s - loss: 9.5155e-04 - val_loss: 9.2564e-04 - 16s/epoch - 172us/sample
Epoch 35/150
95501/95501 - 16s - loss: 9.8007e-04 - val_loss: 8.6506e-04 - 16s/epoch - 172us/sample
Epoch 36/150
95501/95501 - 16s - loss: 9.5411e-04 - val_loss: 8.4836e-04 - 16s/epoch - 172us/sample
Epoch 37/150
95501/95501 - 16s - loss: 9.6355e-04 - val_loss: 8.4040e-04 - 16s/epoch - 172us/sample
Epoch 38/150
95501/95501 - 16s - loss: 9.4242e-04 - val_loss: 8.4328e-04 - 16s/epoch - 172us/sample
Epoch 39/150
95501/95501 - 15s - loss: 9.3480e-04 - val_loss: 8.2770e-04 - 15s/epoch - 161us/sample
Epoch 40/150
95501/95501 - 15s - loss: 9.5386e-04 - val_loss: 8.2665e-04 - 15s/epoch - 161us/sample
Epoch 41/150
95501/95501 - 16s - loss: 9.2424e-04 - val_loss: 8.1749e-04 - 16s/epoch - 163us/sample
Epoch 42/150
95501/95501 - 15s - loss: 9.1985e-04 - val_loss: 8.1358e-04 - 15s/epoch - 162us/sample
Epoch 43/150
95501/95501 - 15s - loss: 9.3774e-04 - val_loss: 8.9584e-04 - 15s/epoch - 162us/sample
Epoch 44/150
95501/95501 - 15s - loss: 9.1864e-04 - val_loss: 8.0752e-04 - 15s/epoch - 162us/sample
Epoch 45/150
95501/95501 - 15s - loss: 9.2464e-04 - val_loss: 8.0848e-04 - 15s/epoch - 162us/sample
Epoch 46/150
95501/95501 - 15s - loss: 9.1398e-04 - val_loss: 8.0788e-04 - 15s/epoch - 162us/sample
Epoch 47/150
95501/95501 - 15s - loss: 9.0441e-04 - val_loss: 8.0679e-04 - 15s/epoch - 162us/sample
Epoch 48/150
95501/95501 - 15s - loss: 9.1322e-04 - val_loss: 8.0330e-04 - 15s/epoch - 162us/sample
Epoch 49/150
95501/95501 - 15s - loss: 8.9730e-04 - val_loss: 8.0458e-04 - 15s/epoch - 162us/sample
Epoch 50/150
95501/95501 - 16s - loss: 8.9451e-04 - val_loss: 7.9952e-04 - 16s/epoch - 163us/sample
Epoch 51/150
95501/95501 - 15s - loss: 8.9290e-04 - val_loss: 7.9640e-04 - 15s/epoch - 162us/sample
Epoch 52/150
95501/95501 - 15s - loss: 8.9613e-04 - val_loss: 7.9369e-04 - 15s/epoch - 162us/sample
Epoch 53/150
95501/95501 - 15s - loss: 8.9181e-04 - val_loss: 7.8816e-04 - 15s/epoch - 162us/sample
Epoch 54/150
95501/95501 - 16s - loss: 8.8584e-04 - val_loss: 7.8945e-04 - 16s/epoch - 163us/sample
Epoch 55/150
95501/95501 - 15s - loss: 8.8389e-04 - val_loss: 7.8303e-04 - 15s/epoch - 162us/sample
Epoch 56/150
95501/95501 - 15s - loss: 9.0281e-04 - val_loss: 7.7564e-04 - 15s/epoch - 162us/sample
Epoch 57/150
95501/95501 - 15s - loss: 8.7940e-04 - val_loss: 7.8055e-04 - 15s/epoch - 162us/sample
Epoch 58/150
95501/95501 - 16s - loss: 8.7554e-04 - val_loss: 7.7280e-04 - 16s/epoch - 162us/sample
Epoch 59/150
95501/95501 - 15s - loss: 8.7958e-04 - val_loss: 7.7271e-04 - 15s/epoch - 162us/sample
Epoch 60/150
95501/95501 - 15s - loss: 8.7330e-04 - val_loss: 7.7670e-04 - 15s/epoch - 162us/sample
Epoch 61/150
95501/95501 - 15s - loss: 8.7138e-04 - val_loss: 7.6743e-04 - 15s/epoch - 162us/sample
Epoch 62/150
95501/95501 - 16s - loss: 8.8241e-04 - val_loss: 7.7484e-04 - 16s/epoch - 162us/sample
Epoch 63/150
95501/95501 - 15s - loss: 8.7007e-04 - val_loss: 7.7631e-04 - 15s/epoch - 162us/sample
Epoch 64/150
95501/95501 - 15s - loss: 8.8200e-04 - val_loss: 7.7163e-04 - 15s/epoch - 162us/sample
Epoch 65/150
95501/95501 - 15s - loss: 8.7666e-04 - val_loss: 7.6938e-04 - 15s/epoch - 162us/sample
Epoch 66/150
95501/95501 - 15s - loss: 8.6047e-04 - val_loss: 7.6530e-04 - 15s/epoch - 162us/sample
Epoch 67/150
95501/95501 - 16s - loss: 8.5855e-04 - val_loss: 7.6384e-04 - 16s/epoch - 163us/sample
Epoch 68/150
95501/95501 - 15s - loss: 8.5815e-04 - val_loss: 7.6257e-04 - 15s/epoch - 162us/sample
Epoch 69/150
95501/95501 - 15s - loss: 8.6705e-04 - val_loss: 7.6165e-04 - 15s/epoch - 162us/sample
Epoch 70/150
95501/95501 - 15s - loss: 8.6086e-04 - val_loss: 7.7636e-04 - 15s/epoch - 162us/sample
Epoch 71/150
95501/95501 - 16s - loss: 8.6129e-04 - val_loss: 7.5298e-04 - 16s/epoch - 163us/sample
Epoch 72/150
95501/95501 - 15s - loss: 8.5203e-04 - val_loss: 7.6327e-04 - 15s/epoch - 162us/sample
Epoch 73/150
95501/95501 - 15s - loss: 8.4951e-04 - val_loss: 7.6333e-04 - 15s/epoch - 162us/sample
Epoch 74/150
95501/95501 - 15s - loss: 8.8364e-04 - val_loss: 7.7795e-04 - 15s/epoch - 162us/sample
Epoch 75/150
95501/95501 - 16s - loss: 8.5038e-04 - val_loss: 7.5157e-04 - 16s/epoch - 163us/sample
Epoch 76/150
95501/95501 - 15s - loss: 8.4993e-04 - val_loss: 7.5881e-04 - 15s/epoch - 162us/sample
Epoch 77/150
95501/95501 - 15s - loss: 8.4361e-04 - val_loss: 7.4571e-04 - 15s/epoch - 162us/sample
Epoch 78/150
95501/95501 - 15s - loss: 8.4508e-04 - val_loss: 0.0010 - 15s/epoch - 162us/sample
Epoch 79/150
95501/95501 - 16s - loss: 8.4498e-04 - val_loss: 7.6489e-04 - 16s/epoch - 163us/sample
Epoch 80/150
95501/95501 - 15s - loss: 8.5102e-04 - val_loss: 7.5706e-04 - 15s/epoch - 162us/sample
Epoch 81/150
95501/95501 - 15s - loss: 8.4650e-04 - val_loss: 7.4283e-04 - 15s/epoch - 162us/sample
Epoch 82/150
95501/95501 - 15s - loss: 8.4850e-04 - val_loss: 7.4843e-04 - 15s/epoch - 162us/sample
Epoch 83/150
95501/95501 - 16s - loss: 8.4187e-04 - val_loss: 7.4443e-04 - 16s/epoch - 163us/sample
Epoch 84/150
95501/95501 - 15s - loss: 8.6287e-04 - val_loss: 7.8650e-04 - 15s/epoch - 162us/sample
Epoch 85/150
95501/95501 - 15s - loss: 8.4456e-04 - val_loss: 7.5565e-04 - 15s/epoch - 162us/sample
Epoch 86/150
95501/95501 - 15s - loss: 8.5039e-04 - val_loss: 7.4813e-04 - 15s/epoch - 162us/sample
Epoch 87/150
95501/95501 - 16s - loss: 8.3403e-04 - val_loss: 7.4931e-04 - 16s/epoch - 163us/sample
Epoch 88/150
95501/95501 - 15s - loss: 8.5460e-04 - val_loss: 7.3849e-04 - 15s/epoch - 162us/sample
Epoch 89/150
95501/95501 - 15s - loss: 8.3601e-04 - val_loss: 7.5224e-04 - 15s/epoch - 162us/sample
Epoch 90/150
95501/95501 - 15s - loss: 8.3348e-04 - val_loss: 7.4328e-04 - 15s/epoch - 162us/sample
Epoch 91/150
95501/95501 - 16s - loss: 8.3752e-04 - val_loss: 7.3969e-04 - 16s/epoch - 163us/sample
Epoch 92/150
95501/95501 - 15s - loss: 8.3392e-04 - val_loss: 7.3270e-04 - 15s/epoch - 162us/sample
Epoch 93/150
95501/95501 - 15s - loss: 8.2999e-04 - val_loss: 7.4350e-04 - 15s/epoch - 161us/sample
Epoch 94/150
95501/95501 - 15s - loss: 8.3227e-04 - val_loss: 7.4161e-04 - 15s/epoch - 162us/sample
Epoch 95/150
95501/95501 - 15s - loss: 8.5554e-04 - val_loss: 8.2153e-04 - 15s/epoch - 162us/sample
Epoch 96/150
95501/95501 - 16s - loss: 8.4809e-04 - val_loss: 7.3631e-04 - 16s/epoch - 163us/sample
Epoch 97/150
95501/95501 - 15s - loss: 8.3592e-04 - val_loss: 0.0013 - 15s/epoch - 162us/sample
Epoch 98/150
95501/95501 - 15s - loss: 8.5722e-04 - val_loss: 7.5057e-04 - 15s/epoch - 161us/sample
Epoch 99/150
95501/95501 - 15s - loss: 8.2939e-04 - val_loss: 7.4332e-04 - 15s/epoch - 162us/sample
Epoch 100/150
95501/95501 - 15s - loss: 8.2593e-04 - val_loss: 8.0535e-04 - 15s/epoch - 162us/sample
Epoch 101/150
95501/95501 - 16s - loss: 8.3120e-04 - val_loss: 7.6588e-04 - 16s/epoch - 162us/sample
Epoch 102/150
95501/95501 - 15s - loss: 8.2859e-04 - val_loss: 7.2676e-04 - 15s/epoch - 162us/sample
Epoch 103/150
95501/95501 - 15s - loss: 8.1572e-04 - val_loss: 7.3328e-04 - 15s/epoch - 161us/sample
Epoch 104/150
95501/95501 - 15s - loss: 8.1984e-04 - val_loss: 8.1225e-04 - 15s/epoch - 162us/sample
Epoch 105/150
95501/95501 - 15s - loss: 8.2996e-04 - val_loss: 7.5448e-04 - 15s/epoch - 161us/sample
Epoch 106/150
95501/95501 - 16s - loss: 8.1972e-04 - val_loss: 7.3425e-04 - 16s/epoch - 162us/sample
Epoch 107/150
95501/95501 - 15s - loss: 8.2824e-04 - val_loss: 7.3506e-04 - 15s/epoch - 162us/sample
Epoch 108/150
95501/95501 - 15s - loss: 8.1456e-04 - val_loss: 7.4484e-04 - 15s/epoch - 162us/sample
Epoch 109/150
95501/95501 - 15s - loss: 8.2927e-04 - val_loss: 7.2328e-04 - 15s/epoch - 162us/sample
Epoch 110/150
95501/95501 - 15s - loss: 8.2296e-04 - val_loss: 7.3004e-04 - 15s/epoch - 162us/sample
Epoch 111/150
95501/95501 - 15s - loss: 8.1239e-04 - val_loss: 7.2805e-04 - 15s/epoch - 162us/sample
Epoch 112/150
95501/95501 - 15s - loss: 8.1709e-04 - val_loss: 7.2514e-04 - 15s/epoch - 162us/sample
Epoch 113/150
95501/95501 - 15s - loss: 8.1329e-04 - val_loss: 7.3861e-04 - 15s/epoch - 162us/sample
Epoch 114/150
95501/95501 - 15s - loss: 8.1171e-04 - val_loss: 7.2393e-04 - 15s/epoch - 162us/sample
Epoch 115/150
95501/95501 - 15s - loss: 8.0657e-04 - val_loss: 7.6447e-04 - 15s/epoch - 162us/sample
Epoch 116/150
95501/95501 - 16s - loss: 8.2630e-04 - val_loss: 7.9010e-04 - 16s/epoch - 162us/sample
Epoch 117/150
95501/95501 - 15s - loss: 8.2554e-04 - val_loss: 7.2317e-04 - 15s/epoch - 162us/sample
Epoch 118/150
95501/95501 - 15s - loss: 8.1331e-04 - val_loss: 7.1812e-04 - 15s/epoch - 162us/sample
Epoch 119/150
95501/95501 - 15s - loss: 8.0692e-04 - val_loss: 7.3836e-04 - 15s/epoch - 162us/sample
Epoch 120/150
95501/95501 - 15s - loss: 8.2095e-04 - val_loss: 7.3673e-04 - 15s/epoch - 162us/sample
Epoch 121/150
95501/95501 - 15s - loss: 8.1269e-04 - val_loss: 8.6709e-04 - 15s/epoch - 162us/sample
Epoch 122/150
95501/95501 - 16s - loss: 8.2944e-04 - val_loss: 7.2752e-04 - 16s/epoch - 163us/sample
Epoch 123/150
95501/95501 - 15s - loss: 8.1820e-04 - val_loss: 7.2503e-04 - 15s/epoch - 162us/sample
Epoch 124/150
95501/95501 - 15s - loss: 8.0534e-04 - val_loss: 7.2324e-04 - 15s/epoch - 162us/sample
Epoch 125/150
95501/95501 - 15s - loss: 8.1409e-04 - val_loss: 7.2079e-04 - 15s/epoch - 162us/sample
Epoch 126/150
95501/95501 - 15s - loss: 8.2591e-04 - val_loss: 7.1765e-04 - 15s/epoch - 162us/sample
Epoch 127/150
95501/95501 - 16s - loss: 8.3757e-04 - val_loss: 7.1223e-04 - 16s/epoch - 162us/sample
Epoch 128/150
95501/95501 - 15s - loss: 8.0453e-04 - val_loss: 7.2193e-04 - 15s/epoch - 162us/sample
Epoch 129/150
95501/95501 - 15s - loss: 8.0514e-04 - val_loss: 7.1731e-04 - 15s/epoch - 162us/sample
Epoch 130/150
95501/95501 - 15s - loss: 8.3141e-04 - val_loss: 7.7225e-04 - 15s/epoch - 162us/sample
Epoch 131/150
95501/95501 - 15s - loss: 8.3446e-04 - val_loss: 7.2478e-04 - 15s/epoch - 162us/sample
Epoch 132/150
95501/95501 - 16s - loss: 8.1210e-04 - val_loss: 7.1581e-04 - 16s/epoch - 163us/sample
Epoch 133/150
95501/95501 - 15s - loss: 8.2790e-04 - val_loss: 7.1547e-04 - 15s/epoch - 162us/sample
Epoch 134/150
95501/95501 - 15s - loss: 8.0237e-04 - val_loss: 7.1923e-04 - 15s/epoch - 162us/sample
Epoch 135/150
95501/95501 - 15s - loss: 8.0750e-04 - val_loss: 7.1725e-04 - 15s/epoch - 162us/sample
Epoch 136/150
95501/95501 - 16s - loss: 8.0008e-04 - val_loss: 7.1254e-04 - 16s/epoch - 163us/sample
Epoch 137/150
95501/95501 - 15s - loss: 7.9952e-04 - val_loss: 7.0750e-04 - 15s/epoch - 162us/sample
Epoch 138/150
95501/95501 - 15s - loss: 7.9777e-04 - val_loss: 7.5127e-04 - 15s/epoch - 162us/sample
Epoch 139/150
95501/95501 - 15s - loss: 8.0045e-04 - val_loss: 7.2461e-04 - 15s/epoch - 162us/sample
Epoch 140/150
95501/95501 - 15s - loss: 7.9535e-04 - val_loss: 7.1551e-04 - 15s/epoch - 162us/sample
Epoch 141/150
95501/95501 - 16s - loss: 8.0744e-04 - val_loss: 7.1082e-04 - 16s/epoch - 163us/sample
Epoch 142/150
95501/95501 - 15s - loss: 8.1646e-04 - val_loss: 7.1454e-04 - 15s/epoch - 162us/sample
Epoch 143/150
95501/95501 - 15s - loss: 7.9188e-04 - val_loss: 7.1207e-04 - 15s/epoch - 162us/sample
Epoch 144/150
95501/95501 - 15s - loss: 7.9588e-04 - val_loss: 7.1563e-04 - 15s/epoch - 162us/sample
Epoch 145/150
95501/95501 - 16s - loss: 8.1888e-04 - val_loss: 7.8339e-04 - 16s/epoch - 162us/sample
Epoch 146/150
95501/95501 - 15s - loss: 8.4219e-04 - val_loss: 7.1223e-04 - 15s/epoch - 162us/sample
Epoch 147/150
95501/95501 - 15s - loss: 8.0108e-04 - val_loss: 7.3161e-04 - 15s/epoch - 162us/sample
Epoch 148/150
95501/95501 - 15s - loss: 7.9623e-04 - val_loss: 7.1660e-04 - 15s/epoch - 162us/sample
Epoch 149/150
95501/95501 - 15s - loss: 8.0030e-04 - val_loss: 7.1746e-04 - 15s/epoch - 162us/sample
Epoch 150/150
95501/95501 - 16s - loss: 7.9088e-04 - val_loss: 7.1202e-04 - 16s/epoch - 163us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0007120228055783832
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 16:46:08.549182: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_1/outputlayer/BiasAdd' id:1690 op device:{requested: '', assigned: ''} def:{{{node decoder_model_1/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_1/outputlayer/MatMul, decoder_model_1/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.005133055069066424
cosine 0.0040518265657051074
MAE: 0.011025691
RMSE: 0.021075284
r2: 0.9711862170717152
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'mse', 32, 150, 0.001, 0.6, 758, 0.0007908834768747664, 0.0007120228055783832, 0.005133055069066424, 0.0040518265657051074, 0.011025691404938698, 0.021075284108519554, 0.9711862170717152, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 70 0.0005 16 0] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_6 (BatchNo  (None, 1896)        7584        ['dense_enc0[0][0]']             
 rmalization)                                                                                     
                                                                                                  
 re_lu_6 (ReLU)                 (None, 1896)         0           ['batch_normalization_6[0][0]']  
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu_6[0][0]']                
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu_6[0][0]']                
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/70
2023-02-14 16:46:14.824574: W tensorflow/c/c_api.cc:291] Operation '{name:'training_4/Adam/batch_normalization_7/beta/v/Assign' id:3698 op device:{requested: '', assigned: ''} def:{{{node training_4/Adam/batch_normalization_7/beta/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_4/Adam/batch_normalization_7/beta/v, training_4/Adam/batch_normalization_7/beta/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 16:46:43.763804: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_2/mul' id:2999 op device:{requested: '', assigned: ''} def:{{{node loss_2/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_2/mul/x, loss_2/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 31s - loss: 1.4318 - val_loss: 1.4140 - 31s/epoch - 319us/sample
Epoch 2/70
95501/95501 - 29s - loss: 1.4458 - val_loss: 1.4336 - 29s/epoch - 300us/sample
Epoch 3/70
95501/95501 - 29s - loss: 1.4269 - val_loss: 1.4260 - 29s/epoch - 298us/sample
Epoch 4/70
95501/95501 - 29s - loss: 1.4188 - val_loss: 1.4134 - 29s/epoch - 299us/sample
Epoch 5/70
95501/95501 - 29s - loss: 1.4123 - val_loss: 1.4100 - 29s/epoch - 299us/sample
Epoch 6/70
95501/95501 - 28s - loss: 1.4090 - val_loss: 1.4071 - 28s/epoch - 298us/sample
Epoch 7/70
95501/95501 - 28s - loss: 1.4071 - val_loss: 1.4117 - 28s/epoch - 298us/sample
Epoch 8/70
95501/95501 - 29s - loss: 1.4072 - val_loss: 1.4076 - 29s/epoch - 300us/sample
Epoch 9/70
95501/95501 - 29s - loss: 1.4068 - val_loss: 1.4101 - 29s/epoch - 299us/sample
Epoch 10/70
95501/95501 - 29s - loss: 1.4072 - val_loss: 1.4068 - 29s/epoch - 299us/sample
Epoch 11/70
95501/95501 - 29s - loss: 1.4075 - val_loss: 1.4092 - 29s/epoch - 300us/sample
Epoch 12/70
95501/95501 - 29s - loss: 1.4064 - val_loss: 1.4057 - 29s/epoch - 299us/sample
Epoch 13/70
95501/95501 - 29s - loss: 1.4034 - val_loss: 1.4038 - 29s/epoch - 299us/sample
Epoch 14/70
95501/95501 - 29s - loss: 1.4022 - val_loss: 1.4035 - 29s/epoch - 300us/sample
Epoch 15/70
95501/95501 - 29s - loss: 1.4018 - val_loss: 1.4035 - 29s/epoch - 299us/sample
Epoch 16/70
95501/95501 - 29s - loss: 1.4021 - val_loss: 1.4037 - 29s/epoch - 299us/sample
Epoch 17/70
95501/95501 - 29s - loss: 1.4020 - val_loss: 1.4034 - 29s/epoch - 299us/sample
Epoch 18/70
95501/95501 - 29s - loss: 1.4015 - val_loss: 1.4030 - 29s/epoch - 299us/sample
Epoch 19/70
95501/95501 - 29s - loss: 1.4015 - val_loss: 1.4019 - 29s/epoch - 299us/sample
Epoch 20/70
95501/95501 - 29s - loss: 1.4020 - val_loss: 1.4023 - 29s/epoch - 300us/sample
Epoch 21/70
95501/95501 - 27s - loss: 1.4016 - val_loss: 1.4035 - 27s/epoch - 286us/sample
Epoch 22/70
95501/95501 - 27s - loss: 1.4009 - val_loss: 1.4016 - 27s/epoch - 283us/sample
Epoch 23/70
95501/95501 - 27s - loss: 1.4010 - val_loss: 1.4020 - 27s/epoch - 284us/sample
Epoch 24/70
95501/95501 - 27s - loss: 1.4011 - val_loss: 1.4014 - 27s/epoch - 283us/sample
Epoch 25/70
95501/95501 - 27s - loss: 1.4001 - val_loss: 1.4014 - 27s/epoch - 282us/sample
Epoch 26/70
95501/95501 - 27s - loss: 1.4001 - val_loss: 1.4022 - 27s/epoch - 284us/sample
Epoch 27/70
95501/95501 - 27s - loss: 1.4010 - val_loss: 1.4017 - 27s/epoch - 282us/sample
Epoch 28/70
95501/95501 - 27s - loss: 1.4002 - val_loss: 1.4024 - 27s/epoch - 283us/sample
Epoch 29/70
95501/95501 - 27s - loss: 1.4020 - val_loss: 1.4023 - 27s/epoch - 283us/sample
Epoch 30/70
95501/95501 - 27s - loss: 1.4008 - val_loss: 1.4018 - 27s/epoch - 283us/sample
Epoch 31/70
95501/95501 - 27s - loss: 1.4017 - val_loss: 1.4027 - 27s/epoch - 282us/sample
Epoch 32/70
95501/95501 - 27s - loss: 1.4013 - val_loss: 1.4022 - 27s/epoch - 284us/sample
Epoch 33/70
95501/95501 - 27s - loss: 1.3999 - val_loss: 1.4012 - 27s/epoch - 282us/sample
Epoch 34/70
95501/95501 - 27s - loss: 1.3999 - val_loss: 1.4012 - 27s/epoch - 283us/sample
Epoch 35/70
95501/95501 - 27s - loss: 1.3995 - val_loss: 1.4006 - 27s/epoch - 283us/sample
Epoch 36/70
95501/95501 - 27s - loss: 1.3996 - val_loss: 1.4008 - 27s/epoch - 283us/sample
Epoch 37/70
95501/95501 - 27s - loss: 1.3999 - val_loss: 1.4012 - 27s/epoch - 282us/sample
Epoch 38/70
95501/95501 - 27s - loss: 1.4005 - val_loss: 1.4014 - 27s/epoch - 283us/sample
Epoch 39/70
95501/95501 - 27s - loss: 1.4007 - val_loss: 1.4010 - 27s/epoch - 282us/sample
Epoch 40/70
95501/95501 - 27s - loss: 1.3996 - val_loss: 1.4007 - 27s/epoch - 282us/sample
Epoch 41/70
95501/95501 - 27s - loss: 1.4007 - val_loss: 1.4009 - 27s/epoch - 283us/sample
Epoch 42/70
95501/95501 - 27s - loss: 1.3992 - val_loss: 1.4007 - 27s/epoch - 283us/sample
Epoch 43/70
95501/95501 - 27s - loss: 1.3997 - val_loss: 1.4011 - 27s/epoch - 282us/sample
Epoch 44/70
95501/95501 - 27s - loss: 1.3999 - val_loss: 1.4020 - 27s/epoch - 283us/sample
Epoch 45/70
95501/95501 - 27s - loss: 1.3998 - val_loss: 1.4009 - 27s/epoch - 283us/sample
Epoch 46/70
95501/95501 - 27s - loss: 1.3995 - val_loss: 1.4006 - 27s/epoch - 283us/sample
Epoch 47/70
95501/95501 - 27s - loss: 1.4000 - val_loss: 1.4013 - 27s/epoch - 283us/sample
Epoch 48/70
95501/95501 - 27s - loss: 1.3994 - val_loss: 1.4006 - 27s/epoch - 283us/sample
Epoch 49/70
95501/95501 - 27s - loss: 1.3992 - val_loss: 1.4001 - 27s/epoch - 283us/sample
Epoch 50/70
95501/95501 - 27s - loss: 1.3989 - val_loss: 1.4000 - 27s/epoch - 283us/sample
Epoch 51/70
95501/95501 - 27s - loss: 1.3991 - val_loss: 1.4005 - 27s/epoch - 283us/sample
Epoch 52/70
95501/95501 - 27s - loss: 1.3995 - val_loss: 1.4017 - 27s/epoch - 282us/sample
Epoch 53/70
95501/95501 - 27s - loss: 1.3996 - val_loss: 1.4005 - 27s/epoch - 283us/sample
Epoch 54/70
95501/95501 - 27s - loss: 1.3986 - val_loss: 1.3999 - 27s/epoch - 283us/sample
Epoch 55/70
95501/95501 - 27s - loss: 1.3988 - val_loss: 1.4001 - 27s/epoch - 283us/sample
Epoch 56/70
95501/95501 - 27s - loss: 1.3991 - val_loss: 1.4005 - 27s/epoch - 283us/sample
Epoch 57/70
95501/95501 - 27s - loss: 1.3991 - val_loss: 1.3997 - 27s/epoch - 283us/sample
Epoch 58/70
95501/95501 - 27s - loss: 1.3987 - val_loss: 1.3999 - 27s/epoch - 283us/sample
Epoch 59/70
95501/95501 - 27s - loss: 1.3984 - val_loss: 1.3998 - 27s/epoch - 282us/sample
Epoch 60/70
95501/95501 - 27s - loss: 1.3985 - val_loss: 1.3999 - 27s/epoch - 284us/sample
Epoch 61/70
95501/95501 - 27s - loss: 1.3986 - val_loss: 1.3999 - 27s/epoch - 282us/sample
Epoch 62/70
95501/95501 - 27s - loss: 1.3986 - val_loss: 1.3997 - 27s/epoch - 284us/sample
Epoch 63/70
95501/95501 - 27s - loss: 1.3985 - val_loss: 1.4001 - 27s/epoch - 283us/sample
Epoch 64/70
95501/95501 - 27s - loss: 1.3988 - val_loss: 1.4003 - 27s/epoch - 283us/sample
Epoch 65/70
95501/95501 - 27s - loss: 1.3987 - val_loss: 1.4001 - 27s/epoch - 283us/sample
Epoch 66/70
95501/95501 - 27s - loss: 1.3991 - val_loss: 1.4012 - 27s/epoch - 283us/sample
Epoch 67/70
95501/95501 - 27s - loss: 1.3992 - val_loss: 1.4003 - 27s/epoch - 282us/sample
Epoch 68/70
95501/95501 - 27s - loss: 1.3988 - val_loss: 1.4000 - 27s/epoch - 283us/sample
Epoch 69/70
95501/95501 - 27s - loss: 1.3991 - val_loss: 1.4004 - 27s/epoch - 283us/sample
Epoch 70/70
95501/95501 - 27s - loss: 1.3987 - val_loss: 1.3999 - 27s/epoch - 282us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 1.3999216399730219
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 17:18:18.915018: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_2/outputlayer/BiasAdd' id:2951 op device:{requested: '', assigned: ''} def:{{{node decoder_model_2/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_2/outputlayer/MatMul, decoder_model_2/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.8166474859954075
cosine 0.9521117266687973
MAE: 8.029042
RMSE: 22.647036
r2: -33269.47981515252
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'binary_crossentropy', 16, 70, 0.0005, 0.6, 758, 1.3987101714469576, 1.3999216399730219, 0.8166474859954075, 0.9521117266687973, 8.02904224395752, 22.647035598754883, -33269.47981515252, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 50 0.0005 64 2] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_9 (BatchNo  (None, 3160)        12640       ['dense_enc0[0][0]']             
 rmalization)                                                                                     
                                                                                                  
 re_lu_9 (ReLU)                 (None, 3160)         0           ['batch_normalization_9[0][0]']  
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2396038     ['re_lu_9[0][0]']                
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2396038     ['re_lu_9[0][0]']                
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         6984938     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 15,787,054
Trainable params: 15,772,898
Non-trainable params: 14,156
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/50
2023-02-14 17:18:25.280063: W tensorflow/c/c_api.cc:291] Operation '{name:'training_6/Adam/batch_normalization_9/gamma/m/Assign' id:4823 op device:{requested: '', assigned: ''} def:{{{node training_6/Adam/batch_normalization_9/gamma/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_6/Adam/batch_normalization_9/gamma/m, training_6/Adam/batch_normalization_9/gamma/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 17:18:35.908700: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_3/mul' id:4330 op device:{requested: '', assigned: ''} def:{{{node loss_3/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_3/mul/x, loss_3/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 12s - loss: 0.0065 - val_loss: 0.1492 - 12s/epoch - 123us/sample
Epoch 2/50
95501/95501 - 10s - loss: 0.0033 - val_loss: 0.0032 - 10s/epoch - 104us/sample
Epoch 3/50
95501/95501 - 10s - loss: 0.0026 - val_loss: 0.0019 - 10s/epoch - 104us/sample
Epoch 4/50
95501/95501 - 10s - loss: 0.0020 - val_loss: 0.0031 - 10s/epoch - 103us/sample
Epoch 5/50
95501/95501 - 10s - loss: 0.7358 - val_loss: 0.0018 - 10s/epoch - 103us/sample
Epoch 6/50
95501/95501 - 10s - loss: 0.0015 - val_loss: 0.0011 - 10s/epoch - 104us/sample
Epoch 7/50
95501/95501 - 10s - loss: 0.0013 - val_loss: 0.0017 - 10s/epoch - 103us/sample
Epoch 8/50
95501/95501 - 10s - loss: 0.0014 - val_loss: 0.0010 - 10s/epoch - 103us/sample
Epoch 9/50
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.9799e-04 - 10s/epoch - 103us/sample
Epoch 10/50
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.7508e-04 - 10s/epoch - 103us/sample
Epoch 11/50
95501/95501 - 10s - loss: 9.0451e-04 - val_loss: 8.3179e-04 - 10s/epoch - 103us/sample
Epoch 12/50
95501/95501 - 10s - loss: 8.6094e-04 - val_loss: 8.2419e-04 - 10s/epoch - 104us/sample
Epoch 13/50
95501/95501 - 10s - loss: 8.1794e-04 - val_loss: 8.0041e-04 - 10s/epoch - 103us/sample
Epoch 14/50
95501/95501 - 10s - loss: 7.8675e-04 - val_loss: 7.5713e-04 - 10s/epoch - 103us/sample
Epoch 15/50
95501/95501 - 10s - loss: 8.9050e-04 - val_loss: 7.0836e-04 - 10s/epoch - 103us/sample
Epoch 16/50
95501/95501 - 10s - loss: 7.4064e-04 - val_loss: 7.0190e-04 - 10s/epoch - 103us/sample
Epoch 17/50
95501/95501 - 10s - loss: 7.1739e-04 - val_loss: 7.1398e-04 - 10s/epoch - 103us/sample
Epoch 18/50
95501/95501 - 10s - loss: 6.9056e-04 - val_loss: 6.5809e-04 - 10s/epoch - 104us/sample
Epoch 19/50
95501/95501 - 10s - loss: 6.8201e-04 - val_loss: 0.0020 - 10s/epoch - 103us/sample
Epoch 20/50
95501/95501 - 10s - loss: 7.8773e-04 - val_loss: 6.1685e-04 - 10s/epoch - 104us/sample
Epoch 21/50
95501/95501 - 10s - loss: 6.4915e-04 - val_loss: 6.0195e-04 - 10s/epoch - 104us/sample
Epoch 22/50
95501/95501 - 10s - loss: 6.3968e-04 - val_loss: 6.6174e-04 - 10s/epoch - 103us/sample
Epoch 23/50
95501/95501 - 10s - loss: 6.1987e-04 - val_loss: 5.7004e-04 - 10s/epoch - 103us/sample
Epoch 24/50
95501/95501 - 10s - loss: 6.0802e-04 - val_loss: 7.8860e-04 - 10s/epoch - 103us/sample
Epoch 25/50
95501/95501 - 10s - loss: 6.6691e-04 - val_loss: 5.5861e-04 - 10s/epoch - 103us/sample
Epoch 26/50
95501/95501 - 10s - loss: 5.9410e-04 - val_loss: 5.8477e-04 - 10s/epoch - 103us/sample
Epoch 27/50
95501/95501 - 10s - loss: 5.8028e-04 - val_loss: 7.2324e-04 - 10s/epoch - 104us/sample
Epoch 28/50
95501/95501 - 10s - loss: 6.9410e-04 - val_loss: 5.4723e-04 - 10s/epoch - 104us/sample
Epoch 29/50
95501/95501 - 10s - loss: 5.7306e-04 - val_loss: 5.4368e-04 - 10s/epoch - 103us/sample
Epoch 30/50
95501/95501 - 10s - loss: 5.6200e-04 - val_loss: 5.4100e-04 - 10s/epoch - 103us/sample
Epoch 31/50
95501/95501 - 10s - loss: 5.5704e-04 - val_loss: 5.2797e-04 - 10s/epoch - 103us/sample
Epoch 32/50
95501/95501 - 10s - loss: 5.4746e-04 - val_loss: 5.2608e-04 - 10s/epoch - 103us/sample
Epoch 33/50
95501/95501 - 10s - loss: 5.5359e-04 - val_loss: 5.4739e-04 - 10s/epoch - 103us/sample
Epoch 34/50
95501/95501 - 10s - loss: 5.4388e-04 - val_loss: 5.1272e-04 - 10s/epoch - 105us/sample
Epoch 35/50
95501/95501 - 10s - loss: 5.3437e-04 - val_loss: 5.0646e-04 - 10s/epoch - 103us/sample
Epoch 36/50
95501/95501 - 10s - loss: 5.3201e-04 - val_loss: 5.0919e-04 - 10s/epoch - 103us/sample
Epoch 37/50
95501/95501 - 10s - loss: 5.2655e-04 - val_loss: 4.9931e-04 - 10s/epoch - 104us/sample
Epoch 38/50
95501/95501 - 10s - loss: 5.5140e-04 - val_loss: 5.6357e-04 - 10s/epoch - 103us/sample
Epoch 39/50
95501/95501 - 10s - loss: 5.2899e-04 - val_loss: 4.9889e-04 - 10s/epoch - 103us/sample
Epoch 40/50
95501/95501 - 10s - loss: 5.1626e-04 - val_loss: 4.9915e-04 - 10s/epoch - 104us/sample
Epoch 41/50
95501/95501 - 10s - loss: 5.3434e-04 - val_loss: 5.0033e-04 - 10s/epoch - 104us/sample
Epoch 42/50
95501/95501 - 10s - loss: 5.1288e-04 - val_loss: 4.8703e-04 - 10s/epoch - 103us/sample
Epoch 43/50
95501/95501 - 10s - loss: 5.1454e-04 - val_loss: 4.9389e-04 - 10s/epoch - 104us/sample
Epoch 44/50
95501/95501 - 10s - loss: 5.0603e-04 - val_loss: 4.7909e-04 - 10s/epoch - 103us/sample
Epoch 45/50
95501/95501 - 10s - loss: 5.2975e-04 - val_loss: 5.0334e-04 - 10s/epoch - 103us/sample
Epoch 46/50
95501/95501 - 10s - loss: 5.1178e-04 - val_loss: 4.8286e-04 - 10s/epoch - 104us/sample
Epoch 47/50
95501/95501 - 10s - loss: 5.0052e-04 - val_loss: 4.8046e-04 - 10s/epoch - 103us/sample
Epoch 48/50
95501/95501 - 10s - loss: 4.9708e-04 - val_loss: 4.8159e-04 - 10s/epoch - 103us/sample
Epoch 49/50
95501/95501 - 10s - loss: 5.0008e-04 - val_loss: 4.7929e-04 - 10s/epoch - 104us/sample
Epoch 50/50
95501/95501 - 10s - loss: 4.9308e-04 - val_loss: 5.1130e-04 - 10s/epoch - 103us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0005112977766591945
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 17:26:41.730274: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_3/outputlayer/BiasAdd' id:4294 op device:{requested: '', assigned: ''} def:{{{node decoder_model_3/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_3/outputlayer/MatMul, decoder_model_3/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.007475162171391869
cosine 0.005900564561733155
MAE: 0.01429452
RMSE: 0.02551938
r2: 0.9577530441509847
RMSE zero-vector: 0.23411466903540806
['2.5custom_VAE', 'logcosh', 64, 50, 0.0005, 0.6, 758, 0.0004930763534928182, 0.0005112977766591945, 0.007475162171391869, 0.005900564561733155, 0.014294520020484924, 0.02551938034594059, 0.9577530441509847, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 110 0.0005 8 0] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_12 (BatchN  (None, 2528)        10112       ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_12 (ReLU)                (None, 2528)         0           ['batch_normalization_12[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1916982     ['re_lu_12[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1916982     ['re_lu_12[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5703874     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 12,745,870
Trainable params: 12,734,242
Non-trainable params: 11,628
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/110
2023-02-14 17:26:48.938914: W tensorflow/c/c_api.cc:291] Operation '{name:'training_8/Adam/batch_normalization_14/beta/v/Assign' id:6366 op device:{requested: '', assigned: ''} def:{{{node training_8/Adam/batch_normalization_14/beta/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_8/Adam/batch_normalization_14/beta/v, training_8/Adam/batch_normalization_14/beta/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 17:27:43.083596: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_4/mul' id:5636 op device:{requested: '', assigned: ''} def:{{{node loss_4/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_4/mul/x, loss_4/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 57s - loss: 1.5616 - val_loss: 20.9875 - 57s/epoch - 596us/sample
Epoch 2/110
95501/95501 - 54s - loss: 1.4544 - val_loss: 1.4632 - 54s/epoch - 570us/sample
Epoch 3/110
95501/95501 - 56s - loss: 1.4387 - val_loss: 3.9519 - 56s/epoch - 584us/sample
Epoch 4/110
95501/95501 - 60s - loss: 1.4401 - val_loss: 1.4302 - 60s/epoch - 627us/sample
Epoch 5/110
95501/95501 - 60s - loss: 1.4326 - val_loss: 261.3461 - 60s/epoch - 627us/sample
Epoch 6/110
95501/95501 - 60s - loss: 1.4280 - val_loss: 1.4311 - 60s/epoch - 627us/sample
Epoch 7/110
95501/95501 - 60s - loss: 1.4250 - val_loss: 1.4292 - 60s/epoch - 627us/sample
Epoch 8/110
95501/95501 - 60s - loss: 1.4285 - val_loss: 1.4436 - 60s/epoch - 626us/sample
Epoch 9/110
95501/95501 - 60s - loss: 1.4305 - val_loss: 1.4327 - 60s/epoch - 626us/sample
Epoch 10/110
95501/95501 - 60s - loss: 1.4309 - val_loss: 1.4307 - 60s/epoch - 627us/sample
Epoch 11/110
95501/95501 - 59s - loss: 1.4334 - val_loss: 1.4352 - 59s/epoch - 622us/sample
Epoch 12/110
95501/95501 - 53s - loss: 1.4305 - val_loss: 1.7560 - 53s/epoch - 554us/sample
Epoch 13/110
95501/95501 - 53s - loss: 1.4294 - val_loss: 1.4324 - 53s/epoch - 553us/sample
Epoch 14/110
95501/95501 - 53s - loss: 1.4306 - val_loss: 1.4299 - 53s/epoch - 554us/sample
Epoch 15/110
95501/95501 - 53s - loss: 1.4301 - val_loss: 1.4307 - 53s/epoch - 555us/sample
Epoch 16/110
95501/95501 - 53s - loss: 1.4308 - val_loss: 1.4405 - 53s/epoch - 554us/sample
Epoch 17/110
95501/95501 - 53s - loss: 1.4320 - val_loss: 1.4315 - 53s/epoch - 554us/sample
Epoch 18/110
95501/95501 - 53s - loss: 1.4295 - val_loss: 1.4299 - 53s/epoch - 554us/sample
Epoch 19/110
95501/95501 - 53s - loss: 1.4303 - val_loss: 1.4333 - 53s/epoch - 555us/sample
Epoch 20/110
95501/95501 - 53s - loss: 1.4309 - val_loss: 1.4342 - 53s/epoch - 554us/sample
Epoch 21/110
95501/95501 - 53s - loss: 1.4295 - val_loss: 1.4308 - 53s/epoch - 554us/sample
Epoch 22/110
95501/95501 - 53s - loss: 1.4291 - val_loss: 1.4304 - 53s/epoch - 554us/sample
Epoch 23/110
95501/95501 - 53s - loss: 1.4289 - val_loss: 1.4301 - 53s/epoch - 553us/sample
Epoch 24/110
95501/95501 - 53s - loss: 1.4298 - val_loss: 1.4300 - 53s/epoch - 555us/sample
Epoch 25/110
95501/95501 - 53s - loss: 1.4299 - val_loss: 1.4348 - 53s/epoch - 555us/sample
Epoch 26/110
95501/95501 - 53s - loss: 1.4301 - val_loss: 1.4305 - 53s/epoch - 553us/sample
Epoch 27/110
95501/95501 - 53s - loss: 1.4290 - val_loss: 1.4295 - 53s/epoch - 554us/sample
Epoch 28/110
95501/95501 - 53s - loss: 1.4307 - val_loss: 1.4311 - 53s/epoch - 554us/sample
Epoch 29/110
95501/95501 - 53s - loss: 1.4308 - val_loss: 1.4312 - 53s/epoch - 555us/sample
Epoch 30/110
95501/95501 - 53s - loss: 1.4314 - val_loss: 1.4306 - 53s/epoch - 553us/sample
Epoch 31/110
95501/95501 - 53s - loss: 1.4295 - val_loss: 1.4303 - 53s/epoch - 554us/sample
Epoch 32/110
95501/95501 - 53s - loss: 1.4290 - val_loss: 1.4287 - 53s/epoch - 554us/sample
Epoch 33/110
95501/95501 - 53s - loss: 1.4301 - val_loss: 1.4322 - 53s/epoch - 554us/sample
Epoch 34/110
95501/95501 - 53s - loss: 1.4281 - val_loss: 1.4285 - 53s/epoch - 552us/sample
Epoch 35/110
95501/95501 - 53s - loss: 1.4288 - val_loss: 1.4281 - 53s/epoch - 554us/sample
Epoch 36/110
95501/95501 - 53s - loss: 1.4292 - val_loss: 1.4326 - 53s/epoch - 554us/sample
Epoch 37/110
95501/95501 - 53s - loss: 1.4300 - val_loss: 1.4289 - 53s/epoch - 554us/sample
Epoch 38/110
95501/95501 - 53s - loss: 1.4283 - val_loss: 1.4348 - 53s/epoch - 553us/sample
Epoch 39/110
95501/95501 - 53s - loss: 1.4280 - val_loss: 1.4284 - 53s/epoch - 553us/sample
Epoch 40/110
95501/95501 - 53s - loss: 1.4274 - val_loss: 1.4274 - 53s/epoch - 555us/sample
Epoch 41/110
95501/95501 - 53s - loss: 1.4276 - val_loss: 1.4285 - 53s/epoch - 554us/sample
Epoch 42/110
95501/95501 - 53s - loss: 1.4285 - val_loss: 1.4327 - 53s/epoch - 553us/sample
Epoch 43/110
95501/95501 - 53s - loss: 1.4300 - val_loss: 1.4371 - 53s/epoch - 554us/sample
Epoch 44/110
95501/95501 - 53s - loss: 1.4316 - val_loss: 1.4291 - 53s/epoch - 555us/sample
Epoch 45/110
95501/95501 - 53s - loss: 1.4280 - val_loss: 1.4289 - 53s/epoch - 555us/sample
Epoch 46/110
95501/95501 - 53s - loss: 1.4289 - val_loss: 1.4285 - 53s/epoch - 552us/sample
Epoch 47/110
95501/95501 - 53s - loss: 1.4275 - val_loss: 1.4315 - 53s/epoch - 554us/sample
Epoch 48/110
95501/95501 - 53s - loss: 1.4286 - val_loss: 1.4280 - 53s/epoch - 554us/sample
Epoch 49/110
95501/95501 - 53s - loss: 1.4283 - val_loss: 1.4302 - 53s/epoch - 554us/sample
Epoch 50/110
95501/95501 - 53s - loss: 1.4285 - val_loss: 1.4292 - 53s/epoch - 554us/sample
Epoch 51/110
95501/95501 - 53s - loss: 1.4276 - val_loss: 1.4291 - 53s/epoch - 552us/sample
Epoch 52/110
95501/95501 - 53s - loss: 1.4268 - val_loss: 1.4267 - 53s/epoch - 555us/sample
Epoch 53/110
95501/95501 - 53s - loss: 1.4274 - val_loss: 1.4275 - 53s/epoch - 554us/sample
Epoch 54/110
95501/95501 - 53s - loss: 1.4264 - val_loss: 1.4322 - 53s/epoch - 553us/sample
Epoch 55/110
95501/95501 - 53s - loss: 1.4264 - val_loss: 1.4282 - 53s/epoch - 554us/sample
Epoch 56/110
95501/95501 - 53s - loss: 1.4269 - val_loss: 1.4270 - 53s/epoch - 555us/sample
Epoch 57/110
95501/95501 - 53s - loss: 1.4282 - val_loss: 1.4301 - 53s/epoch - 554us/sample
Epoch 58/110
95501/95501 - 53s - loss: 1.4273 - val_loss: 1.4298 - 53s/epoch - 553us/sample
Epoch 59/110
95501/95501 - 53s - loss: 1.4269 - val_loss: 1.4278 - 53s/epoch - 554us/sample
Epoch 60/110
95501/95501 - 53s - loss: 1.4264 - val_loss: 1.4340 - 53s/epoch - 554us/sample
Epoch 61/110
95501/95501 - 53s - loss: 1.4267 - val_loss: 1.4285 - 53s/epoch - 555us/sample
Epoch 62/110
95501/95501 - 53s - loss: 1.4283 - val_loss: 1.4270 - 53s/epoch - 553us/sample
Epoch 63/110
95501/95501 - 53s - loss: 1.4274 - val_loss: 1.4287 - 53s/epoch - 554us/sample
Epoch 64/110
95501/95501 - 53s - loss: 1.4270 - val_loss: 1.4270 - 53s/epoch - 554us/sample
Epoch 65/110
95501/95501 - 53s - loss: 1.4278 - val_loss: 1.4296 - 53s/epoch - 553us/sample
Epoch 66/110
95501/95501 - 53s - loss: 1.4267 - val_loss: 1.4276 - 53s/epoch - 553us/sample
Epoch 67/110
95501/95501 - 53s - loss: 1.4265 - val_loss: 1.4278 - 53s/epoch - 554us/sample
Epoch 68/110
95501/95501 - 53s - loss: 1.4265 - val_loss: 1.4310 - 53s/epoch - 554us/sample
Epoch 69/110
95501/95501 - 53s - loss: 1.4262 - val_loss: 1.4265 - 53s/epoch - 554us/sample
Epoch 70/110
95501/95501 - 53s - loss: 1.4263 - val_loss: 1.4294 - 53s/epoch - 554us/sample
Epoch 71/110
95501/95501 - 53s - loss: 1.4281 - val_loss: 1.4270 - 53s/epoch - 554us/sample
Epoch 72/110
95501/95501 - 53s - loss: 1.4256 - val_loss: 1.4266 - 53s/epoch - 555us/sample
Epoch 73/110
95501/95501 - 53s - loss: 1.4276 - val_loss: 1.4306 - 53s/epoch - 554us/sample
Epoch 74/110
95501/95501 - 53s - loss: 1.4260 - val_loss: 1.4265 - 53s/epoch - 552us/sample
Epoch 75/110
95501/95501 - 53s - loss: 1.4272 - val_loss: 1.4263 - 53s/epoch - 553us/sample
Epoch 76/110
95501/95501 - 53s - loss: 1.4251 - val_loss: 1.4263 - 53s/epoch - 554us/sample
Epoch 77/110
95501/95501 - 53s - loss: 1.4255 - val_loss: 1.4266 - 53s/epoch - 554us/sample
Epoch 78/110
95501/95501 - 53s - loss: 1.4257 - val_loss: 1.4272 - 53s/epoch - 553us/sample
Epoch 79/110
95501/95501 - 53s - loss: 1.4261 - val_loss: 1.4264 - 53s/epoch - 552us/sample
Epoch 80/110
95501/95501 - 53s - loss: 1.4251 - val_loss: 1.4263 - 53s/epoch - 554us/sample
Epoch 81/110
95501/95501 - 53s - loss: 1.4251 - val_loss: 1.4263 - 53s/epoch - 554us/sample
Epoch 82/110
95501/95501 - 53s - loss: 1.4251 - val_loss: 1.4265 - 53s/epoch - 554us/sample
Epoch 83/110
95501/95501 - 53s - loss: 1.4263 - val_loss: 1.4270 - 53s/epoch - 553us/sample
Epoch 84/110
95501/95501 - 53s - loss: 1.4280 - val_loss: 1.4348 - 53s/epoch - 554us/sample
Epoch 85/110
95501/95501 - 53s - loss: 1.4297 - val_loss: 1.4271 - 53s/epoch - 554us/sample
Epoch 86/110
95501/95501 - 53s - loss: 1.4255 - val_loss: 1.4271 - 53s/epoch - 553us/sample
Epoch 87/110
95501/95501 - 53s - loss: 1.4253 - val_loss: 1.4266 - 53s/epoch - 553us/sample
Epoch 88/110
95501/95501 - 53s - loss: 1.4256 - val_loss: 1.4276 - 53s/epoch - 554us/sample
Epoch 89/110
95501/95501 - 53s - loss: 1.4256 - val_loss: 1.4265 - 53s/epoch - 554us/sample
Epoch 90/110
95501/95501 - 53s - loss: 1.4255 - val_loss: 1.4264 - 53s/epoch - 553us/sample
Epoch 91/110
95501/95501 - 53s - loss: 1.4251 - val_loss: 1.4264 - 53s/epoch - 552us/sample
Epoch 92/110
95501/95501 - 53s - loss: 1.4251 - val_loss: 1.4262 - 53s/epoch - 553us/sample
Epoch 93/110
95501/95501 - 53s - loss: 1.4258 - val_loss: 1.4267 - 53s/epoch - 555us/sample
Epoch 94/110
95501/95501 - 53s - loss: 1.4251 - val_loss: 1.4273 - 53s/epoch - 554us/sample
Epoch 95/110
95501/95501 - 53s - loss: 1.4282 - val_loss: 1.4282 - 53s/epoch - 555us/sample
Epoch 96/110
95501/95501 - 53s - loss: 1.4259 - val_loss: 1.4263 - 53s/epoch - 554us/sample
Epoch 97/110
95501/95501 - 53s - loss: 1.4251 - val_loss: 1.4262 - 53s/epoch - 553us/sample
Epoch 98/110
95501/95501 - 53s - loss: 1.4253 - val_loss: 1.4262 - 53s/epoch - 552us/sample
Epoch 99/110
95501/95501 - 53s - loss: 1.4325 - val_loss: 1.4337 - 53s/epoch - 553us/sample
Epoch 100/110
95501/95501 - 53s - loss: 1.4287 - val_loss: 1.4283 - 53s/epoch - 553us/sample
Epoch 101/110
95501/95501 - 53s - loss: 1.4269 - val_loss: 1.4283 - 53s/epoch - 554us/sample
Epoch 102/110
95501/95501 - 53s - loss: 1.4272 - val_loss: 1.4281 - 53s/epoch - 554us/sample
Epoch 103/110
95501/95501 - 53s - loss: 1.4270 - val_loss: 1.4282 - 53s/epoch - 554us/sample
Epoch 104/110
95501/95501 - 53s - loss: 1.4279 - val_loss: 1.4294 - 53s/epoch - 553us/sample
Epoch 105/110
95501/95501 - 53s - loss: 1.4287 - val_loss: 1.4282 - 53s/epoch - 554us/sample
Epoch 106/110
95501/95501 - 53s - loss: 1.4271 - val_loss: 1.4283 - 53s/epoch - 554us/sample
Epoch 107/110
95501/95501 - 53s - loss: 1.4269 - val_loss: 1.4297 - 53s/epoch - 554us/sample
Epoch 108/110
95501/95501 - 53s - loss: 1.4269 - val_loss: 1.4281 - 53s/epoch - 553us/sample
Epoch 109/110
95501/95501 - 53s - loss: 1.4269 - val_loss: 1.4280 - 53s/epoch - 555us/sample
Epoch 110/110
95501/95501 - 53s - loss: 1.4326 - val_loss: 1.4329 - 53s/epoch - 553us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 1.4329110356577828
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 19:04:49.548645: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_4/outputlayer/BiasAdd' id:5588 op device:{requested: '', assigned: ''} def:{{{node decoder_model_4/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_4/outputlayer/MatMul, decoder_model_4/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.8668212982294867
cosine 0.9575346072697035
MAE: 12.706952
RMSE: 48.50547
r2: -152619.47954955854
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'binary_crossentropy', 8, 110, 0.0005, 0.6, 758, 1.4326046155852932, 1.4329110356577828, 0.8668212982294867, 0.9575346072697035, 12.706952095031738, 48.505470275878906, -152619.47954955854, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 170 0.0005 16 0] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_15 (BatchN  (None, 2528)        10112       ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_15 (ReLU)                (None, 2528)         0           ['batch_normalization_15[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1916982     ['re_lu_15[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1916982     ['re_lu_15[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5703874     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 12,745,870
Trainable params: 12,734,242
Non-trainable params: 11,628
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/170
2023-02-14 19:04:56.961246: W tensorflow/c/c_api.cc:291] Operation '{name:'bottleneck_zlog_5/bias/Assign' id:6644 op device:{requested: '', assigned: ''} def:{{{node bottleneck_zlog_5/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](bottleneck_zlog_5/bias, bottleneck_zlog_5/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 19:05:26.178723: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_5/mul' id:6982 op device:{requested: '', assigned: ''} def:{{{node loss_5/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_5/mul/x, loss_5/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 31s - loss: 1.4358 - val_loss: 1.4264 - 31s/epoch - 328us/sample
Epoch 2/170
95501/95501 - 29s - loss: 1.4315 - val_loss: 1.4283 - 29s/epoch - 302us/sample
Epoch 3/170
95501/95501 - 29s - loss: 1.4427 - val_loss: 1.4526 - 29s/epoch - 299us/sample
Epoch 4/170
95501/95501 - 29s - loss: 1.4385 - val_loss: 1.4524 - 29s/epoch - 302us/sample
Epoch 5/170
95501/95501 - 29s - loss: 1.4345 - val_loss: 1.4400 - 29s/epoch - 300us/sample
Epoch 6/170
95501/95501 - 29s - loss: 1.4385 - val_loss: 1.4359 - 29s/epoch - 302us/sample
Epoch 7/170
95501/95501 - 29s - loss: 1.4337 - val_loss: 1.4364 - 29s/epoch - 300us/sample
Epoch 8/170
95501/95501 - 29s - loss: 1.4345 - val_loss: 1.4405 - 29s/epoch - 301us/sample
Epoch 9/170
95501/95501 - 29s - loss: 1.4312 - val_loss: 1.4347 - 29s/epoch - 299us/sample
Epoch 10/170
95501/95501 - 29s - loss: 1.4321 - val_loss: 1.4314 - 29s/epoch - 302us/sample
Epoch 11/170
95501/95501 - 29s - loss: 1.4307 - val_loss: 1.4382 - 29s/epoch - 299us/sample
Epoch 12/170
95501/95501 - 29s - loss: 1.4287 - val_loss: 1.4295 - 29s/epoch - 302us/sample
Epoch 13/170
95501/95501 - 29s - loss: 1.4295 - val_loss: 1.4313 - 29s/epoch - 300us/sample
Epoch 14/170
95501/95501 - 29s - loss: 1.4280 - val_loss: 1.4338 - 29s/epoch - 302us/sample
Epoch 15/170
95501/95501 - 29s - loss: 1.4294 - val_loss: 1.4447 - 29s/epoch - 300us/sample
Epoch 16/170
95501/95501 - 30s - loss: 1.4310 - val_loss: 1.4315 - 30s/epoch - 315us/sample
Epoch 17/170
95501/95501 - 30s - loss: 1.4302 - val_loss: 1.5002 - 30s/epoch - 310us/sample
Epoch 18/170
95501/95501 - 30s - loss: 1.4293 - val_loss: 1.4299 - 30s/epoch - 313us/sample
Epoch 19/170
95501/95501 - 30s - loss: 1.4300 - val_loss: 1.4308 - 30s/epoch - 311us/sample
Epoch 20/170
95501/95501 - 30s - loss: 1.4307 - val_loss: 1.4313 - 30s/epoch - 313us/sample
Epoch 21/170
95501/95501 - 30s - loss: 1.4294 - val_loss: 1.4310 - 30s/epoch - 310us/sample
Epoch 22/170
95501/95501 - 30s - loss: 1.4294 - val_loss: 1.4380 - 30s/epoch - 312us/sample
Epoch 23/170
95501/95501 - 30s - loss: 1.4295 - val_loss: 1.4302 - 30s/epoch - 311us/sample
Epoch 24/170
95501/95501 - 30s - loss: 1.4279 - val_loss: 1.4290 - 30s/epoch - 312us/sample
Epoch 25/170
95501/95501 - 30s - loss: 1.4287 - val_loss: 1.4306 - 30s/epoch - 311us/sample
Epoch 26/170
95501/95501 - 30s - loss: 1.4294 - val_loss: 1.4306 - 30s/epoch - 314us/sample
Epoch 27/170
95501/95501 - 30s - loss: 1.4291 - val_loss: 1.4292 - 30s/epoch - 310us/sample
Epoch 28/170
95501/95501 - 30s - loss: 1.4282 - val_loss: 1.4303 - 30s/epoch - 312us/sample
Epoch 29/170
95501/95501 - 30s - loss: 1.4294 - val_loss: 1.4287 - 30s/epoch - 312us/sample
Epoch 30/170
95501/95501 - 30s - loss: 1.4269 - val_loss: 1.4297 - 30s/epoch - 312us/sample
Epoch 31/170
95501/95501 - 30s - loss: 1.4265 - val_loss: 1.4265 - 30s/epoch - 311us/sample
Epoch 32/170
95501/95501 - 30s - loss: 1.4263 - val_loss: 1.4268 - 30s/epoch - 313us/sample
Epoch 33/170
95501/95501 - 30s - loss: 1.4265 - val_loss: 1.4281 - 30s/epoch - 310us/sample
Epoch 34/170
95501/95501 - 30s - loss: 1.4264 - val_loss: 1.4284 - 30s/epoch - 313us/sample
Epoch 35/170
95501/95501 - 30s - loss: 1.4267 - val_loss: 1.4289 - 30s/epoch - 310us/sample
Epoch 36/170
95501/95501 - 30s - loss: 1.4280 - val_loss: 1.4317 - 30s/epoch - 312us/sample
Epoch 37/170
95501/95501 - 30s - loss: 1.4294 - val_loss: 1.4305 - 30s/epoch - 311us/sample
Epoch 38/170
95501/95501 - 30s - loss: 1.4281 - val_loss: 1.4281 - 30s/epoch - 312us/sample
Epoch 39/170
95501/95501 - 30s - loss: 1.4263 - val_loss: 1.4280 - 30s/epoch - 311us/sample
Epoch 40/170
95501/95501 - 30s - loss: 1.4272 - val_loss: 1.4291 - 30s/epoch - 313us/sample
Epoch 41/170
95501/95501 - 30s - loss: 1.4278 - val_loss: 1.4298 - 30s/epoch - 310us/sample
Epoch 42/170
95501/95501 - 30s - loss: 1.4310 - val_loss: 1.4306 - 30s/epoch - 312us/sample
Epoch 43/170
95501/95501 - 30s - loss: 1.4271 - val_loss: 1.4273 - 30s/epoch - 311us/sample
Epoch 44/170
95501/95501 - 30s - loss: 1.4258 - val_loss: 1.4345 - 30s/epoch - 312us/sample
Epoch 45/170
95501/95501 - 30s - loss: 1.4274 - val_loss: 1.4285 - 30s/epoch - 311us/sample
Epoch 46/170
95501/95501 - 30s - loss: 1.4261 - val_loss: 1.4271 - 30s/epoch - 313us/sample
Epoch 47/170
95501/95501 - 30s - loss: 1.4270 - val_loss: 1.4279 - 30s/epoch - 310us/sample
Epoch 48/170
95501/95501 - 30s - loss: 1.4265 - val_loss: 1.4274 - 30s/epoch - 313us/sample
Epoch 49/170
95501/95501 - 30s - loss: 1.4282 - val_loss: 1.4280 - 30s/epoch - 311us/sample
Epoch 50/170
95501/95501 - 30s - loss: 1.4269 - val_loss: 1.4282 - 30s/epoch - 312us/sample
Epoch 51/170
95501/95501 - 30s - loss: 1.4309 - val_loss: 1.4282 - 30s/epoch - 312us/sample
Epoch 52/170
95501/95501 - 30s - loss: 1.4305 - val_loss: 1.4292 - 30s/epoch - 312us/sample
Epoch 53/170
95501/95501 - 30s - loss: 1.4279 - val_loss: 1.4295 - 30s/epoch - 310us/sample
Epoch 54/170
95501/95501 - 30s - loss: 1.4280 - val_loss: 1.4311 - 30s/epoch - 313us/sample
Epoch 55/170
95501/95501 - 30s - loss: 1.4291 - val_loss: 1.4290 - 30s/epoch - 310us/sample
Epoch 56/170
95501/95501 - 30s - loss: 1.4304 - val_loss: 1.4302 - 30s/epoch - 312us/sample
Epoch 57/170
95501/95501 - 30s - loss: 1.4281 - val_loss: 1.4280 - 30s/epoch - 311us/sample
Epoch 58/170
95501/95501 - 30s - loss: 1.4313 - val_loss: 1.4331 - 30s/epoch - 312us/sample
Epoch 59/170
95501/95501 - 30s - loss: 1.4283 - val_loss: 1.4278 - 30s/epoch - 310us/sample
Epoch 60/170
95501/95501 - 30s - loss: 1.4273 - val_loss: 1.4317 - 30s/epoch - 312us/sample
Epoch 61/170
95501/95501 - 30s - loss: 1.4262 - val_loss: 1.4272 - 30s/epoch - 309us/sample
Epoch 62/170
95501/95501 - 30s - loss: 1.4256 - val_loss: 1.4271 - 30s/epoch - 311us/sample
Epoch 63/170
95501/95501 - 30s - loss: 1.4259 - val_loss: 1.4293 - 30s/epoch - 310us/sample
Epoch 64/170
95501/95501 - 30s - loss: 1.4255 - val_loss: 1.4261 - 30s/epoch - 311us/sample
Epoch 65/170
95501/95501 - 30s - loss: 1.4254 - val_loss: 1.4279 - 30s/epoch - 310us/sample
Epoch 66/170
95501/95501 - 30s - loss: 1.4259 - val_loss: 1.4265 - 30s/epoch - 312us/sample
Epoch 67/170
95501/95501 - 30s - loss: 1.4266 - val_loss: 1.4282 - 30s/epoch - 309us/sample
Epoch 68/170
95501/95501 - 30s - loss: 1.4260 - val_loss: 1.4280 - 30s/epoch - 312us/sample
Epoch 69/170
95501/95501 - 30s - loss: 1.4257 - val_loss: 1.4274 - 30s/epoch - 309us/sample
Epoch 70/170
95501/95501 - 30s - loss: 1.4274 - val_loss: 1.4276 - 30s/epoch - 311us/sample
Epoch 71/170
95501/95501 - 30s - loss: 1.4304 - val_loss: 1.4282 - 30s/epoch - 310us/sample
Epoch 72/170
95501/95501 - 30s - loss: 1.4263 - val_loss: 1.4268 - 30s/epoch - 311us/sample
Epoch 73/170
95501/95501 - 30s - loss: 1.4264 - val_loss: 1.4290 - 30s/epoch - 310us/sample
Epoch 74/170
95501/95501 - 30s - loss: 1.4302 - val_loss: 1.4300 - 30s/epoch - 313us/sample
Epoch 75/170
95501/95501 - 30s - loss: 1.4293 - val_loss: 1.4300 - 30s/epoch - 310us/sample
Epoch 76/170
95501/95501 - 30s - loss: 1.4269 - val_loss: 1.4277 - 30s/epoch - 314us/sample
Epoch 77/170
95501/95501 - 30s - loss: 1.4268 - val_loss: 1.4310 - 30s/epoch - 310us/sample
Epoch 78/170
95501/95501 - 30s - loss: 1.4265 - val_loss: 1.4276 - 30s/epoch - 312us/sample
Epoch 79/170
95501/95501 - 30s - loss: 1.4254 - val_loss: 1.4284 - 30s/epoch - 311us/sample
Epoch 80/170
95501/95501 - 30s - loss: 1.4253 - val_loss: 1.4256 - 30s/epoch - 312us/sample
Epoch 81/170
95501/95501 - 30s - loss: 1.4247 - val_loss: 1.4269 - 30s/epoch - 311us/sample
Epoch 82/170
95501/95501 - 30s - loss: 1.4253 - val_loss: 1.4299 - 30s/epoch - 312us/sample
Epoch 83/170
95501/95501 - 30s - loss: 1.4268 - val_loss: 1.4285 - 30s/epoch - 310us/sample
Epoch 84/170
95501/95501 - 30s - loss: 1.4255 - val_loss: 1.4267 - 30s/epoch - 313us/sample
Epoch 85/170
95501/95501 - 30s - loss: 1.4251 - val_loss: 1.4269 - 30s/epoch - 310us/sample
Epoch 86/170
95501/95501 - 30s - loss: 1.4246 - val_loss: 1.4260 - 30s/epoch - 312us/sample
Epoch 87/170
95501/95501 - 30s - loss: 1.4247 - val_loss: 1.4263 - 30s/epoch - 310us/sample
Epoch 88/170
95501/95501 - 30s - loss: 1.4246 - val_loss: 1.4280 - 30s/epoch - 312us/sample
Epoch 89/170
95501/95501 - 30s - loss: 1.4248 - val_loss: 1.4270 - 30s/epoch - 311us/sample
Epoch 90/170
95501/95501 - 30s - loss: 1.4250 - val_loss: 1.4272 - 30s/epoch - 312us/sample
Epoch 91/170
95501/95501 - 30s - loss: 1.4250 - val_loss: 1.4259 - 30s/epoch - 311us/sample
Epoch 92/170
95501/95501 - 30s - loss: 1.4248 - val_loss: 1.4255 - 30s/epoch - 313us/sample
Epoch 93/170
95501/95501 - 30s - loss: 1.4248 - val_loss: 1.4268 - 30s/epoch - 311us/sample
Epoch 94/170
95501/95501 - 30s - loss: 1.4246 - val_loss: 1.4255 - 30s/epoch - 313us/sample
Epoch 95/170
95501/95501 - 30s - loss: 1.4246 - val_loss: 1.4354 - 30s/epoch - 311us/sample
Epoch 96/170
95501/95501 - 30s - loss: 1.4253 - val_loss: 1.4263 - 30s/epoch - 313us/sample
Epoch 97/170
95501/95501 - 30s - loss: 1.4242 - val_loss: 1.4255 - 30s/epoch - 310us/sample
Epoch 98/170
95501/95501 - 30s - loss: 1.4251 - val_loss: 1.4277 - 30s/epoch - 313us/sample
Epoch 99/170
95501/95501 - 30s - loss: 1.4248 - val_loss: 1.4257 - 30s/epoch - 310us/sample
Epoch 100/170
95501/95501 - 30s - loss: 1.4239 - val_loss: 1.4257 - 30s/epoch - 312us/sample
Epoch 101/170
95501/95501 - 30s - loss: 1.4257 - val_loss: 1.4261 - 30s/epoch - 310us/sample
Epoch 102/170
95501/95501 - 30s - loss: 1.4240 - val_loss: 1.4252 - 30s/epoch - 312us/sample
Epoch 103/170
95501/95501 - 30s - loss: 1.4246 - val_loss: 1.4274 - 30s/epoch - 310us/sample
Epoch 104/170
95501/95501 - 30s - loss: 1.4255 - val_loss: 1.4263 - 30s/epoch - 314us/sample
Epoch 105/170
95501/95501 - 30s - loss: 1.4241 - val_loss: 1.4254 - 30s/epoch - 310us/sample
Epoch 106/170
95501/95501 - 30s - loss: 1.4239 - val_loss: 1.4248 - 30s/epoch - 313us/sample
Epoch 107/170
95501/95501 - 30s - loss: 1.4245 - val_loss: 1.4266 - 30s/epoch - 311us/sample
Epoch 108/170
95501/95501 - 30s - loss: 1.4246 - val_loss: 1.4258 - 30s/epoch - 313us/sample
Epoch 109/170
95501/95501 - 30s - loss: 1.4242 - val_loss: 1.4263 - 30s/epoch - 310us/sample
Epoch 110/170
95501/95501 - 30s - loss: 1.4248 - val_loss: 1.4256 - 30s/epoch - 312us/sample
Epoch 111/170
95501/95501 - 30s - loss: 1.4243 - val_loss: 1.4261 - 30s/epoch - 311us/sample
Epoch 112/170
95501/95501 - 30s - loss: 1.4263 - val_loss: 1.4287 - 30s/epoch - 312us/sample
Epoch 113/170
95501/95501 - 30s - loss: 1.4255 - val_loss: 1.4265 - 30s/epoch - 312us/sample
Epoch 114/170
95501/95501 - 30s - loss: 1.4251 - val_loss: 1.4269 - 30s/epoch - 312us/sample
Epoch 115/170
95501/95501 - 30s - loss: 1.4244 - val_loss: 1.4256 - 30s/epoch - 310us/sample
Epoch 116/170
95501/95501 - 30s - loss: 1.4240 - val_loss: 1.4270 - 30s/epoch - 313us/sample
Epoch 117/170
95501/95501 - 30s - loss: 1.4238 - val_loss: 1.4251 - 30s/epoch - 310us/sample
Epoch 118/170
95501/95501 - 30s - loss: 1.4238 - val_loss: 1.4255 - 30s/epoch - 312us/sample
Epoch 119/170
95501/95501 - 30s - loss: 1.4243 - val_loss: 1.4257 - 30s/epoch - 312us/sample
Epoch 120/170
95501/95501 - 30s - loss: 1.4241 - val_loss: 1.4261 - 30s/epoch - 312us/sample
Epoch 121/170
95501/95501 - 30s - loss: 1.4252 - val_loss: 1.4273 - 30s/epoch - 311us/sample
Epoch 122/170
95501/95501 - 30s - loss: 1.4237 - val_loss: 1.4254 - 30s/epoch - 313us/sample
Epoch 123/170
95501/95501 - 30s - loss: 1.4236 - val_loss: 1.4258 - 30s/epoch - 310us/sample
Epoch 124/170
95501/95501 - 30s - loss: 1.4234 - val_loss: 1.4250 - 30s/epoch - 314us/sample
Epoch 125/170
95501/95501 - 30s - loss: 1.4237 - val_loss: 1.4254 - 30s/epoch - 310us/sample
Epoch 126/170
95501/95501 - 30s - loss: 1.4238 - val_loss: 1.4256 - 30s/epoch - 313us/sample
Epoch 127/170
95501/95501 - 30s - loss: 1.4241 - val_loss: 1.4256 - 30s/epoch - 311us/sample
Epoch 128/170
95501/95501 - 30s - loss: 1.4239 - val_loss: 1.4254 - 30s/epoch - 313us/sample
Epoch 129/170
95501/95501 - 30s - loss: 1.4250 - val_loss: 1.4262 - 30s/epoch - 310us/sample
Epoch 130/170
95501/95501 - 30s - loss: 1.4270 - val_loss: 1.4268 - 30s/epoch - 313us/sample
Epoch 131/170
95501/95501 - 30s - loss: 1.4254 - val_loss: 1.4263 - 30s/epoch - 310us/sample
Epoch 132/170
95501/95501 - 30s - loss: 1.4246 - val_loss: 1.4261 - 30s/epoch - 314us/sample
Epoch 133/170
95501/95501 - 30s - loss: 1.4239 - val_loss: 1.4256 - 30s/epoch - 310us/sample
Epoch 134/170
95501/95501 - 30s - loss: 1.4243 - val_loss: 1.4262 - 30s/epoch - 312us/sample
Epoch 135/170
95501/95501 - 30s - loss: 1.4238 - val_loss: 1.4263 - 30s/epoch - 312us/sample
Epoch 136/170
95501/95501 - 30s - loss: 1.4240 - val_loss: 1.4250 - 30s/epoch - 313us/sample
Epoch 137/170
95501/95501 - 30s - loss: 1.4242 - val_loss: 1.4248 - 30s/epoch - 310us/sample
Epoch 138/170
95501/95501 - 30s - loss: 1.4232 - val_loss: 1.4248 - 30s/epoch - 313us/sample
Epoch 139/170
95501/95501 - 30s - loss: 1.4238 - val_loss: 1.4248 - 30s/epoch - 310us/sample
Epoch 140/170
95501/95501 - 30s - loss: 1.4234 - val_loss: 1.4264 - 30s/epoch - 312us/sample
Epoch 141/170
95501/95501 - 30s - loss: 1.4230 - val_loss: 1.4245 - 30s/epoch - 311us/sample
Epoch 142/170
95501/95501 - 30s - loss: 1.4238 - val_loss: 1.4251 - 30s/epoch - 312us/sample
Epoch 143/170
95501/95501 - 30s - loss: 1.4233 - val_loss: 1.4257 - 30s/epoch - 311us/sample
Epoch 144/170
95501/95501 - 30s - loss: 1.4241 - val_loss: 1.4266 - 30s/epoch - 313us/sample
Epoch 145/170
95501/95501 - 30s - loss: 1.4247 - val_loss: 1.4262 - 30s/epoch - 310us/sample
Epoch 146/170
95501/95501 - 30s - loss: 1.4246 - val_loss: 1.4255 - 30s/epoch - 313us/sample
Epoch 147/170
95501/95501 - 30s - loss: 1.4237 - val_loss: 1.4253 - 30s/epoch - 310us/sample
Epoch 148/170
95501/95501 - 30s - loss: 1.4236 - val_loss: 1.4251 - 30s/epoch - 312us/sample
Epoch 149/170
95501/95501 - 30s - loss: 1.4243 - val_loss: 1.4263 - 30s/epoch - 312us/sample
Epoch 150/170
95501/95501 - 30s - loss: 1.4237 - val_loss: 1.4246 - 30s/epoch - 312us/sample
Epoch 151/170
95501/95501 - 30s - loss: 1.4235 - val_loss: 1.4257 - 30s/epoch - 311us/sample
Epoch 152/170
95501/95501 - 30s - loss: 1.4236 - val_loss: 1.4248 - 30s/epoch - 313us/sample
Epoch 153/170
95501/95501 - 30s - loss: 1.4249 - val_loss: 1.4268 - 30s/epoch - 310us/sample
Epoch 154/170
95501/95501 - 30s - loss: 1.4265 - val_loss: 1.4277 - 30s/epoch - 313us/sample
Epoch 155/170
95501/95501 - 30s - loss: 1.4253 - val_loss: 1.4279 - 30s/epoch - 311us/sample
Epoch 156/170
95501/95501 - 30s - loss: 1.4252 - val_loss: 1.4265 - 30s/epoch - 312us/sample
Epoch 157/170
95501/95501 - 30s - loss: 1.4252 - val_loss: 1.4269 - 30s/epoch - 311us/sample
Epoch 158/170
95501/95501 - 30s - loss: 1.4257 - val_loss: 1.4272 - 30s/epoch - 312us/sample
Epoch 159/170
95501/95501 - 30s - loss: 1.4253 - val_loss: 1.4266 - 30s/epoch - 310us/sample
Epoch 160/170
95501/95501 - 30s - loss: 1.4252 - val_loss: 1.4275 - 30s/epoch - 313us/sample
Epoch 161/170
95501/95501 - 30s - loss: 1.4253 - val_loss: 1.4268 - 30s/epoch - 310us/sample
Epoch 162/170
95501/95501 - 30s - loss: 1.4251 - val_loss: 1.4269 - 30s/epoch - 311us/sample
Epoch 163/170
95501/95501 - 30s - loss: 1.4251 - val_loss: 1.4263 - 30s/epoch - 311us/sample
Epoch 164/170
95501/95501 - 30s - loss: 1.4247 - val_loss: 1.4263 - 30s/epoch - 312us/sample
Epoch 165/170
95501/95501 - 30s - loss: 1.4249 - val_loss: 1.4265 - 30s/epoch - 310us/sample
Epoch 166/170
95501/95501 - 30s - loss: 1.4247 - val_loss: 1.4262 - 30s/epoch - 313us/sample
Epoch 167/170
95501/95501 - 30s - loss: 1.4249 - val_loss: 1.4271 - 30s/epoch - 310us/sample
Epoch 168/170
95501/95501 - 30s - loss: 1.4250 - val_loss: 1.4264 - 30s/epoch - 312us/sample
Epoch 169/170
95501/95501 - 30s - loss: 1.4250 - val_loss: 1.4265 - 30s/epoch - 311us/sample
Epoch 170/170
95501/95501 - 30s - loss: 1.4254 - val_loss: 1.4268 - 30s/epoch - 312us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 1.4268354902256672
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 20:29:01.375403: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_5/outputlayer/BiasAdd' id:6934 op device:{requested: '', assigned: ''} def:{{{node decoder_model_5/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_5/outputlayer/MatMul, decoder_model_5/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.8277870422530894
cosine 0.9267904384883419
MAE: 13.117506
RMSE: 44.608757
r2: -129084.06513702002
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'binary_crossentropy', 16, 170, 0.0005, 0.6, 758, 1.4253922360820737, 1.4268354902256672, 0.8277870422530894, 0.9267904384883419, 13.11750602722168, 44.60875701904297, -129084.06513702002, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 10 0.001 16 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_18 (BatchN  (None, 2528)        10112       ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_18 (ReLU)                (None, 2528)         0           ['batch_normalization_18[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1916982     ['re_lu_18[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1916982     ['re_lu_18[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5703874     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 12,745,870
Trainable params: 12,734,242
Non-trainable params: 11,628
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/10
2023-02-14 20:29:08.949286: W tensorflow/c/c_api.cc:291] Operation '{name:'training_12/Adam/dense_enc0_6/bias/m/Assign' id:8766 op device:{requested: '', assigned: ''} def:{{{node training_12/Adam/dense_enc0_6/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_12/Adam/dense_enc0_6/bias/m, training_12/Adam/dense_enc0_6/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 20:29:37.394965: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_6/mul' id:8306 op device:{requested: '', assigned: ''} def:{{{node loss_6/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_6/mul/x, loss_6/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 31s - loss: 0.0114 - val_loss: 2.5971 - 31s/epoch - 322us/sample
Epoch 2/10
95501/95501 - 28s - loss: 0.0046 - val_loss: 0.0040 - 28s/epoch - 293us/sample
Epoch 3/10
95501/95501 - 28s - loss: 0.0034 - val_loss: 0.0026 - 28s/epoch - 293us/sample
Epoch 4/10
95501/95501 - 28s - loss: 0.0027 - val_loss: 0.0022 - 28s/epoch - 294us/sample
Epoch 5/10
95501/95501 - 28s - loss: 0.0024 - val_loss: 0.0019 - 28s/epoch - 293us/sample
Epoch 6/10
95501/95501 - 28s - loss: 0.0022 - val_loss: 0.0018 - 28s/epoch - 293us/sample
Epoch 7/10
95501/95501 - 28s - loss: 0.0020 - val_loss: 0.0017 - 28s/epoch - 294us/sample
Epoch 8/10
95501/95501 - 28s - loss: 0.0019 - val_loss: 0.0017 - 28s/epoch - 292us/sample
Epoch 9/10
95501/95501 - 28s - loss: 0.0018 - val_loss: 0.0016 - 28s/epoch - 293us/sample
Epoch 10/10
95501/95501 - 28s - loss: 0.0018 - val_loss: 0.0016 - 28s/epoch - 295us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0015546445837655114
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 20:33:51.223970: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_6/outputlayer/BiasAdd' id:8277 op device:{requested: '', assigned: ''} def:{{{node decoder_model_6/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_6/outputlayer/MatMul, decoder_model_6/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.01393675624403107
cosine 0.011107750363364293
MAE: 0.018680504
RMSE: 0.03501913
r2: 0.9204451395314948
RMSE zero-vector: 0.23411466903540806
['2.0custom_VAE', 'mse', 16, 10, 0.001, 0.6, 758, 0.0017680237073785245, 0.0015546445837655114, 0.01393675624403107, 0.011107750363364293, 0.018680503591895103, 0.035019129514694214, 0.9204451395314948, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 90 0.001 32 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_21 (BatchN  (None, 1896)        7584        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_21 (ReLU)                (None, 1896)         0           ['batch_normalization_21[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu_21[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu_21[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/90
2023-02-14 20:33:59.205953: W tensorflow/c/c_api.cc:291] Operation '{name:'training_14/Adam/dense_dec1_7/kernel/m/Assign' id:10072 op device:{requested: '', assigned: ''} def:{{{node training_14/Adam/dense_dec1_7/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_14/Adam/dense_dec1_7/kernel/m, training_14/Adam/dense_dec1_7/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 20:34:16.500800: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_7/mul' id:9567 op device:{requested: '', assigned: ''} def:{{{node loss_7/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_7/mul/x, loss_7/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 19s - loss: 0.0092 - val_loss: 0.0056 - 19s/epoch - 202us/sample
Epoch 2/90
95501/95501 - 17s - loss: 0.0043 - val_loss: 0.0037 - 17s/epoch - 176us/sample
Epoch 3/90
95501/95501 - 17s - loss: 0.0033 - val_loss: 0.0024 - 17s/epoch - 175us/sample
Epoch 4/90
95501/95501 - 17s - loss: 0.0026 - val_loss: 0.0019 - 17s/epoch - 177us/sample
Epoch 5/90
95501/95501 - 17s - loss: 0.0021 - val_loss: 0.0017 - 17s/epoch - 174us/sample
Epoch 6/90
95501/95501 - 17s - loss: 0.0018 - val_loss: 0.0015 - 17s/epoch - 176us/sample
Epoch 7/90
95501/95501 - 17s - loss: 0.0016 - val_loss: 0.0014 - 17s/epoch - 174us/sample
Epoch 8/90
95501/95501 - 17s - loss: 0.0015 - val_loss: 0.0013 - 17s/epoch - 176us/sample
Epoch 9/90
95501/95501 - 17s - loss: 0.0014 - val_loss: 0.0012 - 17s/epoch - 176us/sample
Epoch 10/90
95501/95501 - 17s - loss: 0.0014 - val_loss: 0.0012 - 17s/epoch - 176us/sample
Epoch 11/90
95501/95501 - 17s - loss: 0.0013 - val_loss: 0.0011 - 17s/epoch - 174us/sample
Epoch 12/90
95501/95501 - 17s - loss: 0.0013 - val_loss: 0.0011 - 17s/epoch - 176us/sample
Epoch 13/90
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0011 - 17s/epoch - 175us/sample
Epoch 14/90
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0010 - 17s/epoch - 177us/sample
Epoch 15/90
95501/95501 - 17s - loss: 0.0012 - val_loss: 0.0012 - 17s/epoch - 175us/sample
Epoch 16/90
95501/95501 - 17s - loss: 0.0011 - val_loss: 9.8811e-04 - 17s/epoch - 176us/sample
Epoch 17/90
95501/95501 - 17s - loss: 0.0011 - val_loss: 9.7019e-04 - 17s/epoch - 175us/sample
Epoch 18/90
95501/95501 - 17s - loss: 0.0011 - val_loss: 9.7882e-04 - 17s/epoch - 177us/sample
Epoch 19/90
95501/95501 - 17s - loss: 0.0011 - val_loss: 9.4449e-04 - 17s/epoch - 175us/sample
Epoch 20/90
95501/95501 - 17s - loss: 0.0011 - val_loss: 9.5610e-04 - 17s/epoch - 175us/sample
Epoch 21/90
95501/95501 - 17s - loss: 0.0011 - val_loss: 9.2428e-04 - 17s/epoch - 175us/sample
Epoch 22/90
95501/95501 - 17s - loss: 0.0011 - val_loss: 9.6291e-04 - 17s/epoch - 176us/sample
Epoch 23/90
95501/95501 - 17s - loss: 0.0010 - val_loss: 9.1211e-04 - 17s/epoch - 175us/sample
Epoch 24/90
95501/95501 - 17s - loss: 0.0010 - val_loss: 8.9445e-04 - 17s/epoch - 176us/sample
Epoch 25/90
95501/95501 - 17s - loss: 0.0010 - val_loss: 8.9816e-04 - 17s/epoch - 175us/sample
Epoch 26/90
95501/95501 - 17s - loss: 0.0010 - val_loss: 9.0944e-04 - 17s/epoch - 176us/sample
Epoch 27/90
95501/95501 - 17s - loss: 0.0010 - val_loss: 9.9758e-04 - 17s/epoch - 176us/sample
Epoch 28/90
95501/95501 - 17s - loss: 0.0010 - val_loss: 8.9206e-04 - 17s/epoch - 176us/sample
Epoch 29/90
95501/95501 - 17s - loss: 9.8865e-04 - val_loss: 8.5747e-04 - 17s/epoch - 182us/sample
Epoch 30/90
95501/95501 - 17s - loss: 0.0010 - val_loss: 8.7686e-04 - 17s/epoch - 183us/sample
Epoch 31/90
95501/95501 - 17s - loss: 9.6929e-04 - val_loss: 8.5618e-04 - 17s/epoch - 183us/sample
Epoch 32/90
95501/95501 - 18s - loss: 9.7070e-04 - val_loss: 8.5072e-04 - 18s/epoch - 184us/sample
Epoch 33/90
95501/95501 - 17s - loss: 9.6953e-04 - val_loss: 9.3672e-04 - 17s/epoch - 183us/sample
Epoch 34/90
95501/95501 - 17s - loss: 9.8605e-04 - val_loss: 8.4280e-04 - 17s/epoch - 183us/sample
Epoch 35/90
95501/95501 - 17s - loss: 9.4345e-04 - val_loss: 8.4428e-04 - 17s/epoch - 183us/sample
Epoch 36/90
95501/95501 - 17s - loss: 9.7896e-04 - val_loss: 8.4359e-04 - 17s/epoch - 183us/sample
Epoch 37/90
95501/95501 - 18s - loss: 9.6658e-04 - val_loss: 8.4003e-04 - 18s/epoch - 184us/sample
Epoch 38/90
95501/95501 - 17s - loss: 9.5358e-04 - val_loss: 8.3421e-04 - 17s/epoch - 183us/sample
Epoch 39/90
95501/95501 - 17s - loss: 9.3543e-04 - val_loss: 8.3291e-04 - 17s/epoch - 183us/sample
Epoch 40/90
95501/95501 - 17s - loss: 9.2846e-04 - val_loss: 8.8323e-04 - 17s/epoch - 183us/sample
Epoch 41/90
95501/95501 - 17s - loss: 9.6328e-04 - val_loss: 8.2415e-04 - 17s/epoch - 182us/sample
Epoch 42/90
95501/95501 - 18s - loss: 9.3181e-04 - val_loss: 8.1912e-04 - 18s/epoch - 184us/sample
Epoch 43/90
95501/95501 - 17s - loss: 9.1229e-04 - val_loss: 8.1137e-04 - 17s/epoch - 182us/sample
Epoch 44/90
95501/95501 - 17s - loss: 9.2295e-04 - val_loss: 8.2150e-04 - 17s/epoch - 183us/sample
Epoch 45/90
95501/95501 - 17s - loss: 9.1113e-04 - val_loss: 8.0860e-04 - 17s/epoch - 183us/sample
Epoch 46/90
95501/95501 - 17s - loss: 9.1885e-04 - val_loss: 8.2753e-04 - 17s/epoch - 183us/sample
Epoch 47/90
95501/95501 - 18s - loss: 9.0271e-04 - val_loss: 7.9176e-04 - 18s/epoch - 184us/sample
Epoch 48/90
95501/95501 - 17s - loss: 9.0414e-04 - val_loss: 8.0663e-04 - 17s/epoch - 183us/sample
Epoch 49/90
95501/95501 - 17s - loss: 9.0413e-04 - val_loss: 7.9076e-04 - 17s/epoch - 183us/sample
Epoch 50/90
95501/95501 - 17s - loss: 8.9373e-04 - val_loss: 8.0097e-04 - 17s/epoch - 183us/sample
Epoch 51/90
95501/95501 - 18s - loss: 9.1834e-04 - val_loss: 8.1519e-04 - 18s/epoch - 183us/sample
Epoch 52/90
95501/95501 - 17s - loss: 8.9418e-04 - val_loss: 7.9926e-04 - 17s/epoch - 183us/sample
Epoch 53/90
95501/95501 - 17s - loss: 8.9484e-04 - val_loss: 8.0497e-04 - 17s/epoch - 183us/sample
Epoch 54/90
95501/95501 - 17s - loss: 8.8297e-04 - val_loss: 7.8566e-04 - 17s/epoch - 183us/sample
Epoch 55/90
95501/95501 - 17s - loss: 8.8483e-04 - val_loss: 7.8011e-04 - 17s/epoch - 183us/sample
Epoch 56/90
95501/95501 - 18s - loss: 8.8268e-04 - val_loss: 7.8496e-04 - 18s/epoch - 184us/sample
Epoch 57/90
95501/95501 - 17s - loss: 8.7985e-04 - val_loss: 8.1136e-04 - 17s/epoch - 183us/sample
Epoch 58/90
95501/95501 - 17s - loss: 8.7959e-04 - val_loss: 7.7659e-04 - 17s/epoch - 183us/sample
Epoch 59/90
95501/95501 - 17s - loss: 8.7078e-04 - val_loss: 7.7927e-04 - 17s/epoch - 183us/sample
Epoch 60/90
95501/95501 - 17s - loss: 8.6891e-04 - val_loss: 7.7858e-04 - 17s/epoch - 183us/sample
Epoch 61/90
95501/95501 - 18s - loss: 8.6578e-04 - val_loss: 8.1112e-04 - 18s/epoch - 183us/sample
Epoch 62/90
95501/95501 - 18s - loss: 8.7739e-04 - val_loss: 7.8039e-04 - 18s/epoch - 183us/sample
Epoch 63/90
95501/95501 - 17s - loss: 8.6592e-04 - val_loss: 7.7672e-04 - 17s/epoch - 183us/sample
Epoch 64/90
95501/95501 - 17s - loss: 8.6396e-04 - val_loss: 7.7356e-04 - 17s/epoch - 183us/sample
Epoch 65/90
95501/95501 - 18s - loss: 8.6781e-04 - val_loss: 7.6390e-04 - 18s/epoch - 184us/sample
Epoch 66/90
95501/95501 - 18s - loss: 8.5785e-04 - val_loss: 7.6795e-04 - 18s/epoch - 183us/sample
Epoch 67/90
95501/95501 - 17s - loss: 8.5617e-04 - val_loss: 7.6718e-04 - 17s/epoch - 183us/sample
Epoch 68/90
95501/95501 - 17s - loss: 8.5592e-04 - val_loss: 8.1204e-04 - 17s/epoch - 183us/sample
Epoch 69/90
95501/95501 - 17s - loss: 8.6008e-04 - val_loss: 7.7570e-04 - 17s/epoch - 183us/sample
Epoch 70/90
95501/95501 - 18s - loss: 8.5556e-04 - val_loss: 7.5894e-04 - 18s/epoch - 184us/sample
Epoch 71/90
95501/95501 - 17s - loss: 8.4538e-04 - val_loss: 7.5613e-04 - 17s/epoch - 183us/sample
Epoch 72/90
95501/95501 - 17s - loss: 8.8253e-04 - val_loss: 7.7851e-04 - 17s/epoch - 183us/sample
Epoch 73/90
95501/95501 - 17s - loss: 8.5729e-04 - val_loss: 7.5863e-04 - 17s/epoch - 183us/sample
Epoch 74/90
95501/95501 - 18s - loss: 8.4675e-04 - val_loss: 7.5836e-04 - 18s/epoch - 184us/sample
Epoch 75/90
95501/95501 - 17s - loss: 8.8686e-04 - val_loss: 7.6877e-04 - 17s/epoch - 183us/sample
Epoch 76/90
95501/95501 - 18s - loss: 8.4990e-04 - val_loss: 7.6239e-04 - 18s/epoch - 183us/sample
Epoch 77/90
95501/95501 - 17s - loss: 8.5154e-04 - val_loss: 9.1954e-04 - 17s/epoch - 183us/sample
Epoch 78/90
95501/95501 - 18s - loss: 9.1297e-04 - val_loss: 7.6577e-04 - 18s/epoch - 184us/sample
Epoch 79/90
95501/95501 - 17s - loss: 8.4512e-04 - val_loss: 7.6040e-04 - 17s/epoch - 183us/sample
Epoch 80/90
95501/95501 - 17s - loss: 8.4731e-04 - val_loss: 7.5137e-04 - 17s/epoch - 183us/sample
Epoch 81/90
95501/95501 - 17s - loss: 8.5662e-04 - val_loss: 7.6517e-04 - 17s/epoch - 183us/sample
Epoch 82/90
95501/95501 - 18s - loss: 8.5802e-04 - val_loss: 7.6320e-04 - 18s/epoch - 184us/sample
Epoch 83/90
95501/95501 - 17s - loss: 8.3526e-04 - val_loss: 7.5002e-04 - 17s/epoch - 183us/sample
Epoch 84/90
95501/95501 - 17s - loss: 8.4241e-04 - val_loss: 0.0013 - 17s/epoch - 183us/sample
Epoch 85/90
95501/95501 - 17s - loss: 8.4948e-04 - val_loss: 7.4911e-04 - 17s/epoch - 183us/sample
Epoch 86/90
95501/95501 - 18s - loss: 8.6580e-04 - val_loss: 7.4391e-04 - 18s/epoch - 184us/sample
Epoch 87/90
95501/95501 - 17s - loss: 8.4493e-04 - val_loss: 7.4355e-04 - 17s/epoch - 183us/sample
Epoch 88/90
95501/95501 - 17s - loss: 8.5438e-04 - val_loss: 7.4271e-04 - 17s/epoch - 183us/sample
Epoch 89/90
95501/95501 - 17s - loss: 8.3647e-04 - val_loss: 7.5030e-04 - 17s/epoch - 182us/sample
Epoch 90/90
95501/95501 - 18s - loss: 8.3539e-04 - val_loss: 7.5229e-04 - 18s/epoch - 183us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0007522857818488191
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 20:59:54.552990: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_7/outputlayer/BiasAdd' id:9538 op device:{requested: '', assigned: ''} def:{{{node decoder_model_7/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_7/outputlayer/MatMul, decoder_model_7/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.005537474544103129
cosine 0.0043819493958512385
MAE: 0.011550102
RMSE: 0.021863716
r2: 0.9689899731134992
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'mse', 32, 90, 0.001, 0.6, 758, 0.0008353933914001481, 0.0007522857818488191, 0.005537474544103129, 0.0043819493958512385, 0.01155010238289833, 0.02186371572315693, 0.9689899731134992, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 10 0.001 64 0] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_24 (BatchN  (None, 632)         2528        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_24 (ReLU)                (None, 632)          0           ['batch_normalization_24[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          479814      ['re_lu_24[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          479814      ['re_lu_24[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1860682     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 3,622,318
Trainable params: 3,618,274
Non-trainable params: 4,044
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/10
2023-02-14 21:00:02.922464: W tensorflow/c/c_api.cc:291] Operation '{name:'bottleneck_zmean_8/bias/Assign' id:10476 op device:{requested: '', assigned: ''} def:{{{node bottleneck_zmean_8/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](bottleneck_zmean_8/bias, bottleneck_zmean_8/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 21:00:14.619090: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_8/mul' id:10827 op device:{requested: '', assigned: ''} def:{{{node loss_8/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_8/mul/x, loss_8/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 14s - loss: 93.4241 - val_loss: 0.5349 - 14s/epoch - 144us/sample
Epoch 2/10
95501/95501 - 11s - loss: 0.5725 - val_loss: 0.4839 - 11s/epoch - 115us/sample
Epoch 3/10
95501/95501 - 11s - loss: 0.4514 - val_loss: 0.3850 - 11s/epoch - 115us/sample
Epoch 4/10
95501/95501 - 11s - loss: 0.4644 - val_loss: 0.3413 - 11s/epoch - 115us/sample
Epoch 5/10
95501/95501 - 11s - loss: 0.5122 - val_loss: 0.4311 - 11s/epoch - 115us/sample
Epoch 6/10
95501/95501 - 11s - loss: 0.3921 - val_loss: 0.3542 - 11s/epoch - 115us/sample
Epoch 7/10
95501/95501 - 11s - loss: 0.3472 - val_loss: 0.3186 - 11s/epoch - 116us/sample
Epoch 8/10
95501/95501 - 11s - loss: 0.3205 - val_loss: 0.3091 - 11s/epoch - 115us/sample
Epoch 9/10
95501/95501 - 11s - loss: 0.3225 - val_loss: 0.3051 - 11s/epoch - 115us/sample
Epoch 10/10
95501/95501 - 11s - loss: 0.3103 - val_loss: 0.2914 - 11s/epoch - 115us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.2914474784788706
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 21:01:54.739897: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_8/outputlayer/BiasAdd' id:10779 op device:{requested: '', assigned: ''} def:{{{node decoder_model_8/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_8/outputlayer/MatMul, decoder_model_8/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.20031354204803153
cosine 0.2719561403834258
MAE: 0.8127817
RMSE: 4.057267
r2: -1065.964511673054
RMSE zero-vector: 0.23411466903540806
['0.5custom_VAE', 'binary_crossentropy', 64, 10, 0.001, 0.6, 758, 0.31028007378608247, 0.2914474784788706, 0.20031354204803153, 0.2719561403834258, 0.8127816915512085, 4.057267189025879, -1065.964511673054, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 30 0.0005 8 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_27 (BatchN  (None, 1896)        7584        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_27 (ReLU)                (None, 1896)         0           ['batch_normalization_27[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu_27[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu_27[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/30
2023-02-14 21:02:03.141233: W tensorflow/c/c_api.cc:291] Operation '{name:'training_18/Adam/batch_normalization_28/gamma/v/Assign' id:12761 op device:{requested: '', assigned: ''} def:{{{node training_18/Adam/batch_normalization_28/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_18/Adam/batch_normalization_28/gamma/v, training_18/Adam/batch_normalization_28/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 21:02:59.666672: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_9/mul' id:12130 op device:{requested: '', assigned: ''} def:{{{node loss_9/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_9/mul/x, loss_9/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 60s - loss: 0.0100 - val_loss: 0.0080 - 60s/epoch - 632us/sample
Epoch 2/30
95501/95501 - 56s - loss: 0.0051 - val_loss: 0.0083 - 56s/epoch - 590us/sample
Epoch 3/30
95501/95501 - 56s - loss: 0.0043 - val_loss: 0.0079 - 56s/epoch - 592us/sample
Epoch 4/30
95501/95501 - 56s - loss: 0.0038 - val_loss: 0.0059 - 56s/epoch - 590us/sample
Epoch 5/30
95501/95501 - 56s - loss: 0.0035 - val_loss: 0.0055 - 56s/epoch - 590us/sample
Epoch 6/30
95501/95501 - 56s - loss: 0.0033 - val_loss: 0.0047 - 56s/epoch - 591us/sample
Epoch 7/30
95501/95501 - 58s - loss: 0.0032 - val_loss: 0.0047 - 58s/epoch - 608us/sample
Epoch 8/30
95501/95501 - 60s - loss: 0.0031 - val_loss: 0.0047 - 60s/epoch - 624us/sample
Epoch 9/30
95501/95501 - 54s - loss: 0.0030 - val_loss: 0.0047 - 54s/epoch - 561us/sample
Epoch 10/30
95501/95501 - 52s - loss: 0.0029 - val_loss: 0.0041 - 52s/epoch - 540us/sample
Epoch 11/30
95501/95501 - 51s - loss: 0.0029 - val_loss: 0.0039 - 51s/epoch - 538us/sample
Epoch 12/30
95501/95501 - 52s - loss: 0.0028 - val_loss: 0.0035 - 52s/epoch - 540us/sample
Epoch 13/30
95501/95501 - 51s - loss: 0.0027 - val_loss: 0.0035 - 51s/epoch - 539us/sample
Epoch 14/30
95501/95501 - 51s - loss: 0.0027 - val_loss: 0.0034 - 51s/epoch - 539us/sample
Epoch 15/30
95501/95501 - 52s - loss: 0.0027 - val_loss: 0.0035 - 52s/epoch - 539us/sample
Epoch 16/30
95501/95501 - 51s - loss: 0.0026 - val_loss: 0.0031 - 51s/epoch - 539us/sample
Epoch 17/30
95501/95501 - 52s - loss: 0.0026 - val_loss: 0.0033 - 52s/epoch - 540us/sample
Epoch 18/30
95501/95501 - 51s - loss: 0.0026 - val_loss: 0.0032 - 51s/epoch - 538us/sample
Epoch 19/30
95501/95501 - 52s - loss: 0.0026 - val_loss: 0.0032 - 52s/epoch - 540us/sample
Epoch 20/30
95501/95501 - 52s - loss: 0.0025 - val_loss: 0.0030 - 52s/epoch - 540us/sample
Epoch 21/30
95501/95501 - 51s - loss: 0.0025 - val_loss: 0.0033 - 51s/epoch - 539us/sample
Epoch 22/30
95501/95501 - 51s - loss: 0.0025 - val_loss: 0.0034 - 51s/epoch - 539us/sample
Epoch 23/30
95501/95501 - 51s - loss: 0.0025 - val_loss: 0.0033 - 51s/epoch - 539us/sample
Epoch 24/30
95501/95501 - 51s - loss: 0.0025 - val_loss: 0.0035 - 51s/epoch - 538us/sample
Epoch 25/30
95501/95501 - 52s - loss: 0.0024 - val_loss: 0.0035 - 52s/epoch - 540us/sample
Epoch 26/30
95501/95501 - 51s - loss: 0.0024 - val_loss: 0.0033 - 51s/epoch - 538us/sample
Epoch 27/30
95501/95501 - 52s - loss: 0.0024 - val_loss: 0.0032 - 52s/epoch - 540us/sample
Epoch 28/30
95501/95501 - 52s - loss: 0.0024 - val_loss: 0.0036 - 52s/epoch - 540us/sample
Epoch 29/30
95501/95501 - 52s - loss: 0.0024 - val_loss: 0.0032 - 52s/epoch - 539us/sample
Epoch 30/30
95501/95501 - 51s - loss: 0.0024 - val_loss: 0.0032 - 51s/epoch - 538us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0032106335730356142
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 21:28:37.247690: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_9/outputlayer/BiasAdd' id:12101 op device:{requested: '', assigned: ''} def:{{{node decoder_model_9/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_9/outputlayer/MatMul, decoder_model_9/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.02508366645180051
cosine 0.02048077509846276
MAE: 0.024116257
RMSE: 0.054032348
r2: 0.8106052224199839
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'mse', 8, 30, 0.0005, 0.6, 758, 0.0023647140523684056, 0.0032106335730356142, 0.02508366645180051, 0.02048077509846276, 0.024116257205605507, 0.0540323480963707, 0.8106052224199839, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 85 0.001 64 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_30 (BatchN  (None, 1896)        7584        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_30 (ReLU)                (None, 1896)         0           ['batch_normalization_30[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu_30[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu_30[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/85
2023-02-14 21:28:46.246394: W tensorflow/c/c_api.cc:291] Operation '{name:'training_20/Adam/dense_dec1_10/bias/v/Assign' id:14017 op device:{requested: '', assigned: ''} def:{{{node training_20/Adam/dense_dec1_10/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_20/Adam/dense_dec1_10/bias/v, training_20/Adam/dense_dec1_10/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 21:28:57.813330: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_10/mul' id:13391 op device:{requested: '', assigned: ''} def:{{{node loss_10/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_10/mul/x, loss_10/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 14s - loss: 0.0197 - val_loss: 0.0115 - 14s/epoch - 146us/sample
Epoch 2/85
95501/95501 - 10s - loss: 0.0051 - val_loss: 0.0036 - 10s/epoch - 110us/sample
Epoch 3/85
95501/95501 - 10s - loss: 0.0038 - val_loss: 0.0042 - 10s/epoch - 109us/sample
Epoch 4/85
95501/95501 - 11s - loss: 0.0035 - val_loss: 0.0029 - 11s/epoch - 111us/sample
Epoch 5/85
95501/95501 - 10s - loss: 0.3258 - val_loss: 0.0026 - 10s/epoch - 110us/sample
Epoch 6/85
95501/95501 - 10s - loss: 0.0027 - val_loss: 0.0022 - 10s/epoch - 109us/sample
Epoch 7/85
95501/95501 - 10s - loss: 0.0024 - val_loss: 0.0020 - 10s/epoch - 110us/sample
Epoch 8/85
95501/95501 - 10s - loss: 0.0179 - val_loss: 0.0018 - 10s/epoch - 109us/sample
Epoch 9/85
95501/95501 - 10s - loss: 0.0019 - val_loss: 0.0023 - 10s/epoch - 109us/sample
Epoch 10/85
95501/95501 - 10s - loss: 0.0019 - val_loss: 0.0073 - 10s/epoch - 110us/sample
Epoch 11/85
95501/95501 - 10s - loss: 0.0071 - val_loss: 0.0024 - 10s/epoch - 109us/sample
Epoch 12/85
95501/95501 - 10s - loss: 0.0023 - val_loss: 0.0018 - 10s/epoch - 109us/sample
Epoch 13/85
95501/95501 - 11s - loss: 0.0020 - val_loss: 0.0016 - 11s/epoch - 111us/sample
Epoch 14/85
95501/95501 - 10s - loss: 0.0019 - val_loss: 0.0076 - 10s/epoch - 110us/sample
Epoch 15/85
95501/95501 - 10s - loss: 0.0024 - val_loss: 0.0017 - 10s/epoch - 110us/sample
Epoch 16/85
95501/95501 - 10s - loss: 0.0021 - val_loss: 0.0017 - 10s/epoch - 109us/sample
Epoch 17/85
95501/95501 - 10s - loss: 0.0017 - val_loss: 0.0015 - 10s/epoch - 109us/sample
Epoch 18/85
95501/95501 - 10s - loss: 0.0015 - val_loss: 0.0032 - 10s/epoch - 110us/sample
Epoch 19/85
95501/95501 - 10s - loss: 0.0019 - val_loss: 0.0013 - 10s/epoch - 110us/sample
Epoch 20/85
95501/95501 - 10s - loss: 0.0017 - val_loss: 0.0030 - 10s/epoch - 109us/sample
Epoch 21/85
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0013 - 11s/epoch - 111us/sample
Epoch 22/85
95501/95501 - 10s - loss: 0.0014 - val_loss: 0.0035 - 10s/epoch - 110us/sample
Epoch 23/85
95501/95501 - 10s - loss: 0.0021 - val_loss: 0.0014 - 10s/epoch - 110us/sample
Epoch 24/85
95501/95501 - 10s - loss: 0.0014 - val_loss: 0.0012 - 10s/epoch - 109us/sample
Epoch 25/85
95501/95501 - 10s - loss: 0.0013 - val_loss: 0.0014 - 10s/epoch - 109us/sample
Epoch 26/85
95501/95501 - 10s - loss: 0.0014 - val_loss: 0.0012 - 10s/epoch - 109us/sample
Epoch 27/85
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 110us/sample
Epoch 28/85
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 110us/sample
Epoch 29/85
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 111us/sample
Epoch 30/85
95501/95501 - 10s - loss: 0.0012 - val_loss: 0.0011 - 10s/epoch - 110us/sample
Epoch 31/85
95501/95501 - 10s - loss: 0.0011 - val_loss: 9.9020e-04 - 10s/epoch - 109us/sample
Epoch 32/85
95501/95501 - 10s - loss: 0.0012 - val_loss: 9.8136e-04 - 10s/epoch - 109us/sample
Epoch 33/85
95501/95501 - 10s - loss: 0.0011 - val_loss: 9.5269e-04 - 10s/epoch - 110us/sample
Epoch 34/85
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.6793e-04 - 10s/epoch - 109us/sample
Epoch 35/85
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.6497e-04 - 10s/epoch - 109us/sample
Epoch 36/85
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.3491e-04 - 10s/epoch - 110us/sample
Epoch 37/85
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.4126e-04 - 11s/epoch - 110us/sample
Epoch 38/85
95501/95501 - 11s - loss: 9.9936e-04 - val_loss: 9.3934e-04 - 11s/epoch - 110us/sample
Epoch 39/85
95501/95501 - 10s - loss: 0.0010 - val_loss: 8.9948e-04 - 10s/epoch - 109us/sample
Epoch 40/85
95501/95501 - 10s - loss: 0.0010 - val_loss: 0.0010 - 10s/epoch - 110us/sample
Epoch 41/85
95501/95501 - 10s - loss: 0.0010 - val_loss: 0.0013 - 10s/epoch - 110us/sample
Epoch 42/85
95501/95501 - 10s - loss: 0.0012 - val_loss: 9.1188e-04 - 10s/epoch - 109us/sample
Epoch 43/85
95501/95501 - 10s - loss: 0.0011 - val_loss: 9.6329e-04 - 10s/epoch - 110us/sample
Epoch 44/85
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.3651e-04 - 11s/epoch - 111us/sample
Epoch 45/85
95501/95501 - 10s - loss: 0.0010 - val_loss: 9.3594e-04 - 10s/epoch - 109us/sample
Epoch 46/85
95501/95501 - 10s - loss: 9.6975e-04 - val_loss: 8.7295e-04 - 10s/epoch - 110us/sample
Epoch 47/85
95501/95501 - 10s - loss: 9.4550e-04 - val_loss: 8.6374e-04 - 10s/epoch - 109us/sample
Epoch 48/85
95501/95501 - 10s - loss: 9.5284e-04 - val_loss: 0.0010 - 10s/epoch - 109us/sample
Epoch 49/85
95501/95501 - 10s - loss: 0.0010 - val_loss: 8.5782e-04 - 10s/epoch - 110us/sample
Epoch 50/85
95501/95501 - 11s - loss: 9.7193e-04 - val_loss: 8.7110e-04 - 11s/epoch - 112us/sample
Epoch 51/85
95501/95501 - 11s - loss: 9.2213e-04 - val_loss: 8.4331e-04 - 11s/epoch - 115us/sample
Epoch 52/85
95501/95501 - 11s - loss: 9.2648e-04 - val_loss: 8.5102e-04 - 11s/epoch - 115us/sample
Epoch 53/85
95501/95501 - 11s - loss: 9.1334e-04 - val_loss: 8.2258e-04 - 11s/epoch - 114us/sample
Epoch 54/85
95501/95501 - 11s - loss: 9.1352e-04 - val_loss: 8.5680e-04 - 11s/epoch - 114us/sample
Epoch 55/85
95501/95501 - 11s - loss: 9.0957e-04 - val_loss: 8.3468e-04 - 11s/epoch - 114us/sample
Epoch 56/85
95501/95501 - 11s - loss: 9.0263e-04 - val_loss: 8.4576e-04 - 11s/epoch - 114us/sample
Epoch 57/85
95501/95501 - 11s - loss: 8.8801e-04 - val_loss: 8.2910e-04 - 11s/epoch - 114us/sample
Epoch 58/85
95501/95501 - 11s - loss: 9.0299e-04 - val_loss: 9.2656e-04 - 11s/epoch - 114us/sample
Epoch 59/85
95501/95501 - 11s - loss: 9.7615e-04 - val_loss: 8.2148e-04 - 11s/epoch - 115us/sample
Epoch 60/85
95501/95501 - 11s - loss: 8.8497e-04 - val_loss: 8.5896e-04 - 11s/epoch - 115us/sample
Epoch 61/85
95501/95501 - 11s - loss: 9.1754e-04 - val_loss: 8.2743e-04 - 11s/epoch - 114us/sample
Epoch 62/85
95501/95501 - 11s - loss: 8.7791e-04 - val_loss: 8.0799e-04 - 11s/epoch - 114us/sample
Epoch 63/85
95501/95501 - 11s - loss: 9.0659e-04 - val_loss: 8.2982e-04 - 11s/epoch - 114us/sample
Epoch 64/85
95501/95501 - 11s - loss: 8.7964e-04 - val_loss: 8.1051e-04 - 11s/epoch - 114us/sample
Epoch 65/85
95501/95501 - 11s - loss: 8.7607e-04 - val_loss: 8.1661e-04 - 11s/epoch - 114us/sample
Epoch 66/85
95501/95501 - 11s - loss: 8.8890e-04 - val_loss: 8.1514e-04 - 11s/epoch - 114us/sample
Epoch 67/85
95501/95501 - 11s - loss: 8.6372e-04 - val_loss: 7.8775e-04 - 11s/epoch - 115us/sample
Epoch 68/85
95501/95501 - 11s - loss: 8.5660e-04 - val_loss: 8.2246e-04 - 11s/epoch - 114us/sample
Epoch 69/85
95501/95501 - 11s - loss: 8.6650e-04 - val_loss: 7.9333e-04 - 11s/epoch - 114us/sample
Epoch 70/85
95501/95501 - 11s - loss: 8.4720e-04 - val_loss: 7.8951e-04 - 11s/epoch - 114us/sample
Epoch 71/85
95501/95501 - 11s - loss: 8.9606e-04 - val_loss: 8.5692e-04 - 11s/epoch - 114us/sample
Epoch 72/85
95501/95501 - 11s - loss: 9.1919e-04 - val_loss: 7.8852e-04 - 11s/epoch - 114us/sample
Epoch 73/85
95501/95501 - 11s - loss: 8.5075e-04 - val_loss: 7.7850e-04 - 11s/epoch - 114us/sample
Epoch 74/85
95501/95501 - 11s - loss: 8.7294e-04 - val_loss: 7.8371e-04 - 11s/epoch - 115us/sample
Epoch 75/85
95501/95501 - 11s - loss: 8.4767e-04 - val_loss: 7.9850e-04 - 11s/epoch - 114us/sample
Epoch 76/85
95501/95501 - 11s - loss: 8.5495e-04 - val_loss: 0.0010 - 11s/epoch - 114us/sample
Epoch 77/85
95501/95501 - 11s - loss: 9.9591e-04 - val_loss: 7.8573e-04 - 11s/epoch - 114us/sample
Epoch 78/85
95501/95501 - 11s - loss: 8.5979e-04 - val_loss: 8.5733e-04 - 11s/epoch - 114us/sample
Epoch 79/85
95501/95501 - 11s - loss: 9.6616e-04 - val_loss: 8.5068e-04 - 11s/epoch - 114us/sample
Epoch 80/85
95501/95501 - 11s - loss: 9.1532e-04 - val_loss: 7.9017e-04 - 11s/epoch - 114us/sample
Epoch 81/85
95501/95501 - 11s - loss: 8.4490e-04 - val_loss: 7.7169e-04 - 11s/epoch - 114us/sample
Epoch 82/85
95501/95501 - 11s - loss: 8.4165e-04 - val_loss: 8.7728e-04 - 11s/epoch - 115us/sample
Epoch 83/85
95501/95501 - 11s - loss: 9.0414e-04 - val_loss: 7.6982e-04 - 11s/epoch - 115us/sample
Epoch 84/85
95501/95501 - 11s - loss: 8.4909e-04 - val_loss: 8.2610e-04 - 11s/epoch - 114us/sample
Epoch 85/85
95501/95501 - 11s - loss: 8.3947e-04 - val_loss: 7.6807e-04 - 11s/epoch - 114us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0007680717195420011
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 21:43:54.709449: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_10/outputlayer/BiasAdd' id:13362 op device:{requested: '', assigned: ''} def:{{{node decoder_model_10/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_10/outputlayer/MatMul, decoder_model_10/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.005602323504560092
cosine 0.004448933401199453
MAE: 0.011499754
RMSE: 0.021921776
r2: 0.9688250003434112
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'mse', 64, 85, 0.001, 0.6, 758, 0.0008394701743941067, 0.0007680717195420011, 0.005602323504560092, 0.004448933401199453, 0.011499754153192043, 0.021921776235103607, 0.9688250003434112, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6 145 0.0014 64 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2022)         2557830     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_33 (BatchN  (None, 2022)        8088        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_33 (ReLU)                (None, 2022)         0           ['batch_normalization_33[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1533434     ['re_lu_33[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1533434     ['re_lu_33[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4678212     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 10,310,998
Trainable params: 10,301,394
Non-trainable params: 9,604
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-14 21:44:04.093559: W tensorflow/c/c_api.cc:291] Operation '{name:'training_22/Adam/iter/Assign' id:15081 op device:{requested: '', assigned: ''} def:{{{node training_22/Adam/iter/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_INT64, validate_shape=false](training_22/Adam/iter, training_22/Adam/iter/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 21:44:16.308933: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_11/mul' id:14652 op device:{requested: '', assigned: ''} def:{{{node loss_11/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_11/mul/x, loss_11/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 15s - loss: 0.0107 - val_loss: 0.0051 - 15s/epoch - 155us/sample
Epoch 2/145
95501/95501 - 11s - loss: 0.0042 - val_loss: 0.0038 - 11s/epoch - 116us/sample
Epoch 3/145
95501/95501 - 11s - loss: 0.0035 - val_loss: 0.0029 - 11s/epoch - 116us/sample
Epoch 4/145
95501/95501 - 11s - loss: 0.0031 - val_loss: 0.0026 - 11s/epoch - 116us/sample
Epoch 5/145
95501/95501 - 11s - loss: 0.0027 - val_loss: 0.0026 - 11s/epoch - 116us/sample
Epoch 6/145
95501/95501 - 11s - loss: 0.0022 - val_loss: 0.0019 - 11s/epoch - 115us/sample
Epoch 7/145
95501/95501 - 11s - loss: 0.0022 - val_loss: 0.0017 - 11s/epoch - 116us/sample
Epoch 8/145
95501/95501 - 11s - loss: 0.0018 - val_loss: 0.0056 - 11s/epoch - 115us/sample
Epoch 9/145
95501/95501 - 11s - loss: 0.0020 - val_loss: 0.0014 - 11s/epoch - 116us/sample
Epoch 10/145
95501/95501 - 11s - loss: 0.0015 - val_loss: 0.0013 - 11s/epoch - 115us/sample
Epoch 11/145
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0012 - 11s/epoch - 116us/sample
Epoch 12/145
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0014 - 11s/epoch - 116us/sample
Epoch 13/145
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0011 - 11s/epoch - 116us/sample
Epoch 14/145
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 116us/sample
Epoch 15/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0016 - 11s/epoch - 115us/sample
Epoch 16/145
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0010 - 11s/epoch - 116us/sample
Epoch 17/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 115us/sample
Epoch 18/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.6211e-04 - 11s/epoch - 116us/sample
Epoch 19/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.4426e-04 - 11s/epoch - 116us/sample
Epoch 20/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.2189e-04 - 11s/epoch - 117us/sample
Epoch 21/145
95501/95501 - 11s - loss: 9.8501e-04 - val_loss: 9.0477e-04 - 11s/epoch - 116us/sample
Epoch 22/145
95501/95501 - 11s - loss: 9.7334e-04 - val_loss: 0.0012 - 11s/epoch - 115us/sample
Epoch 23/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.1964e-04 - 11s/epoch - 115us/sample
Epoch 24/145
95501/95501 - 11s - loss: 9.5501e-04 - val_loss: 9.1418e-04 - 11s/epoch - 115us/sample
Epoch 25/145
95501/95501 - 11s - loss: 9.8161e-04 - val_loss: 8.7421e-04 - 11s/epoch - 115us/sample
Epoch 26/145
95501/95501 - 11s - loss: 9.3419e-04 - val_loss: 8.7000e-04 - 11s/epoch - 116us/sample
Epoch 27/145
95501/95501 - 11s - loss: 9.1735e-04 - val_loss: 8.6152e-04 - 11s/epoch - 116us/sample
Epoch 28/145
95501/95501 - 11s - loss: 9.0504e-04 - val_loss: 8.3873e-04 - 11s/epoch - 116us/sample
Epoch 29/145
95501/95501 - 11s - loss: 8.9042e-04 - val_loss: 8.4060e-04 - 11s/epoch - 115us/sample
Epoch 30/145
95501/95501 - 11s - loss: 8.8703e-04 - val_loss: 8.1153e-04 - 11s/epoch - 116us/sample
Epoch 31/145
95501/95501 - 11s - loss: 8.8363e-04 - val_loss: 9.9356e-04 - 11s/epoch - 115us/sample
Epoch 32/145
95501/95501 - 11s - loss: 9.4282e-04 - val_loss: 0.0012 - 11s/epoch - 116us/sample
Epoch 33/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 8.1321e-04 - 11s/epoch - 115us/sample
Epoch 34/145
95501/95501 - 11s - loss: 8.6899e-04 - val_loss: 7.9890e-04 - 11s/epoch - 116us/sample
Epoch 35/145
95501/95501 - 11s - loss: 8.7790e-04 - val_loss: 8.0756e-04 - 11s/epoch - 116us/sample
Epoch 36/145
95501/95501 - 11s - loss: 8.7162e-04 - val_loss: 0.0012 - 11s/epoch - 115us/sample
Epoch 37/145
95501/95501 - 11s - loss: 9.7796e-04 - val_loss: 8.9253e-04 - 11s/epoch - 116us/sample
Epoch 38/145
95501/95501 - 11s - loss: 9.1581e-04 - val_loss: 8.2126e-04 - 11s/epoch - 115us/sample
Epoch 39/145
95501/95501 - 11s - loss: 8.4601e-04 - val_loss: 8.2677e-04 - 11s/epoch - 116us/sample
Epoch 40/145
95501/95501 - 11s - loss: 8.8430e-04 - val_loss: 7.8770e-04 - 11s/epoch - 115us/sample
Epoch 41/145
95501/95501 - 11s - loss: 8.4167e-04 - val_loss: 8.0129e-04 - 11s/epoch - 116us/sample
Epoch 42/145
95501/95501 - 11s - loss: 8.3255e-04 - val_loss: 7.9611e-04 - 11s/epoch - 117us/sample
Epoch 43/145
95501/95501 - 11s - loss: 8.4577e-04 - val_loss: 7.9423e-04 - 11s/epoch - 116us/sample
Epoch 44/145
95501/95501 - 11s - loss: 8.2040e-04 - val_loss: 7.9853e-04 - 11s/epoch - 116us/sample
Epoch 45/145
95501/95501 - 11s - loss: 8.5086e-04 - val_loss: 7.8959e-04 - 11s/epoch - 115us/sample
Epoch 46/145
95501/95501 - 11s - loss: 8.2349e-04 - val_loss: 7.6148e-04 - 11s/epoch - 116us/sample
Epoch 47/145
95501/95501 - 11s - loss: 8.6217e-04 - val_loss: 7.6697e-04 - 11s/epoch - 115us/sample
Epoch 48/145
95501/95501 - 11s - loss: 8.0980e-04 - val_loss: 8.2093e-04 - 11s/epoch - 116us/sample
Epoch 49/145
95501/95501 - 11s - loss: 8.5130e-04 - val_loss: 7.5193e-04 - 11s/epoch - 116us/sample
Epoch 50/145
95501/95501 - 11s - loss: 8.0009e-04 - val_loss: 9.3110e-04 - 11s/epoch - 116us/sample
Epoch 51/145
95501/95501 - 11s - loss: 9.2337e-04 - val_loss: 7.7712e-04 - 11s/epoch - 116us/sample
Epoch 52/145
95501/95501 - 11s - loss: 8.1650e-04 - val_loss: 7.4048e-04 - 11s/epoch - 115us/sample
Epoch 53/145
95501/95501 - 11s - loss: 7.9598e-04 - val_loss: 7.4069e-04 - 11s/epoch - 116us/sample
Epoch 54/145
95501/95501 - 11s - loss: 7.8563e-04 - val_loss: 7.5035e-04 - 11s/epoch - 115us/sample
Epoch 55/145
95501/95501 - 11s - loss: 7.8629e-04 - val_loss: 7.3485e-04 - 11s/epoch - 115us/sample
Epoch 56/145
95501/95501 - 11s - loss: 7.7573e-04 - val_loss: 7.3760e-04 - 11s/epoch - 116us/sample
Epoch 57/145
95501/95501 - 11s - loss: 7.7808e-04 - val_loss: 7.3432e-04 - 11s/epoch - 116us/sample
Epoch 58/145
95501/95501 - 11s - loss: 7.7611e-04 - val_loss: 7.4233e-04 - 11s/epoch - 116us/sample
Epoch 59/145
95501/95501 - 11s - loss: 7.8699e-04 - val_loss: 8.0621e-04 - 11s/epoch - 115us/sample
Epoch 60/145
95501/95501 - 11s - loss: 8.3227e-04 - val_loss: 7.4823e-04 - 11s/epoch - 116us/sample
Epoch 61/145
95501/95501 - 11s - loss: 7.6780e-04 - val_loss: 7.2178e-04 - 11s/epoch - 115us/sample
Epoch 62/145
95501/95501 - 11s - loss: 7.7977e-04 - val_loss: 7.8399e-04 - 11s/epoch - 116us/sample
Epoch 63/145
95501/95501 - 11s - loss: 7.9772e-04 - val_loss: 7.8051e-04 - 11s/epoch - 116us/sample
Epoch 64/145
95501/95501 - 11s - loss: 8.1111e-04 - val_loss: 8.8026e-04 - 11s/epoch - 116us/sample
Epoch 65/145
95501/95501 - 11s - loss: 8.8865e-04 - val_loss: 7.7823e-04 - 11s/epoch - 116us/sample
Epoch 66/145
95501/95501 - 11s - loss: 8.1714e-04 - val_loss: 7.2412e-04 - 11s/epoch - 116us/sample
Epoch 67/145
95501/95501 - 11s - loss: 7.6798e-04 - val_loss: 7.1530e-04 - 11s/epoch - 115us/sample
Epoch 68/145
95501/95501 - 11s - loss: 7.7147e-04 - val_loss: 7.1183e-04 - 11s/epoch - 115us/sample
Epoch 69/145
95501/95501 - 11s - loss: 7.6780e-04 - val_loss: 8.0092e-04 - 11s/epoch - 116us/sample
Epoch 70/145
95501/95501 - 11s - loss: 8.2158e-04 - val_loss: 7.2372e-04 - 11s/epoch - 116us/sample
Epoch 71/145
95501/95501 - 11s - loss: 7.6022e-04 - val_loss: 7.5968e-04 - 11s/epoch - 115us/sample
Epoch 72/145
95501/95501 - 11s - loss: 7.5642e-04 - val_loss: 7.7524e-04 - 11s/epoch - 117us/sample
Epoch 73/145
95501/95501 - 11s - loss: 8.0765e-04 - val_loss: 8.2468e-04 - 11s/epoch - 116us/sample
Epoch 74/145
95501/95501 - 11s - loss: 8.7072e-04 - val_loss: 7.2987e-04 - 11s/epoch - 116us/sample
Epoch 75/145
95501/95501 - 11s - loss: 7.6361e-04 - val_loss: 7.0329e-04 - 11s/epoch - 115us/sample
Epoch 76/145
95501/95501 - 11s - loss: 7.5671e-04 - val_loss: 7.4417e-04 - 11s/epoch - 116us/sample
Epoch 77/145
95501/95501 - 11s - loss: 7.8209e-04 - val_loss: 7.7375e-04 - 11s/epoch - 115us/sample
Epoch 78/145
95501/95501 - 11s - loss: 8.0037e-04 - val_loss: 7.2246e-04 - 11s/epoch - 115us/sample
Epoch 79/145
95501/95501 - 11s - loss: 7.5847e-04 - val_loss: 6.9888e-04 - 11s/epoch - 116us/sample
Epoch 80/145
95501/95501 - 11s - loss: 7.4369e-04 - val_loss: 7.0855e-04 - 11s/epoch - 116us/sample
Epoch 81/145
95501/95501 - 11s - loss: 7.5717e-04 - val_loss: 7.5427e-04 - 11s/epoch - 116us/sample
Epoch 82/145
95501/95501 - 11s - loss: 7.8914e-04 - val_loss: 6.9926e-04 - 11s/epoch - 115us/sample
Epoch 83/145
95501/95501 - 11s - loss: 7.4313e-04 - val_loss: 8.1242e-04 - 11s/epoch - 115us/sample
Epoch 84/145
95501/95501 - 11s - loss: 8.2808e-04 - val_loss: 7.5623e-04 - 11s/epoch - 115us/sample
Epoch 85/145
95501/95501 - 11s - loss: 7.6823e-04 - val_loss: 6.9213e-04 - 11s/epoch - 115us/sample
Epoch 86/145
95501/95501 - 11s - loss: 7.3577e-04 - val_loss: 6.8919e-04 - 11s/epoch - 116us/sample
Epoch 87/145
95501/95501 - 11s - loss: 7.3533e-04 - val_loss: 7.1340e-04 - 11s/epoch - 116us/sample
Epoch 88/145
95501/95501 - 11s - loss: 7.3774e-04 - val_loss: 7.9781e-04 - 11s/epoch - 116us/sample
Epoch 89/145
95501/95501 - 11s - loss: 8.1431e-04 - val_loss: 7.6375e-04 - 11s/epoch - 115us/sample
Epoch 90/145
95501/95501 - 11s - loss: 8.0503e-04 - val_loss: 6.9732e-04 - 11s/epoch - 115us/sample
Epoch 91/145
95501/95501 - 11s - loss: 7.3898e-04 - val_loss: 6.8897e-04 - 11s/epoch - 115us/sample
Epoch 92/145
95501/95501 - 11s - loss: 7.2887e-04 - val_loss: 6.8659e-04 - 11s/epoch - 115us/sample
Epoch 93/145
95501/95501 - 11s - loss: 7.3550e-04 - val_loss: 6.9352e-04 - 11s/epoch - 116us/sample
Epoch 94/145
95501/95501 - 11s - loss: 7.2741e-04 - val_loss: 7.8340e-04 - 11s/epoch - 115us/sample
Epoch 95/145
95501/95501 - 11s - loss: 7.8754e-04 - val_loss: 6.9082e-04 - 11s/epoch - 116us/sample
Epoch 96/145
95501/95501 - 11s - loss: 7.2534e-04 - val_loss: 6.9455e-04 - 11s/epoch - 116us/sample
Epoch 97/145
95501/95501 - 11s - loss: 7.2537e-04 - val_loss: 8.0516e-04 - 11s/epoch - 116us/sample
Epoch 98/145
95501/95501 - 11s - loss: 7.9442e-04 - val_loss: 7.1865e-04 - 11s/epoch - 115us/sample
Epoch 99/145
95501/95501 - 11s - loss: 7.6317e-04 - val_loss: 6.9682e-04 - 11s/epoch - 115us/sample
Epoch 100/145
95501/95501 - 11s - loss: 7.3383e-04 - val_loss: 6.9703e-04 - 11s/epoch - 116us/sample
Epoch 101/145
95501/95501 - 11s - loss: 7.2886e-04 - val_loss: 6.8442e-04 - 11s/epoch - 115us/sample
Epoch 102/145
95501/95501 - 11s - loss: 7.1881e-04 - val_loss: 6.8373e-04 - 11s/epoch - 116us/sample
Epoch 103/145
95501/95501 - 11s - loss: 7.1723e-04 - val_loss: 6.8245e-04 - 11s/epoch - 116us/sample
Epoch 104/145
95501/95501 - 11s - loss: 7.1241e-04 - val_loss: 6.7643e-04 - 11s/epoch - 116us/sample
Epoch 105/145
95501/95501 - 11s - loss: 7.1359e-04 - val_loss: 6.7364e-04 - 11s/epoch - 115us/sample
Epoch 106/145
95501/95501 - 11s - loss: 7.1354e-04 - val_loss: 6.7310e-04 - 11s/epoch - 116us/sample
Epoch 107/145
95501/95501 - 11s - loss: 7.1286e-04 - val_loss: 6.7837e-04 - 11s/epoch - 116us/sample
Epoch 108/145
95501/95501 - 11s - loss: 7.0857e-04 - val_loss: 6.7958e-04 - 11s/epoch - 115us/sample
Epoch 109/145
95501/95501 - 11s - loss: 7.2980e-04 - val_loss: 0.0024 - 11s/epoch - 116us/sample
Epoch 110/145
95501/95501 - 11s - loss: 8.6577e-04 - val_loss: 7.7570e-04 - 11s/epoch - 116us/sample
Epoch 111/145
95501/95501 - 11s - loss: 7.9364e-04 - val_loss: 6.8643e-04 - 11s/epoch - 116us/sample
Epoch 112/145
95501/95501 - 11s - loss: 7.2420e-04 - val_loss: 7.1820e-04 - 11s/epoch - 115us/sample
Epoch 113/145
95501/95501 - 11s - loss: 7.5950e-04 - val_loss: 6.7682e-04 - 11s/epoch - 116us/sample
Epoch 114/145
95501/95501 - 11s - loss: 7.1482e-04 - val_loss: 6.8599e-04 - 11s/epoch - 115us/sample
Epoch 115/145
95501/95501 - 11s - loss: 7.0928e-04 - val_loss: 6.7505e-04 - 11s/epoch - 115us/sample
Epoch 116/145
95501/95501 - 11s - loss: 7.1008e-04 - val_loss: 6.7864e-04 - 11s/epoch - 115us/sample
Epoch 117/145
95501/95501 - 11s - loss: 7.0430e-04 - val_loss: 6.8743e-04 - 11s/epoch - 116us/sample
Epoch 118/145
95501/95501 - 11s - loss: 7.0062e-04 - val_loss: 6.7657e-04 - 11s/epoch - 116us/sample
Epoch 119/145
95501/95501 - 11s - loss: 7.3245e-04 - val_loss: 6.8546e-04 - 11s/epoch - 115us/sample
Epoch 120/145
95501/95501 - 11s - loss: 7.2338e-04 - val_loss: 7.1082e-04 - 11s/epoch - 115us/sample
Epoch 121/145
95501/95501 - 11s - loss: 7.2266e-04 - val_loss: 6.6294e-04 - 11s/epoch - 116us/sample
Epoch 122/145
95501/95501 - 11s - loss: 7.0505e-04 - val_loss: 6.8081e-04 - 11s/epoch - 115us/sample
Epoch 123/145
95501/95501 - 11s - loss: 7.1305e-04 - val_loss: 6.6710e-04 - 11s/epoch - 116us/sample
Epoch 124/145
95501/95501 - 11s - loss: 7.0037e-04 - val_loss: 7.8276e-04 - 11s/epoch - 115us/sample
Epoch 125/145
95501/95501 - 11s - loss: 7.8758e-04 - val_loss: 6.7357e-04 - 11s/epoch - 116us/sample
Epoch 126/145
95501/95501 - 11s - loss: 7.0370e-04 - val_loss: 6.6495e-04 - 11s/epoch - 116us/sample
Epoch 127/145
95501/95501 - 11s - loss: 6.9883e-04 - val_loss: 6.6036e-04 - 11s/epoch - 116us/sample
Epoch 128/145
95501/95501 - 11s - loss: 6.9672e-04 - val_loss: 6.5913e-04 - 11s/epoch - 115us/sample
Epoch 129/145
95501/95501 - 11s - loss: 6.9733e-04 - val_loss: 6.9333e-04 - 11s/epoch - 115us/sample
Epoch 130/145
95501/95501 - 11s - loss: 7.1709e-04 - val_loss: 7.0313e-04 - 11s/epoch - 116us/sample
Epoch 131/145
95501/95501 - 11s - loss: 7.4231e-04 - val_loss: 6.8556e-04 - 11s/epoch - 115us/sample
Epoch 132/145
95501/95501 - 11s - loss: 7.0574e-04 - val_loss: 6.6140e-04 - 11s/epoch - 116us/sample
Epoch 133/145
95501/95501 - 11s - loss: 7.0129e-04 - val_loss: 6.6325e-04 - 11s/epoch - 116us/sample
Epoch 134/145
95501/95501 - 11s - loss: 6.9254e-04 - val_loss: 6.8113e-04 - 11s/epoch - 116us/sample
Epoch 135/145
95501/95501 - 11s - loss: 7.0609e-04 - val_loss: 6.7196e-04 - 11s/epoch - 115us/sample
Epoch 136/145
95501/95501 - 11s - loss: 7.0545e-04 - val_loss: 7.5142e-04 - 11s/epoch - 115us/sample
Epoch 137/145
95501/95501 - 11s - loss: 7.6029e-04 - val_loss: 6.6043e-04 - 11s/epoch - 115us/sample
Epoch 138/145
95501/95501 - 11s - loss: 6.9612e-04 - val_loss: 6.6108e-04 - 11s/epoch - 115us/sample
Epoch 139/145
95501/95501 - 11s - loss: 7.1461e-04 - val_loss: 8.6199e-04 - 11s/epoch - 116us/sample
Epoch 140/145
95501/95501 - 11s - loss: 8.4594e-04 - val_loss: 6.8252e-04 - 11s/epoch - 115us/sample
Epoch 141/145
95501/95501 - 11s - loss: 7.0670e-04 - val_loss: 6.5795e-04 - 11s/epoch - 116us/sample
Epoch 142/145
95501/95501 - 11s - loss: 7.0026e-04 - val_loss: 6.5897e-04 - 11s/epoch - 116us/sample
Epoch 143/145
95501/95501 - 11s - loss: 6.9194e-04 - val_loss: 6.5667e-04 - 11s/epoch - 115us/sample
Epoch 144/145
95501/95501 - 11s - loss: 6.9535e-04 - val_loss: 6.6403e-04 - 11s/epoch - 115us/sample
Epoch 145/145
95501/95501 - 11s - loss: 6.9719e-04 - val_loss: 6.5875e-04 - 11s/epoch - 115us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0006587521067175971
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 22:10:47.763130: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_11/outputlayer/BiasAdd' id:14623 op device:{requested: '', assigned: ''} def:{{{node decoder_model_11/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_11/outputlayer/MatMul, decoder_model_11/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.004337600889045968
cosine 0.003418675294202811
MAE: 0.0101238545
RMSE: 0.019393636
r2: 0.975601242915839
RMSE zero-vector: 0.23411466903540806
['1.6custom_VAE', 'mse', 64, 145, 0.0014, 0.6, 758, 0.0006971895760963148, 0.0006587521067175971, 0.004337600889045968, 0.003418675294202811, 0.010123854503035545, 0.019393635913729668, 0.975601242915839, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 45 0.0005 64 2] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_36 (BatchN  (None, 3160)        12640       ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_36 (ReLU)                (None, 3160)         0           ['batch_normalization_36[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2396038     ['re_lu_36[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2396038     ['re_lu_36[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         6984938     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 15,787,054
Trainable params: 15,772,898
Non-trainable params: 14,156
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/45
2023-02-14 22:10:57.667962: W tensorflow/c/c_api.cc:291] Operation '{name:'training_24/Adam/dense_enc0_12/kernel/v/Assign' id:16515 op device:{requested: '', assigned: ''} def:{{{node training_24/Adam/dense_enc0_12/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_24/Adam/dense_enc0_12/kernel/v, training_24/Adam/dense_enc0_12/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 22:11:10.738814: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_12/mul' id:15923 op device:{requested: '', assigned: ''} def:{{{node loss_12/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_12/mul/x, loss_12/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 16s - loss: 0.0130 - val_loss: 0.0045 - 16s/epoch - 167us/sample
Epoch 2/45
95501/95501 - 12s - loss: 0.0040 - val_loss: 0.0035 - 12s/epoch - 121us/sample
Epoch 3/45
95501/95501 - 12s - loss: 0.0028 - val_loss: 0.0026 - 12s/epoch - 121us/sample
Epoch 4/45
95501/95501 - 11s - loss: 0.0022 - val_loss: 0.0025 - 11s/epoch - 120us/sample
Epoch 5/45
95501/95501 - 11s - loss: 0.0019 - val_loss: 0.0015 - 11s/epoch - 120us/sample
Epoch 6/45
95501/95501 - 12s - loss: 0.0016 - val_loss: 0.0014 - 12s/epoch - 121us/sample
Epoch 7/45
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0018 - 11s/epoch - 120us/sample
Epoch 8/45
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0011 - 11s/epoch - 120us/sample
Epoch 9/45
95501/95501 - 11s - loss: 0.0031 - val_loss: 0.0017 - 11s/epoch - 120us/sample
Epoch 10/45
95501/95501 - 12s - loss: 0.0011 - val_loss: 0.0011 - 12s/epoch - 121us/sample
Epoch 11/45
95501/95501 - 12s - loss: 9.7474e-04 - val_loss: 8.9806e-04 - 12s/epoch - 121us/sample
Epoch 12/45
95501/95501 - 11s - loss: 9.4035e-04 - val_loss: 0.0012 - 11s/epoch - 120us/sample
Epoch 13/45
95501/95501 - 11s - loss: 9.2924e-04 - val_loss: 0.0017 - 11s/epoch - 120us/sample
Epoch 14/45
95501/95501 - 11s - loss: 8.8808e-04 - val_loss: 7.6943e-04 - 11s/epoch - 120us/sample
Epoch 15/45
95501/95501 - 11s - loss: 8.1169e-04 - val_loss: 8.3135e-04 - 11s/epoch - 120us/sample
Epoch 16/45
95501/95501 - 12s - loss: 7.6431e-04 - val_loss: 8.6727e-04 - 12s/epoch - 121us/sample
Epoch 17/45
95501/95501 - 12s - loss: 9.7610e-04 - val_loss: 7.8179e-04 - 12s/epoch - 121us/sample
Epoch 18/45
95501/95501 - 11s - loss: 7.2708e-04 - val_loss: 9.2301e-04 - 11s/epoch - 120us/sample
Epoch 19/45
95501/95501 - 11s - loss: 7.4617e-04 - val_loss: 6.6629e-04 - 11s/epoch - 120us/sample
Epoch 20/45
95501/95501 - 12s - loss: 7.1888e-04 - val_loss: 6.9020e-04 - 12s/epoch - 120us/sample
Epoch 21/45
95501/95501 - 11s - loss: 6.6723e-04 - val_loss: 6.1630e-04 - 11s/epoch - 120us/sample
Epoch 22/45
95501/95501 - 11s - loss: 6.4363e-04 - val_loss: 6.1908e-04 - 11s/epoch - 120us/sample
Epoch 23/45
95501/95501 - 11s - loss: 6.3811e-04 - val_loss: 0.0038 - 11s/epoch - 120us/sample
Epoch 24/45
95501/95501 - 12s - loss: 8.7908e-04 - val_loss: 6.6996e-04 - 12s/epoch - 121us/sample
Epoch 25/45
95501/95501 - 12s - loss: 6.4492e-04 - val_loss: 5.9156e-04 - 12s/epoch - 121us/sample
Epoch 26/45
95501/95501 - 11s - loss: 6.0314e-04 - val_loss: 8.6588e-04 - 11s/epoch - 120us/sample
Epoch 27/45
95501/95501 - 11s - loss: 6.5532e-04 - val_loss: 6.8912e-04 - 11s/epoch - 120us/sample
Epoch 28/45
95501/95501 - 11s - loss: 6.3157e-04 - val_loss: 5.5048e-04 - 11s/epoch - 120us/sample
Epoch 29/45
95501/95501 - 11s - loss: 5.7406e-04 - val_loss: 5.7827e-04 - 11s/epoch - 120us/sample
Epoch 30/45
95501/95501 - 12s - loss: 5.8043e-04 - val_loss: 5.4099e-04 - 12s/epoch - 120us/sample
Epoch 31/45
95501/95501 - 12s - loss: 5.6921e-04 - val_loss: 5.3383e-04 - 12s/epoch - 121us/sample
Epoch 32/45
95501/95501 - 12s - loss: 5.5235e-04 - val_loss: 5.2645e-04 - 12s/epoch - 121us/sample
Epoch 33/45
95501/95501 - 12s - loss: 5.4585e-04 - val_loss: 6.3268e-04 - 12s/epoch - 120us/sample
Epoch 34/45
95501/95501 - 11s - loss: 6.2352e-04 - val_loss: 5.2293e-04 - 11s/epoch - 120us/sample
Epoch 35/45
95501/95501 - 12s - loss: 5.4551e-04 - val_loss: 5.5594e-04 - 12s/epoch - 120us/sample
Epoch 36/45
95501/95501 - 11s - loss: 5.6457e-04 - val_loss: 5.1652e-04 - 11s/epoch - 120us/sample
Epoch 37/45
95501/95501 - 12s - loss: 5.3134e-04 - val_loss: 5.1047e-04 - 12s/epoch - 120us/sample
Epoch 38/45
95501/95501 - 12s - loss: 5.2815e-04 - val_loss: 5.1301e-04 - 12s/epoch - 120us/sample
Epoch 39/45
95501/95501 - 12s - loss: 5.3735e-04 - val_loss: 5.1438e-04 - 12s/epoch - 121us/sample
Epoch 40/45
95501/95501 - 12s - loss: 5.1813e-04 - val_loss: 4.9944e-04 - 12s/epoch - 121us/sample
Epoch 41/45
95501/95501 - 11s - loss: 5.1428e-04 - val_loss: 4.9529e-04 - 11s/epoch - 120us/sample
Epoch 42/45
95501/95501 - 11s - loss: 5.1255e-04 - val_loss: 4.9998e-04 - 11s/epoch - 120us/sample
Epoch 43/45
95501/95501 - 12s - loss: 5.1750e-04 - val_loss: 4.9128e-04 - 12s/epoch - 120us/sample
Epoch 44/45
95501/95501 - 11s - loss: 5.0478e-04 - val_loss: 4.8369e-04 - 11s/epoch - 120us/sample
Epoch 45/45
95501/95501 - 11s - loss: 5.0313e-04 - val_loss: 4.9498e-04 - 11s/epoch - 120us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0004949756806099888
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 22:19:38.810355: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_12/outputlayer/BiasAdd' id:15887 op device:{requested: '', assigned: ''} def:{{{node decoder_model_12/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_12/outputlayer/MatMul, decoder_model_12/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.007094250713210712
cosine 0.005609564710642947
MAE: 0.013196204
RMSE: 0.02479
r2: 0.9601334587940404
RMSE zero-vector: 0.23411466903540806
['2.5custom_VAE', 'logcosh', 64, 45, 0.0005, 0.6, 758, 0.0005031255367389337, 0.0004949756806099888, 0.007094250713210712, 0.005609564710642947, 0.013196203857660294, 0.024790000170469284, 0.9601334587940404, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 90 0.001 8 0] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_39 (BatchN  (None, 1896)        7584        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_39 (ReLU)                (None, 1896)         0           ['batch_normalization_39[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu_39[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu_39[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/90
2023-02-14 22:19:49.492618: W tensorflow/c/c_api.cc:291] Operation '{name:'training_26/Adam/outputlayer_13/bias/m/Assign' id:17851 op device:{requested: '', assigned: ''} def:{{{node training_26/Adam/outputlayer_13/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_26/Adam/outputlayer_13/bias/m, training_26/Adam/outputlayer_13/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 22:20:54.125176: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_13/mul' id:17226 op device:{requested: '', assigned: ''} def:{{{node loss_13/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_13/mul/x, loss_13/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 69s - loss: 1.4489 - val_loss: 1.4397 - 69s/epoch - 727us/sample
Epoch 2/90
95501/95501 - 65s - loss: 1.4461 - val_loss: 1.4361 - 65s/epoch - 677us/sample
Epoch 3/90
95501/95501 - 64s - loss: 1.4325 - val_loss: 1.4305 - 64s/epoch - 675us/sample
Epoch 4/90
95501/95501 - 64s - loss: 1.4246 - val_loss: 1.4250 - 64s/epoch - 670us/sample
Epoch 5/90
95501/95501 - 64s - loss: 1.4240 - val_loss: 1.4278 - 64s/epoch - 672us/sample
Epoch 6/90
95501/95501 - 64s - loss: 1.4234 - val_loss: 1.4305 - 64s/epoch - 672us/sample
Epoch 7/90
95501/95501 - 64s - loss: 1.4226 - val_loss: 1.4546 - 64s/epoch - 673us/sample
Epoch 8/90
95501/95501 - 64s - loss: 1.4234 - val_loss: 1.4243 - 64s/epoch - 673us/sample
Epoch 9/90
95501/95501 - 64s - loss: 1.4203 - val_loss: 1.4224 - 64s/epoch - 671us/sample
Epoch 10/90
95501/95501 - 64s - loss: 1.4191 - val_loss: 1.4288 - 64s/epoch - 673us/sample
Epoch 11/90
95501/95501 - 64s - loss: 1.4228 - val_loss: 1.4248 - 64s/epoch - 671us/sample
Epoch 12/90
95501/95501 - 64s - loss: 1.4228 - val_loss: 1.4634 - 64s/epoch - 674us/sample
Epoch 13/90
95501/95501 - 64s - loss: 1.4237 - val_loss: 1.4312 - 64s/epoch - 673us/sample
Epoch 14/90
95501/95501 - 64s - loss: 1.4252 - val_loss: 1.4899 - 64s/epoch - 670us/sample
Epoch 15/90
95501/95501 - 64s - loss: 1.4230 - val_loss: 1.4239 - 64s/epoch - 672us/sample
Epoch 16/90
95501/95501 - 64s - loss: 1.4229 - val_loss: 1.4261 - 64s/epoch - 671us/sample
Epoch 17/90
95501/95501 - 64s - loss: 1.4222 - val_loss: 1.4286 - 64s/epoch - 671us/sample
Epoch 18/90
95501/95501 - 64s - loss: 1.4221 - val_loss: 1.4219 - 64s/epoch - 673us/sample
Epoch 19/90
95501/95501 - 64s - loss: 1.4213 - val_loss: 1.4244 - 64s/epoch - 673us/sample
Epoch 20/90
95501/95501 - 64s - loss: 1.4219 - val_loss: 1.4322 - 64s/epoch - 671us/sample
Epoch 21/90
95501/95501 - 64s - loss: 1.4220 - val_loss: 1.4231 - 64s/epoch - 672us/sample
Epoch 22/90
95501/95501 - 64s - loss: 1.4207 - val_loss: 1.4210 - 64s/epoch - 672us/sample
Epoch 23/90
95501/95501 - 64s - loss: 1.4210 - val_loss: 1.4656 - 64s/epoch - 672us/sample
Epoch 24/90
95501/95501 - 64s - loss: 1.4202 - val_loss: 1.4207 - 64s/epoch - 672us/sample
Epoch 25/90
95501/95501 - 64s - loss: 1.4224 - val_loss: 1.4251 - 64s/epoch - 672us/sample
Epoch 26/90
95501/95501 - 64s - loss: 1.4248 - val_loss: 1.4249 - 64s/epoch - 671us/sample
Epoch 27/90
95501/95501 - 64s - loss: 1.4238 - val_loss: 1.4252 - 64s/epoch - 674us/sample
Epoch 28/90
95501/95501 - 63s - loss: 1.4241 - val_loss: 1.4247 - 63s/epoch - 662us/sample
Epoch 29/90
95501/95501 - 64s - loss: 1.4241 - val_loss: 1.4246 - 64s/epoch - 669us/sample
Epoch 30/90
95501/95501 - 64s - loss: 1.4238 - val_loss: 1.4249 - 64s/epoch - 672us/sample
Epoch 31/90
95501/95501 - 64s - loss: 1.4238 - val_loss: 1.4251 - 64s/epoch - 670us/sample
Epoch 32/90
95501/95501 - 64s - loss: 1.4250 - val_loss: 1.4299 - 64s/epoch - 671us/sample
Epoch 33/90
95501/95501 - 64s - loss: 1.4247 - val_loss: 1.4241 - 64s/epoch - 671us/sample
Epoch 34/90
95501/95501 - 64s - loss: 1.4227 - val_loss: 1.4242 - 64s/epoch - 670us/sample
Epoch 35/90
95501/95501 - 64s - loss: 1.4233 - val_loss: 1.4236 - 64s/epoch - 671us/sample
Epoch 36/90
95501/95501 - 64s - loss: 1.4253 - val_loss: 1.4289 - 64s/epoch - 672us/sample
Epoch 37/90
95501/95501 - 64s - loss: 1.4241 - val_loss: 1.4232 - 64s/epoch - 669us/sample
Epoch 38/90
95501/95501 - 64s - loss: 1.4225 - val_loss: 1.4229 - 64s/epoch - 672us/sample
Epoch 39/90
95501/95501 - 64s - loss: 1.4224 - val_loss: 1.4235 - 64s/epoch - 671us/sample
Epoch 40/90
95501/95501 - 64s - loss: 1.4223 - val_loss: 1.4225 - 64s/epoch - 671us/sample
Epoch 41/90
95501/95501 - 64s - loss: 1.4230 - val_loss: 1.4236 - 64s/epoch - 671us/sample
Epoch 42/90
95501/95501 - 64s - loss: 1.4228 - val_loss: 1.4237 - 64s/epoch - 672us/sample
Epoch 43/90
95501/95501 - 64s - loss: 1.4228 - val_loss: 1.4237 - 64s/epoch - 672us/sample
Epoch 44/90
95501/95501 - 64s - loss: 1.4224 - val_loss: 1.4231 - 64s/epoch - 673us/sample
Epoch 45/90
95501/95501 - 64s - loss: 1.4230 - val_loss: 1.4257 - 64s/epoch - 671us/sample
Epoch 46/90
95501/95501 - 64s - loss: 1.4235 - val_loss: 1.4230 - 64s/epoch - 671us/sample
Epoch 47/90
95501/95501 - 64s - loss: 1.4219 - val_loss: 1.4225 - 64s/epoch - 671us/sample
Epoch 48/90
95501/95501 - 64s - loss: 1.4217 - val_loss: 1.4230 - 64s/epoch - 672us/sample
Epoch 49/90
95501/95501 - 64s - loss: 1.4218 - val_loss: 1.4230 - 64s/epoch - 669us/sample
Epoch 50/90
95501/95501 - 64s - loss: 1.4223 - val_loss: 1.4249 - 64s/epoch - 671us/sample
Epoch 51/90
95501/95501 - 64s - loss: 1.4218 - val_loss: 1.4224 - 64s/epoch - 672us/sample
Epoch 52/90
95501/95501 - 64s - loss: 1.4219 - val_loss: 1.4251 - 64s/epoch - 672us/sample
Epoch 53/90
95501/95501 - 64s - loss: 1.4218 - val_loss: 1.4224 - 64s/epoch - 670us/sample
Epoch 54/90
95501/95501 - 59s - loss: 1.4212 - val_loss: 1.4223 - 59s/epoch - 621us/sample
Epoch 55/90
95501/95501 - 58s - loss: 1.4212 - val_loss: 1.4222 - 58s/epoch - 609us/sample
Epoch 56/90
95501/95501 - 58s - loss: 1.4218 - val_loss: 1.4233 - 58s/epoch - 608us/sample
Epoch 57/90
95501/95501 - 58s - loss: 1.4214 - val_loss: 1.4228 - 58s/epoch - 608us/sample
Epoch 58/90
95501/95501 - 58s - loss: 1.4212 - val_loss: 1.4226 - 58s/epoch - 607us/sample
Epoch 59/90
95501/95501 - 58s - loss: 1.4212 - val_loss: 1.4218 - 58s/epoch - 608us/sample
Epoch 60/90
95501/95501 - 58s - loss: 1.4209 - val_loss: 1.4216 - 58s/epoch - 609us/sample
Epoch 61/90
95501/95501 - 58s - loss: 1.4209 - val_loss: 1.4235 - 58s/epoch - 608us/sample
Epoch 62/90
95501/95501 - 58s - loss: 1.4212 - val_loss: 1.4220 - 58s/epoch - 609us/sample
Epoch 63/90
95501/95501 - 58s - loss: 1.4214 - val_loss: 1.4218 - 58s/epoch - 608us/sample
Epoch 64/90
95501/95501 - 58s - loss: 1.4210 - val_loss: 1.4221 - 58s/epoch - 609us/sample
Epoch 65/90
95501/95501 - 58s - loss: 1.4206 - val_loss: 1.4217 - 58s/epoch - 609us/sample
Epoch 66/90
95501/95501 - 58s - loss: 1.4205 - val_loss: 1.4215 - 58s/epoch - 607us/sample
Epoch 67/90
95501/95501 - 58s - loss: 1.4204 - val_loss: 1.4216 - 58s/epoch - 607us/sample
Epoch 68/90
95501/95501 - 58s - loss: 1.4205 - val_loss: 1.4216 - 58s/epoch - 609us/sample
Epoch 69/90
95501/95501 - 58s - loss: 1.4204 - val_loss: 1.4215 - 58s/epoch - 608us/sample
Epoch 70/90
95501/95501 - 58s - loss: 1.4202 - val_loss: 1.4214 - 58s/epoch - 607us/sample
Epoch 71/90
95501/95501 - 58s - loss: 1.4203 - val_loss: 1.4216 - 58s/epoch - 608us/sample
Epoch 72/90
95501/95501 - 58s - loss: 1.4205 - val_loss: 1.4216 - 58s/epoch - 608us/sample
Epoch 73/90
95501/95501 - 58s - loss: 1.4204 - val_loss: 1.4216 - 58s/epoch - 608us/sample
Epoch 74/90
95501/95501 - 58s - loss: 1.4205 - val_loss: 1.4224 - 58s/epoch - 608us/sample
Epoch 75/90
95501/95501 - 58s - loss: 1.4210 - val_loss: 1.4218 - 58s/epoch - 608us/sample
Epoch 76/90
95501/95501 - 58s - loss: 1.4205 - val_loss: 1.4215 - 58s/epoch - 610us/sample
Epoch 77/90
95501/95501 - 58s - loss: 1.4204 - val_loss: 1.4217 - 58s/epoch - 609us/sample
Epoch 78/90
95501/95501 - 58s - loss: 1.4207 - val_loss: 1.4222 - 58s/epoch - 607us/sample
Epoch 79/90
95501/95501 - 58s - loss: 1.4207 - val_loss: 1.4217 - 58s/epoch - 608us/sample
Epoch 80/90
95501/95501 - 58s - loss: 1.4203 - val_loss: 1.4218 - 58s/epoch - 610us/sample
Epoch 81/90
95501/95501 - 58s - loss: 1.4204 - val_loss: 1.4218 - 58s/epoch - 607us/sample
Epoch 82/90
95501/95501 - 58s - loss: 1.4203 - val_loss: 1.4218 - 58s/epoch - 609us/sample
Epoch 83/90
95501/95501 - 58s - loss: 1.4205 - val_loss: 1.4213 - 58s/epoch - 609us/sample
Epoch 84/90
95501/95501 - 58s - loss: 1.4203 - val_loss: 1.4216 - 58s/epoch - 607us/sample
Epoch 85/90
95501/95501 - 58s - loss: 1.4205 - val_loss: 1.4219 - 58s/epoch - 608us/sample
Epoch 86/90
95501/95501 - 58s - loss: 1.4205 - val_loss: 1.4216 - 58s/epoch - 608us/sample
Epoch 87/90
95501/95501 - 58s - loss: 1.4204 - val_loss: 1.4214 - 58s/epoch - 607us/sample
Epoch 88/90
95501/95501 - 58s - loss: 1.4202 - val_loss: 1.4213 - 58s/epoch - 609us/sample
Epoch 89/90
95501/95501 - 58s - loss: 1.4203 - val_loss: 1.4215 - 58s/epoch - 608us/sample
Epoch 90/90
95501/95501 - 58s - loss: 1.4203 - val_loss: 1.4215 - 58s/epoch - 608us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 1.4214960757714599
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-14 23:52:22.248940: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_13/outputlayer/BiasAdd' id:17178 op device:{requested: '', assigned: ''} def:{{{node decoder_model_13/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_13/outputlayer/MatMul, decoder_model_13/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.8200091290847875
cosine 0.9560856568051647
MAE: 10.307256
RMSE: 26.947138
r2: -47096.491679012666
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'binary_crossentropy', 8, 90, 0.001, 0.6, 758, 1.420334983020052, 1.4214960757714599, 0.8200091290847875, 0.9560856568051647, 10.307255744934082, 26.9471378326416, -47096.491679012666, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 90 0.0012 64 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_42 (BatchN  (None, 1896)        7584        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_42 (ReLU)                (None, 1896)         0           ['batch_normalization_42[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu_42[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu_42[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/90
2023-02-14 23:52:32.678224: W tensorflow/c/c_api.cc:291] Operation '{name:'training_28/Adam/beta_2/Assign' id:18986 op device:{requested: '', assigned: ''} def:{{{node training_28/Adam/beta_2/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_28/Adam/beta_2, training_28/Adam/beta_2/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-14 23:52:45.600075: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_14/mul' id:18550 op device:{requested: '', assigned: ''} def:{{{node loss_14/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_14/mul/x, loss_14/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 16s - loss: 0.0103 - val_loss: 0.0049 - 16s/epoch - 169us/sample
Epoch 2/90
95501/95501 - 11s - loss: 0.0043 - val_loss: 0.0037 - 11s/epoch - 119us/sample
Epoch 3/90
95501/95501 - 11s - loss: 0.0037 - val_loss: 0.0032 - 11s/epoch - 119us/sample
Epoch 4/90
95501/95501 - 11s - loss: 0.0033 - val_loss: 0.0060 - 11s/epoch - 119us/sample
Epoch 5/90
95501/95501 - 11s - loss: 0.0051 - val_loss: 0.0074 - 11s/epoch - 119us/sample
Epoch 6/90
95501/95501 - 11s - loss: 0.0026 - val_loss: 0.0022 - 11s/epoch - 119us/sample
Epoch 7/90
95501/95501 - 11s - loss: 0.0023 - val_loss: 0.0019 - 11s/epoch - 119us/sample
Epoch 8/90
95501/95501 - 11s - loss: 0.0020 - val_loss: 0.0017 - 11s/epoch - 120us/sample
Epoch 9/90
95501/95501 - 11s - loss: 0.0018 - val_loss: 0.0015 - 11s/epoch - 119us/sample
Epoch 10/90
95501/95501 - 11s - loss: 0.0016 - val_loss: 0.0015 - 11s/epoch - 119us/sample
Epoch 11/90
95501/95501 - 11s - loss: 0.0015 - val_loss: 0.0013 - 11s/epoch - 119us/sample
Epoch 12/90
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0012 - 11s/epoch - 119us/sample
Epoch 13/90
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0012 - 11s/epoch - 119us/sample
Epoch 14/90
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 119us/sample
Epoch 15/90
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0011 - 11s/epoch - 119us/sample
Epoch 16/90
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0014 - 11s/epoch - 118us/sample
Epoch 17/90
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0010 - 11s/epoch - 119us/sample
Epoch 18/90
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.8081e-04 - 11s/epoch - 119us/sample
Epoch 19/90
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.7657e-04 - 11s/epoch - 119us/sample
Epoch 20/90
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0019 - 11s/epoch - 119us/sample
Epoch 21/90
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0033 - 11s/epoch - 119us/sample
Epoch 22/90
95501/95501 - 11s - loss: 0.0020 - val_loss: 0.0011 - 11s/epoch - 120us/sample
Epoch 23/90
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.7927e-04 - 11s/epoch - 119us/sample
Epoch 24/90
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0036 - 11s/epoch - 119us/sample
Epoch 25/90
95501/95501 - 11s - loss: 0.0020 - val_loss: 0.0017 - 11s/epoch - 119us/sample
Epoch 26/90
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0010 - 11s/epoch - 119us/sample
Epoch 27/90
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.6951e-04 - 11s/epoch - 119us/sample
Epoch 28/90
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.9033e-04 - 11s/epoch - 120us/sample
Epoch 29/90
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.1571e-04 - 11s/epoch - 119us/sample
Epoch 30/90
95501/95501 - 11s - loss: 9.9724e-04 - val_loss: 9.0078e-04 - 11s/epoch - 119us/sample
Epoch 31/90
95501/95501 - 11s - loss: 0.0010 - val_loss: 8.7319e-04 - 11s/epoch - 119us/sample
Epoch 32/90
95501/95501 - 11s - loss: 9.5328e-04 - val_loss: 9.3151e-04 - 11s/epoch - 119us/sample
Epoch 33/90
95501/95501 - 11s - loss: 9.6966e-04 - val_loss: 8.8804e-04 - 11s/epoch - 119us/sample
Epoch 34/90
95501/95501 - 11s - loss: 9.3769e-04 - val_loss: 8.7921e-04 - 11s/epoch - 119us/sample
Epoch 35/90
95501/95501 - 11s - loss: 9.1416e-04 - val_loss: 8.3694e-04 - 11s/epoch - 119us/sample
Epoch 36/90
95501/95501 - 11s - loss: 9.0552e-04 - val_loss: 0.0014 - 11s/epoch - 120us/sample
Epoch 37/90
95501/95501 - 11s - loss: 9.0423e-04 - val_loss: 8.3221e-04 - 11s/epoch - 119us/sample
Epoch 38/90
95501/95501 - 11s - loss: 8.9350e-04 - val_loss: 8.1494e-04 - 11s/epoch - 119us/sample
Epoch 39/90
95501/95501 - 11s - loss: 8.7662e-04 - val_loss: 8.1086e-04 - 11s/epoch - 119us/sample
Epoch 40/90
95501/95501 - 11s - loss: 8.6929e-04 - val_loss: 8.0466e-04 - 11s/epoch - 119us/sample
Epoch 41/90
95501/95501 - 11s - loss: 8.6442e-04 - val_loss: 9.4288e-04 - 11s/epoch - 119us/sample
Epoch 42/90
95501/95501 - 11s - loss: 8.7400e-04 - val_loss: 7.9366e-04 - 11s/epoch - 119us/sample
Epoch 43/90
95501/95501 - 11s - loss: 8.5115e-04 - val_loss: 7.9891e-04 - 11s/epoch - 120us/sample
Epoch 44/90
95501/95501 - 11s - loss: 8.5912e-04 - val_loss: 7.9436e-04 - 11s/epoch - 119us/sample
Epoch 45/90
95501/95501 - 11s - loss: 8.3858e-04 - val_loss: 7.8948e-04 - 11s/epoch - 119us/sample
Epoch 46/90
95501/95501 - 11s - loss: 8.5636e-04 - val_loss: 7.9768e-04 - 11s/epoch - 119us/sample
Epoch 47/90
95501/95501 - 11s - loss: 8.4344e-04 - val_loss: 9.6240e-04 - 11s/epoch - 119us/sample
Epoch 48/90
95501/95501 - 11s - loss: 9.2208e-04 - val_loss: 7.8673e-04 - 11s/epoch - 119us/sample
Epoch 49/90
95501/95501 - 11s - loss: 8.4030e-04 - val_loss: 9.0202e-04 - 11s/epoch - 118us/sample
Epoch 50/90
95501/95501 - 11s - loss: 9.2586e-04 - val_loss: 8.4637e-04 - 11s/epoch - 119us/sample
Epoch 51/90
95501/95501 - 11s - loss: 9.0597e-04 - val_loss: 0.0015 - 11s/epoch - 119us/sample
Epoch 52/90
95501/95501 - 11s - loss: 9.5661e-04 - val_loss: 7.8329e-04 - 11s/epoch - 119us/sample
Epoch 53/90
95501/95501 - 11s - loss: 8.3558e-04 - val_loss: 8.1414e-04 - 11s/epoch - 119us/sample
Epoch 54/90
95501/95501 - 11s - loss: 8.2602e-04 - val_loss: 7.9134e-04 - 11s/epoch - 119us/sample
Epoch 55/90
95501/95501 - 11s - loss: 8.5477e-04 - val_loss: 8.1451e-04 - 11s/epoch - 119us/sample
Epoch 56/90
95501/95501 - 11s - loss: 8.5376e-04 - val_loss: 7.5829e-04 - 11s/epoch - 119us/sample
Epoch 57/90
95501/95501 - 11s - loss: 8.1310e-04 - val_loss: 7.5609e-04 - 11s/epoch - 120us/sample
Epoch 58/90
95501/95501 - 11s - loss: 8.0698e-04 - val_loss: 7.5261e-04 - 11s/epoch - 119us/sample
Epoch 59/90
95501/95501 - 11s - loss: 8.0587e-04 - val_loss: 7.4624e-04 - 11s/epoch - 119us/sample
Epoch 60/90
95501/95501 - 11s - loss: 8.0922e-04 - val_loss: 7.5171e-04 - 11s/epoch - 119us/sample
Epoch 61/90
95501/95501 - 11s - loss: 8.1188e-04 - val_loss: 8.0240e-04 - 11s/epoch - 119us/sample
Epoch 62/90
95501/95501 - 11s - loss: 8.4261e-04 - val_loss: 7.4045e-04 - 11s/epoch - 119us/sample
Epoch 63/90
95501/95501 - 11s - loss: 7.9154e-04 - val_loss: 7.5455e-04 - 11s/epoch - 119us/sample
Epoch 64/90
95501/95501 - 11s - loss: 7.9481e-04 - val_loss: 7.4432e-04 - 11s/epoch - 120us/sample
Epoch 65/90
95501/95501 - 11s - loss: 7.8413e-04 - val_loss: 7.5055e-04 - 11s/epoch - 119us/sample
Epoch 66/90
95501/95501 - 11s - loss: 7.8580e-04 - val_loss: 8.0859e-04 - 11s/epoch - 119us/sample
Epoch 67/90
95501/95501 - 11s - loss: 8.0982e-04 - val_loss: 7.3152e-04 - 11s/epoch - 119us/sample
Epoch 68/90
95501/95501 - 11s - loss: 7.8092e-04 - val_loss: 8.1665e-04 - 11s/epoch - 119us/sample
Epoch 69/90
95501/95501 - 11s - loss: 8.5939e-04 - val_loss: 9.5737e-04 - 11s/epoch - 119us/sample
Epoch 70/90
95501/95501 - 11s - loss: 9.6404e-04 - val_loss: 8.9215e-04 - 11s/epoch - 119us/sample
Epoch 71/90
95501/95501 - 11s - loss: 8.7692e-04 - val_loss: 7.8207e-04 - 11s/epoch - 119us/sample
Epoch 72/90
95501/95501 - 11s - loss: 8.4033e-04 - val_loss: 7.5497e-04 - 11s/epoch - 120us/sample
Epoch 73/90
95501/95501 - 11s - loss: 8.0199e-04 - val_loss: 7.3769e-04 - 11s/epoch - 119us/sample
Epoch 74/90
95501/95501 - 11s - loss: 7.8215e-04 - val_loss: 7.3161e-04 - 11s/epoch - 119us/sample
Epoch 75/90
95501/95501 - 11s - loss: 7.7707e-04 - val_loss: 7.3650e-04 - 11s/epoch - 118us/sample
Epoch 76/90
95501/95501 - 11s - loss: 7.7640e-04 - val_loss: 7.2527e-04 - 11s/epoch - 119us/sample
Epoch 77/90
95501/95501 - 11s - loss: 7.6651e-04 - val_loss: 7.3042e-04 - 11s/epoch - 119us/sample
Epoch 78/90
95501/95501 - 11s - loss: 7.8385e-04 - val_loss: 7.1482e-04 - 11s/epoch - 119us/sample
Epoch 79/90
95501/95501 - 11s - loss: 7.6404e-04 - val_loss: 7.1757e-04 - 11s/epoch - 120us/sample
Epoch 80/90
95501/95501 - 11s - loss: 7.6000e-04 - val_loss: 7.2308e-04 - 11s/epoch - 119us/sample
Epoch 81/90
95501/95501 - 11s - loss: 7.6035e-04 - val_loss: 9.0380e-04 - 11s/epoch - 119us/sample
Epoch 82/90
95501/95501 - 11s - loss: 7.7600e-04 - val_loss: 7.3155e-04 - 11s/epoch - 119us/sample
Epoch 83/90
95501/95501 - 11s - loss: 7.7719e-04 - val_loss: 8.2863e-04 - 11s/epoch - 118us/sample
Epoch 84/90
95501/95501 - 11s - loss: 8.5843e-04 - val_loss: 7.2880e-04 - 11s/epoch - 119us/sample
Epoch 85/90
95501/95501 - 11s - loss: 7.6394e-04 - val_loss: 8.4280e-04 - 11s/epoch - 119us/sample
Epoch 86/90
95501/95501 - 11s - loss: 8.2894e-04 - val_loss: 7.1879e-04 - 11s/epoch - 119us/sample
Epoch 87/90
95501/95501 - 11s - loss: 7.6089e-04 - val_loss: 7.2099e-04 - 11s/epoch - 120us/sample
Epoch 88/90
95501/95501 - 11s - loss: 7.6778e-04 - val_loss: 7.3377e-04 - 11s/epoch - 119us/sample
Epoch 89/90
95501/95501 - 11s - loss: 7.6878e-04 - val_loss: 7.0351e-04 - 11s/epoch - 119us/sample
Epoch 90/90
95501/95501 - 11s - loss: 7.5171e-04 - val_loss: 8.2448e-04 - 11s/epoch - 119us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.00082447583306387
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 00:09:38.101316: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_14/outputlayer/BiasAdd' id:18521 op device:{requested: '', assigned: ''} def:{{{node decoder_model_14/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_14/outputlayer/MatMul, decoder_model_14/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.006070559923710843
cosine 0.004822157949624851
MAE: 0.01421046
RMSE: 0.023025839
r2: 0.9656061528207622
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'mse', 64, 90, 0.0012, 0.6, 758, 0.000751713987822655, 0.00082447583306387, 0.006070559923710843, 0.004822157949624851, 0.014210459776222706, 0.023025838658213615, 0.9656061528207622, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 145 0.0012 64 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_45 (BatchN  (None, 1769)        7076        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_45 (ReLU)                (None, 1769)         0           ['batch_normalization_45[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1341660     ['re_lu_45[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1341660     ['re_lu_45[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4165381     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9,093,562
Trainable params: 9,084,970
Non-trainable params: 8,592
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-15 00:09:48.846577: W tensorflow/c/c_api.cc:291] Operation '{name:'training_30/Adam/dense_enc0_15/kernel/m/Assign' id:20267 op device:{requested: '', assigned: ''} def:{{{node training_30/Adam/dense_enc0_15/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_30/Adam/dense_enc0_15/kernel/m, training_30/Adam/dense_enc0_15/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 00:10:01.836775: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_15/mul' id:19811 op device:{requested: '', assigned: ''} def:{{{node loss_15/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_15/mul/x, loss_15/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 16s - loss: 0.0106 - val_loss: 0.0044 - 16s/epoch - 171us/sample
Epoch 2/145
95501/95501 - 12s - loss: 0.0042 - val_loss: 0.0034 - 12s/epoch - 120us/sample
Epoch 3/145
95501/95501 - 11s - loss: 0.0036 - val_loss: 0.0052 - 11s/epoch - 119us/sample
Epoch 4/145
95501/95501 - 11s - loss: 0.0034 - val_loss: 0.0028 - 11s/epoch - 119us/sample
Epoch 5/145
95501/95501 - 11s - loss: 0.0027 - val_loss: 0.0023 - 11s/epoch - 119us/sample
Epoch 6/145
95501/95501 - 11s - loss: 0.0476 - val_loss: 0.0034 - 11s/epoch - 119us/sample
Epoch 7/145
95501/95501 - 11s - loss: 0.0027 - val_loss: 0.0027 - 11s/epoch - 120us/sample
Epoch 8/145
95501/95501 - 11s - loss: 0.0027 - val_loss: 0.0017 - 11s/epoch - 119us/sample
Epoch 9/145
95501/95501 - 11s - loss: 0.0018 - val_loss: 0.0015 - 11s/epoch - 119us/sample
Epoch 10/145
95501/95501 - 11s - loss: 0.0017 - val_loss: 0.0016 - 11s/epoch - 120us/sample
Epoch 11/145
95501/95501 - 11s - loss: 0.0016 - val_loss: 0.0014 - 11s/epoch - 119us/sample
Epoch 12/145
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0013 - 11s/epoch - 119us/sample
Epoch 13/145
95501/95501 - 11s - loss: 0.0014 - val_loss: 0.0015 - 11s/epoch - 120us/sample
Epoch 14/145
95501/95501 - 11s - loss: 0.0015 - val_loss: 0.0011 - 11s/epoch - 119us/sample
Epoch 15/145
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0038 - 11s/epoch - 119us/sample
Epoch 16/145
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0011 - 11s/epoch - 119us/sample
Epoch 17/145
95501/95501 - 11s - loss: 0.0012 - val_loss: 0.0013 - 11s/epoch - 120us/sample
Epoch 18/145
95501/95501 - 11s - loss: 0.0013 - val_loss: 0.0010 - 11s/epoch - 119us/sample
Epoch 19/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0010 - 11s/epoch - 120us/sample
Epoch 20/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 0.0011 - 11s/epoch - 119us/sample
Epoch 21/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 9.5532e-04 - 11s/epoch - 119us/sample
Epoch 22/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 0.0011 - 11s/epoch - 119us/sample
Epoch 23/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.1720e-04 - 11s/epoch - 119us/sample
Epoch 24/145
95501/95501 - 11s - loss: 9.7973e-04 - val_loss: 0.0011 - 11s/epoch - 120us/sample
Epoch 25/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 9.0262e-04 - 11s/epoch - 120us/sample
Epoch 26/145
95501/95501 - 11s - loss: 9.5747e-04 - val_loss: 0.0012 - 11s/epoch - 119us/sample
Epoch 27/145
95501/95501 - 11s - loss: 9.6085e-04 - val_loss: 9.5864e-04 - 11s/epoch - 119us/sample
Epoch 28/145
95501/95501 - 11s - loss: 9.6560e-04 - val_loss: 8.6298e-04 - 11s/epoch - 119us/sample
Epoch 29/145
95501/95501 - 11s - loss: 9.3236e-04 - val_loss: 0.0012 - 11s/epoch - 119us/sample
Epoch 30/145
95501/95501 - 11s - loss: 9.2017e-04 - val_loss: 0.0013 - 11s/epoch - 119us/sample
Epoch 31/145
95501/95501 - 11s - loss: 0.0011 - val_loss: 8.4551e-04 - 11s/epoch - 119us/sample
Epoch 32/145
95501/95501 - 11s - loss: 9.1352e-04 - val_loss: 0.0011 - 11s/epoch - 120us/sample
Epoch 33/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 8.2680e-04 - 11s/epoch - 119us/sample
Epoch 34/145
95501/95501 - 11s - loss: 8.9828e-04 - val_loss: 0.0010 - 11s/epoch - 119us/sample
Epoch 35/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 8.3812e-04 - 11s/epoch - 119us/sample
Epoch 36/145
95501/95501 - 11s - loss: 8.9798e-04 - val_loss: 8.1882e-04 - 11s/epoch - 119us/sample
Epoch 37/145
95501/95501 - 11s - loss: 8.7769e-04 - val_loss: 8.0456e-04 - 11s/epoch - 119us/sample
Epoch 38/145
95501/95501 - 11s - loss: 8.7067e-04 - val_loss: 8.0051e-04 - 11s/epoch - 119us/sample
Epoch 39/145
95501/95501 - 11s - loss: 8.6218e-04 - val_loss: 8.2003e-04 - 11s/epoch - 120us/sample
Epoch 40/145
95501/95501 - 11s - loss: 8.5410e-04 - val_loss: 8.0074e-04 - 11s/epoch - 119us/sample
Epoch 41/145
95501/95501 - 11s - loss: 8.5512e-04 - val_loss: 0.0010 - 11s/epoch - 119us/sample
Epoch 42/145
95501/95501 - 11s - loss: 0.0010 - val_loss: 8.0965e-04 - 11s/epoch - 119us/sample
Epoch 43/145
95501/95501 - 11s - loss: 8.5063e-04 - val_loss: 8.0393e-04 - 11s/epoch - 119us/sample
Epoch 44/145
95501/95501 - 11s - loss: 8.6659e-04 - val_loss: 9.0244e-04 - 11s/epoch - 119us/sample
Epoch 45/145
95501/95501 - 11s - loss: 9.1346e-04 - val_loss: 9.1985e-04 - 11s/epoch - 119us/sample
Epoch 46/145
95501/95501 - 11s - loss: 9.2927e-04 - val_loss: 8.2003e-04 - 11s/epoch - 120us/sample
Epoch 47/145
95501/95501 - 11s - loss: 8.4733e-04 - val_loss: 8.6829e-04 - 11s/epoch - 120us/sample
Epoch 48/145
95501/95501 - 11s - loss: 9.2735e-04 - val_loss: 9.5465e-04 - 11s/epoch - 119us/sample
Epoch 49/145
95501/95501 - 11s - loss: 9.5932e-04 - val_loss: 7.8842e-04 - 11s/epoch - 119us/sample
Epoch 50/145
95501/95501 - 11s - loss: 8.3532e-04 - val_loss: 8.5700e-04 - 11s/epoch - 119us/sample
Epoch 51/145
95501/95501 - 11s - loss: 9.1782e-04 - val_loss: 7.7681e-04 - 11s/epoch - 119us/sample
Epoch 52/145
95501/95501 - 11s - loss: 8.2582e-04 - val_loss: 7.7150e-04 - 11s/epoch - 119us/sample
Epoch 53/145
95501/95501 - 11s - loss: 8.2605e-04 - val_loss: 7.9778e-04 - 11s/epoch - 120us/sample
Epoch 54/145
95501/95501 - 11s - loss: 8.3049e-04 - val_loss: 7.6287e-04 - 11s/epoch - 120us/sample
Epoch 55/145
95501/95501 - 11s - loss: 8.0883e-04 - val_loss: 8.8107e-04 - 11s/epoch - 119us/sample
Epoch 56/145
95501/95501 - 11s - loss: 8.8521e-04 - val_loss: 7.5376e-04 - 11s/epoch - 119us/sample
Epoch 57/145
95501/95501 - 11s - loss: 8.2320e-04 - val_loss: 8.0818e-04 - 11s/epoch - 119us/sample
Epoch 58/145
95501/95501 - 11s - loss: 8.4385e-04 - val_loss: 7.6963e-04 - 11s/epoch - 119us/sample
Epoch 59/145
95501/95501 - 11s - loss: 8.1825e-04 - val_loss: 7.5009e-04 - 11s/epoch - 119us/sample
Epoch 60/145
95501/95501 - 11s - loss: 8.0200e-04 - val_loss: 7.5518e-04 - 11s/epoch - 120us/sample
Epoch 61/145
95501/95501 - 11s - loss: 7.9999e-04 - val_loss: 8.0456e-04 - 11s/epoch - 119us/sample
Epoch 62/145
95501/95501 - 11s - loss: 8.2239e-04 - val_loss: 7.3453e-04 - 11s/epoch - 119us/sample
Epoch 63/145
95501/95501 - 11s - loss: 7.8661e-04 - val_loss: 8.9303e-04 - 11s/epoch - 119us/sample
Epoch 64/145
95501/95501 - 11s - loss: 8.8612e-04 - val_loss: 7.4026e-04 - 11s/epoch - 119us/sample
Epoch 65/145
95501/95501 - 11s - loss: 7.9434e-04 - val_loss: 7.3852e-04 - 11s/epoch - 119us/sample
Epoch 66/145
95501/95501 - 11s - loss: 7.8651e-04 - val_loss: 7.3214e-04 - 11s/epoch - 119us/sample
Epoch 67/145
95501/95501 - 11s - loss: 7.8408e-04 - val_loss: 7.3041e-04 - 11s/epoch - 120us/sample
Epoch 68/145
95501/95501 - 11s - loss: 7.8455e-04 - val_loss: 7.2700e-04 - 11s/epoch - 119us/sample
Epoch 69/145
95501/95501 - 11s - loss: 7.7364e-04 - val_loss: 7.3005e-04 - 11s/epoch - 119us/sample
Epoch 70/145
95501/95501 - 11s - loss: 7.7294e-04 - val_loss: 8.5552e-04 - 11s/epoch - 120us/sample
Epoch 71/145
95501/95501 - 11s - loss: 8.4853e-04 - val_loss: 7.3113e-04 - 11s/epoch - 119us/sample
Epoch 72/145
95501/95501 - 11s - loss: 7.7184e-04 - val_loss: 7.1627e-04 - 11s/epoch - 119us/sample
Epoch 73/145
95501/95501 - 11s - loss: 7.6346e-04 - val_loss: 7.5065e-04 - 11s/epoch - 119us/sample
Epoch 74/145
95501/95501 - 11s - loss: 7.8407e-04 - val_loss: 7.1496e-04 - 11s/epoch - 120us/sample
Epoch 75/145
95501/95501 - 11s - loss: 7.6385e-04 - val_loss: 7.2260e-04 - 11s/epoch - 120us/sample
Epoch 76/145
95501/95501 - 11s - loss: 7.5903e-04 - val_loss: 7.7806e-04 - 11s/epoch - 119us/sample
Epoch 77/145
95501/95501 - 11s - loss: 8.1024e-04 - val_loss: 7.3191e-04 - 11s/epoch - 119us/sample
Epoch 78/145
95501/95501 - 11s - loss: 7.7589e-04 - val_loss: 8.6939e-04 - 11s/epoch - 119us/sample
Epoch 79/145
95501/95501 - 11s - loss: 8.5423e-04 - val_loss: 7.1505e-04 - 11s/epoch - 119us/sample
Epoch 80/145
95501/95501 - 11s - loss: 7.6147e-04 - val_loss: 8.6933e-04 - 11s/epoch - 119us/sample
Epoch 81/145
95501/95501 - 11s - loss: 8.9685e-04 - val_loss: 7.2694e-04 - 11s/epoch - 120us/sample
Epoch 82/145
95501/95501 - 11s - loss: 7.7439e-04 - val_loss: 7.1601e-04 - 11s/epoch - 120us/sample
Epoch 83/145
95501/95501 - 11s - loss: 7.6692e-04 - val_loss: 7.2108e-04 - 11s/epoch - 119us/sample
Epoch 84/145
95501/95501 - 11s - loss: 7.6079e-04 - val_loss: 7.0374e-04 - 11s/epoch - 119us/sample
Epoch 85/145
95501/95501 - 11s - loss: 7.5085e-04 - val_loss: 7.1781e-04 - 11s/epoch - 119us/sample
Epoch 86/145
95501/95501 - 11s - loss: 7.5315e-04 - val_loss: 7.0417e-04 - 11s/epoch - 119us/sample
Epoch 87/145
95501/95501 - 11s - loss: 7.5075e-04 - val_loss: 8.2826e-04 - 11s/epoch - 119us/sample
Epoch 88/145
95501/95501 - 11s - loss: 8.5405e-04 - val_loss: 7.5757e-04 - 11s/epoch - 119us/sample
Epoch 89/145
95501/95501 - 11s - loss: 7.5681e-04 - val_loss: 7.1993e-04 - 11s/epoch - 120us/sample
Epoch 90/145
95501/95501 - 11s - loss: 7.7006e-04 - val_loss: 7.9939e-04 - 11s/epoch - 119us/sample
Epoch 91/145
95501/95501 - 11s - loss: 8.2783e-04 - val_loss: 7.0778e-04 - 11s/epoch - 119us/sample
Epoch 92/145
95501/95501 - 11s - loss: 7.4942e-04 - val_loss: 6.9798e-04 - 11s/epoch - 119us/sample
Epoch 93/145
95501/95501 - 11s - loss: 7.7365e-04 - val_loss: 7.7572e-04 - 11s/epoch - 120us/sample
Epoch 94/145
95501/95501 - 11s - loss: 8.0281e-04 - val_loss: 6.9810e-04 - 11s/epoch - 119us/sample
Epoch 95/145
95501/95501 - 11s - loss: 7.5025e-04 - val_loss: 6.9788e-04 - 11s/epoch - 119us/sample
Epoch 96/145
95501/95501 - 11s - loss: 7.4357e-04 - val_loss: 6.9739e-04 - 11s/epoch - 120us/sample
Epoch 97/145
95501/95501 - 11s - loss: 7.4084e-04 - val_loss: 7.0224e-04 - 11s/epoch - 119us/sample
Epoch 98/145
95501/95501 - 11s - loss: 7.4561e-04 - val_loss: 7.0203e-04 - 11s/epoch - 120us/sample
Epoch 99/145
95501/95501 - 11s - loss: 7.5141e-04 - val_loss: 6.9558e-04 - 11s/epoch - 119us/sample
Epoch 100/145
95501/95501 - 11s - loss: 7.3408e-04 - val_loss: 6.9393e-04 - 11s/epoch - 119us/sample
Epoch 101/145
95501/95501 - 11s - loss: 7.3263e-04 - val_loss: 6.8375e-04 - 11s/epoch - 119us/sample
Epoch 102/145
95501/95501 - 11s - loss: 7.2968e-04 - val_loss: 6.9579e-04 - 11s/epoch - 119us/sample
Epoch 103/145
95501/95501 - 11s - loss: 7.2884e-04 - val_loss: 6.8630e-04 - 11s/epoch - 120us/sample
Epoch 104/145
95501/95501 - 11s - loss: 7.2633e-04 - val_loss: 7.8879e-04 - 11s/epoch - 120us/sample
Epoch 105/145
95501/95501 - 11s - loss: 7.6771e-04 - val_loss: 7.1634e-04 - 11s/epoch - 119us/sample
Epoch 106/145
95501/95501 - 11s - loss: 7.4535e-04 - val_loss: 6.8671e-04 - 11s/epoch - 119us/sample
Epoch 107/145
95501/95501 - 11s - loss: 7.2567e-04 - val_loss: 6.8122e-04 - 11s/epoch - 119us/sample
Epoch 108/145
95501/95501 - 11s - loss: 7.2705e-04 - val_loss: 7.6065e-04 - 11s/epoch - 119us/sample
Epoch 109/145
95501/95501 - 11s - loss: 7.9096e-04 - val_loss: 6.8246e-04 - 11s/epoch - 120us/sample
Epoch 110/145
95501/95501 - 11s - loss: 7.2614e-04 - val_loss: 6.8564e-04 - 11s/epoch - 120us/sample
Epoch 111/145
95501/95501 - 11s - loss: 7.2085e-04 - val_loss: 7.3107e-04 - 11s/epoch - 119us/sample
Epoch 112/145
95501/95501 - 11s - loss: 7.3050e-04 - val_loss: 6.8794e-04 - 11s/epoch - 119us/sample
Epoch 113/145
95501/95501 - 11s - loss: 7.1997e-04 - val_loss: 6.7927e-04 - 11s/epoch - 119us/sample
Epoch 114/145
95501/95501 - 11s - loss: 7.1922e-04 - val_loss: 6.7712e-04 - 11s/epoch - 119us/sample
Epoch 115/145
95501/95501 - 11s - loss: 7.1718e-04 - val_loss: 7.1562e-04 - 11s/epoch - 119us/sample
Epoch 116/145
95501/95501 - 11s - loss: 7.4241e-04 - val_loss: 8.1885e-04 - 11s/epoch - 120us/sample
Epoch 117/145
95501/95501 - 11s - loss: 8.0993e-04 - val_loss: 6.8057e-04 - 11s/epoch - 120us/sample
Epoch 118/145
95501/95501 - 11s - loss: 7.2552e-04 - val_loss: 7.2763e-04 - 11s/epoch - 119us/sample
Epoch 119/145
95501/95501 - 11s - loss: 7.7055e-04 - val_loss: 6.7766e-04 - 11s/epoch - 119us/sample
Epoch 120/145
95501/95501 - 11s - loss: 7.2124e-04 - val_loss: 6.9298e-04 - 11s/epoch - 119us/sample
Epoch 121/145
95501/95501 - 11s - loss: 7.2025e-04 - val_loss: 7.0890e-04 - 11s/epoch - 119us/sample
Epoch 122/145
95501/95501 - 11s - loss: 7.4015e-04 - val_loss: 6.7853e-04 - 11s/epoch - 119us/sample
Epoch 123/145
95501/95501 - 11s - loss: 7.2037e-04 - val_loss: 7.6789e-04 - 11s/epoch - 119us/sample
Epoch 124/145
95501/95501 - 11s - loss: 7.8927e-04 - val_loss: 6.8933e-04 - 11s/epoch - 120us/sample
Epoch 125/145
95501/95501 - 11s - loss: 7.3821e-04 - val_loss: 6.8390e-04 - 11s/epoch - 120us/sample
Epoch 126/145
95501/95501 - 11s - loss: 7.1932e-04 - val_loss: 6.8067e-04 - 11s/epoch - 119us/sample
Epoch 127/145
95501/95501 - 11s - loss: 7.2422e-04 - val_loss: 7.5251e-04 - 11s/epoch - 120us/sample
Epoch 128/145
95501/95501 - 11s - loss: 7.6675e-04 - val_loss: 6.7679e-04 - 11s/epoch - 119us/sample
Epoch 129/145
95501/95501 - 11s - loss: 7.1426e-04 - val_loss: 6.6939e-04 - 11s/epoch - 119us/sample
Epoch 130/145
95501/95501 - 11s - loss: 7.1167e-04 - val_loss: 6.7854e-04 - 11s/epoch - 119us/sample
Epoch 131/145
95501/95501 - 11s - loss: 7.1217e-04 - val_loss: 7.9477e-04 - 11s/epoch - 120us/sample
Epoch 132/145
95501/95501 - 11s - loss: 7.9427e-04 - val_loss: 6.7430e-04 - 11s/epoch - 119us/sample
Epoch 133/145
95501/95501 - 11s - loss: 7.1526e-04 - val_loss: 6.6612e-04 - 11s/epoch - 119us/sample
Epoch 134/145
95501/95501 - 11s - loss: 7.1188e-04 - val_loss: 6.9026e-04 - 11s/epoch - 119us/sample
Epoch 135/145
95501/95501 - 11s - loss: 7.3208e-04 - val_loss: 6.7046e-04 - 11s/epoch - 119us/sample
Epoch 136/145
95501/95501 - 11s - loss: 7.0800e-04 - val_loss: 7.3404e-04 - 11s/epoch - 119us/sample
Epoch 137/145
95501/95501 - 11s - loss: 7.6626e-04 - val_loss: 6.7407e-04 - 11s/epoch - 119us/sample
Epoch 138/145
95501/95501 - 11s - loss: 7.1373e-04 - val_loss: 8.0192e-04 - 11s/epoch - 120us/sample
Epoch 139/145
95501/95501 - 11s - loss: 8.1000e-04 - val_loss: 6.7733e-04 - 11s/epoch - 120us/sample
Epoch 140/145
95501/95501 - 11s - loss: 7.1357e-04 - val_loss: 6.6800e-04 - 11s/epoch - 119us/sample
Epoch 141/145
95501/95501 - 11s - loss: 7.0567e-04 - val_loss: 6.6614e-04 - 11s/epoch - 119us/sample
Epoch 142/145
95501/95501 - 11s - loss: 7.1767e-04 - val_loss: 7.1499e-04 - 11s/epoch - 119us/sample
Epoch 143/145
95501/95501 - 11s - loss: 7.2537e-04 - val_loss: 8.1849e-04 - 11s/epoch - 119us/sample
Epoch 144/145
95501/95501 - 11s - loss: 7.3583e-04 - val_loss: 6.6658e-04 - 11s/epoch - 120us/sample
Epoch 145/145
95501/95501 - 11s - loss: 7.0988e-04 - val_loss: 7.2044e-04 - 11s/epoch - 120us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0007204421434064929
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 00:37:25.018811: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_15/outputlayer/BiasAdd' id:19782 op device:{requested: '', assigned: ''} def:{{{node decoder_model_15/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_15/outputlayer/MatMul, decoder_model_15/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.005071468738353886
cosine 0.003993016855473164
MAE: 0.011947496
RMSE: 0.020949047
r2: 0.9715304861032256
RMSE zero-vector: 0.23411466903540806
['1.4custom_VAE', 'mse', 64, 145, 0.0012, 0.6, 758, 0.0007098818186748675, 0.0007204421434064929, 0.005071468738353886, 0.003993016855473164, 0.011947495862841606, 0.020949047058820724, 0.9715304861032256, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 45 0.0005 64 2] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_48 (BatchN  (None, 1896)        7584        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_48 (ReLU)                (None, 1896)         0           ['batch_normalization_48[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu_48[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu_48[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/45
2023-02-15 00:37:36.098648: W tensorflow/c/c_api.cc:291] Operation '{name:'training_32/Adam/dense_dec0_16/bias/m/Assign' id:21639 op device:{requested: '', assigned: ''} def:{{{node training_32/Adam/dense_dec0_16/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_32/Adam/dense_dec0_16/bias/m, training_32/Adam/dense_dec0_16/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 00:37:49.261172: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_16/mul' id:21082 op device:{requested: '', assigned: ''} def:{{{node loss_16/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_16/mul/x, loss_16/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 17s - loss: 0.0611 - val_loss: 0.0032 - 17s/epoch - 174us/sample
Epoch 2/45
95501/95501 - 12s - loss: 0.0028 - val_loss: 0.0027 - 12s/epoch - 121us/sample
Epoch 3/45
95501/95501 - 12s - loss: 0.0116 - val_loss: 0.0039 - 12s/epoch - 121us/sample
Epoch 4/45
95501/95501 - 12s - loss: 0.0020 - val_loss: 0.0017 - 12s/epoch - 121us/sample
Epoch 5/45
95501/95501 - 12s - loss: 0.0017 - val_loss: 0.0014 - 12s/epoch - 121us/sample
Epoch 6/45
95501/95501 - 12s - loss: 0.0015 - val_loss: 0.0012 - 12s/epoch - 122us/sample
Epoch 7/45
95501/95501 - 12s - loss: 0.0013 - val_loss: 0.0011 - 12s/epoch - 121us/sample
Epoch 8/45
95501/95501 - 12s - loss: 0.0012 - val_loss: 0.0010 - 12s/epoch - 121us/sample
Epoch 9/45
95501/95501 - 12s - loss: 0.0012 - val_loss: 0.0011 - 12s/epoch - 121us/sample
Epoch 10/45
95501/95501 - 12s - loss: 0.0010 - val_loss: 0.0026 - 12s/epoch - 121us/sample
Epoch 11/45
95501/95501 - 12s - loss: 0.0011 - val_loss: 9.5511e-04 - 12s/epoch - 121us/sample
Epoch 12/45
95501/95501 - 12s - loss: 9.3382e-04 - val_loss: 8.0757e-04 - 12s/epoch - 121us/sample
Epoch 13/45
95501/95501 - 12s - loss: 8.6350e-04 - val_loss: 7.7493e-04 - 12s/epoch - 122us/sample
Epoch 14/45
95501/95501 - 12s - loss: 8.3530e-04 - val_loss: 9.1986e-04 - 12s/epoch - 122us/sample
Epoch 15/45
95501/95501 - 12s - loss: 7.9386e-04 - val_loss: 8.7432e-04 - 12s/epoch - 121us/sample
Epoch 16/45
95501/95501 - 12s - loss: 7.6924e-04 - val_loss: 0.0017 - 12s/epoch - 121us/sample
Epoch 17/45
95501/95501 - 12s - loss: 8.5958e-04 - val_loss: 6.8751e-04 - 12s/epoch - 121us/sample
Epoch 18/45
95501/95501 - 12s - loss: 7.1611e-04 - val_loss: 6.7116e-04 - 12s/epoch - 121us/sample
Epoch 19/45
95501/95501 - 12s - loss: 6.9503e-04 - val_loss: 6.5080e-04 - 12s/epoch - 121us/sample
Epoch 20/45
95501/95501 - 12s - loss: 7.7028e-04 - val_loss: 6.4694e-04 - 12s/epoch - 122us/sample
Epoch 21/45
95501/95501 - 12s - loss: 6.7756e-04 - val_loss: 6.1462e-04 - 12s/epoch - 122us/sample
Epoch 22/45
95501/95501 - 12s - loss: 6.4724e-04 - val_loss: 5.9326e-04 - 12s/epoch - 121us/sample
Epoch 23/45
95501/95501 - 12s - loss: 6.3355e-04 - val_loss: 5.8729e-04 - 12s/epoch - 121us/sample
Epoch 24/45
95501/95501 - 12s - loss: 6.2353e-04 - val_loss: 7.5350e-04 - 12s/epoch - 121us/sample
Epoch 25/45
95501/95501 - 12s - loss: 6.8770e-04 - val_loss: 5.8103e-04 - 12s/epoch - 121us/sample
Epoch 26/45
95501/95501 - 12s - loss: 6.1367e-04 - val_loss: 5.7244e-04 - 12s/epoch - 121us/sample
Epoch 27/45
95501/95501 - 12s - loss: 6.0301e-04 - val_loss: 5.5786e-04 - 12s/epoch - 121us/sample
Epoch 28/45
95501/95501 - 12s - loss: 5.9772e-04 - val_loss: 5.5867e-04 - 12s/epoch - 122us/sample
Epoch 29/45
95501/95501 - 12s - loss: 5.9317e-04 - val_loss: 5.5318e-04 - 12s/epoch - 121us/sample
Epoch 30/45
95501/95501 - 12s - loss: 5.8320e-04 - val_loss: 5.4512e-04 - 12s/epoch - 121us/sample
Epoch 31/45
95501/95501 - 12s - loss: 5.7658e-04 - val_loss: 5.4440e-04 - 12s/epoch - 121us/sample
Epoch 32/45
95501/95501 - 12s - loss: 5.7116e-04 - val_loss: 5.3579e-04 - 12s/epoch - 121us/sample
Epoch 33/45
95501/95501 - 12s - loss: 5.6777e-04 - val_loss: 5.3914e-04 - 12s/epoch - 121us/sample
Epoch 34/45
95501/95501 - 12s - loss: 5.6095e-04 - val_loss: 5.3850e-04 - 12s/epoch - 121us/sample
Epoch 35/45
95501/95501 - 12s - loss: 5.5732e-04 - val_loss: 5.3027e-04 - 12s/epoch - 121us/sample
Epoch 36/45
95501/95501 - 12s - loss: 5.7436e-04 - val_loss: 5.2591e-04 - 12s/epoch - 122us/sample
Epoch 37/45
95501/95501 - 12s - loss: 5.5302e-04 - val_loss: 5.2217e-04 - 12s/epoch - 122us/sample
Epoch 38/45
95501/95501 - 12s - loss: 5.4883e-04 - val_loss: 5.9389e-04 - 12s/epoch - 121us/sample
Epoch 39/45
95501/95501 - 12s - loss: 5.9230e-04 - val_loss: 5.2465e-04 - 12s/epoch - 121us/sample
Epoch 40/45
95501/95501 - 12s - loss: 5.4561e-04 - val_loss: 5.1302e-04 - 12s/epoch - 121us/sample
Epoch 41/45
95501/95501 - 12s - loss: 5.4818e-04 - val_loss: 5.3823e-04 - 12s/epoch - 121us/sample
Epoch 42/45
95501/95501 - 12s - loss: 5.4351e-04 - val_loss: 5.4187e-04 - 12s/epoch - 122us/sample
Epoch 43/45
95501/95501 - 12s - loss: 5.5895e-04 - val_loss: 5.1078e-04 - 12s/epoch - 122us/sample
Epoch 44/45
95501/95501 - 12s - loss: 5.3643e-04 - val_loss: 5.3245e-04 - 12s/epoch - 121us/sample
Epoch 45/45
95501/95501 - 12s - loss: 5.5408e-04 - val_loss: 5.1164e-04 - 12s/epoch - 121us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0005116352433292424
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 00:46:20.709103: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_16/outputlayer/BiasAdd' id:21046 op device:{requested: '', assigned: ''} def:{{{node decoder_model_16/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_16/outputlayer/MatMul, decoder_model_16/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.007273013114120391
cosine 0.0057476388983487454
MAE: 0.013184632
RMSE: 0.024985725
r2: 0.9595013295742801
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'logcosh', 64, 45, 0.0005, 0.6, 758, 0.000554081274064971, 0.0005116352433292424, 0.007273013114120391, 0.0057476388983487454, 0.013184632174670696, 0.024985725060105324, 0.9595013295742801, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 90 0.0012000000000000001 32 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_51 (BatchN  (None, 3160)        12640       ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_51 (ReLU)                (None, 3160)         0           ['batch_normalization_51[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2396038     ['re_lu_51[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2396038     ['re_lu_51[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         6984938     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 15,787,054
Trainable params: 15,772,898
Non-trainable params: 14,156
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/90
2023-02-15 00:46:32.134980: W tensorflow/c/c_api.cc:291] Operation '{name:'training_34/Adam/dense_dec1_17/bias/v/Assign' id:22992 op device:{requested: '', assigned: ''} def:{{{node training_34/Adam/dense_dec1_17/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_34/Adam/dense_dec1_17/bias/v, training_34/Adam/dense_dec1_17/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 00:46:53.183482: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_17/mul' id:22366 op device:{requested: '', assigned: ''} def:{{{node loss_17/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_17/mul/x, loss_17/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 25s - loss: 0.0114 - val_loss: 0.0058 - 25s/epoch - 261us/sample
Epoch 2/90
95501/95501 - 19s - loss: 0.0050 - val_loss: 0.0040 - 19s/epoch - 202us/sample
Epoch 3/90
95501/95501 - 19s - loss: 0.0040 - val_loss: 0.0029 - 19s/epoch - 201us/sample
Epoch 4/90
95501/95501 - 19s - loss: 0.0028 - val_loss: 0.0022 - 19s/epoch - 201us/sample
Epoch 5/90
95501/95501 - 19s - loss: 0.0043 - val_loss: 0.0019 - 19s/epoch - 201us/sample
Epoch 6/90
95501/95501 - 19s - loss: 0.0021 - val_loss: 0.0028 - 19s/epoch - 201us/sample
Epoch 7/90
95501/95501 - 19s - loss: 0.0019 - val_loss: 0.0015 - 19s/epoch - 203us/sample
Epoch 8/90
95501/95501 - 19s - loss: 0.0017 - val_loss: 0.0014 - 19s/epoch - 201us/sample
Epoch 9/90
95501/95501 - 19s - loss: 0.0016 - val_loss: 0.0013 - 19s/epoch - 201us/sample
Epoch 10/90
95501/95501 - 19s - loss: 0.0015 - val_loss: 0.0012 - 19s/epoch - 202us/sample
Epoch 11/90
95501/95501 - 19s - loss: 0.0014 - val_loss: 0.0012 - 19s/epoch - 203us/sample
Epoch 12/90
95501/95501 - 19s - loss: 0.0013 - val_loss: 0.0011 - 19s/epoch - 201us/sample
Epoch 13/90
95501/95501 - 19s - loss: 0.0013 - val_loss: 0.0011 - 19s/epoch - 201us/sample
Epoch 14/90
95501/95501 - 19s - loss: 0.0012 - val_loss: 0.0011 - 19s/epoch - 202us/sample
Epoch 15/90
95501/95501 - 19s - loss: 0.0012 - val_loss: 0.0010 - 19s/epoch - 202us/sample
Epoch 16/90
95501/95501 - 19s - loss: 0.0011 - val_loss: 0.0010 - 19s/epoch - 201us/sample
Epoch 17/90
95501/95501 - 19s - loss: 0.0011 - val_loss: 0.0011 - 19s/epoch - 202us/sample
Epoch 18/90
95501/95501 - 19s - loss: 0.0012 - val_loss: 9.7156e-04 - 19s/epoch - 202us/sample
Epoch 19/90
95501/95501 - 19s - loss: 0.0011 - val_loss: 9.5620e-04 - 19s/epoch - 202us/sample
Epoch 20/90
95501/95501 - 19s - loss: 0.0011 - val_loss: 9.4169e-04 - 19s/epoch - 201us/sample
Epoch 21/90
95501/95501 - 19s - loss: 0.0011 - val_loss: 9.3369e-04 - 19s/epoch - 201us/sample
Epoch 22/90
95501/95501 - 19s - loss: 0.0010 - val_loss: 9.6063e-04 - 19s/epoch - 202us/sample
Epoch 23/90
95501/95501 - 19s - loss: 0.0010 - val_loss: 9.0766e-04 - 19s/epoch - 203us/sample
Epoch 24/90
95501/95501 - 19s - loss: 0.0010 - val_loss: 8.8602e-04 - 19s/epoch - 201us/sample
Epoch 25/90
95501/95501 - 19s - loss: 0.0011 - val_loss: 8.9273e-04 - 19s/epoch - 201us/sample
Epoch 26/90
95501/95501 - 19s - loss: 9.9648e-04 - val_loss: 8.9004e-04 - 19s/epoch - 202us/sample
Epoch 27/90
95501/95501 - 19s - loss: 9.9511e-04 - val_loss: 8.9583e-04 - 19s/epoch - 203us/sample
Epoch 28/90
95501/95501 - 19s - loss: 9.7708e-04 - val_loss: 9.3411e-04 - 19s/epoch - 201us/sample
Epoch 29/90
95501/95501 - 19s - loss: 9.8160e-04 - val_loss: 8.5490e-04 - 19s/epoch - 202us/sample
Epoch 30/90
95501/95501 - 19s - loss: 9.5354e-04 - val_loss: 8.6700e-04 - 19s/epoch - 201us/sample
Epoch 31/90
95501/95501 - 19s - loss: 9.6842e-04 - val_loss: 8.4943e-04 - 19s/epoch - 202us/sample
Epoch 32/90
95501/95501 - 19s - loss: 9.4044e-04 - val_loss: 8.3982e-04 - 19s/epoch - 202us/sample
Epoch 33/90
95501/95501 - 19s - loss: 9.4265e-04 - val_loss: 8.4764e-04 - 19s/epoch - 202us/sample
Epoch 34/90
95501/95501 - 19s - loss: 9.3112e-04 - val_loss: 8.4727e-04 - 19s/epoch - 201us/sample
Epoch 35/90
95501/95501 - 19s - loss: 9.2471e-04 - val_loss: 8.2872e-04 - 19s/epoch - 201us/sample
Epoch 36/90
95501/95501 - 19s - loss: 9.2361e-04 - val_loss: 8.2703e-04 - 19s/epoch - 202us/sample
Epoch 37/90
95501/95501 - 19s - loss: 9.1534e-04 - val_loss: 8.2269e-04 - 19s/epoch - 201us/sample
Epoch 38/90
95501/95501 - 19s - loss: 9.2528e-04 - val_loss: 8.3038e-04 - 19s/epoch - 201us/sample
Epoch 39/90
95501/95501 - 19s - loss: 9.0571e-04 - val_loss: 8.1745e-04 - 19s/epoch - 202us/sample
Epoch 40/90
95501/95501 - 19s - loss: 8.9879e-04 - val_loss: 8.1255e-04 - 19s/epoch - 203us/sample
Epoch 41/90
95501/95501 - 19s - loss: 9.1749e-04 - val_loss: 8.3697e-04 - 19s/epoch - 201us/sample
Epoch 42/90
95501/95501 - 19s - loss: 8.9989e-04 - val_loss: 9.1378e-04 - 19s/epoch - 202us/sample
Epoch 43/90
95501/95501 - 19s - loss: 9.2076e-04 - val_loss: 8.1630e-04 - 19s/epoch - 201us/sample
Epoch 44/90
95501/95501 - 19s - loss: 9.0852e-04 - val_loss: 8.1389e-04 - 19s/epoch - 203us/sample
Epoch 45/90
95501/95501 - 19s - loss: 9.2902e-04 - val_loss: 8.0003e-04 - 19s/epoch - 202us/sample
Epoch 46/90
95501/95501 - 19s - loss: 8.8673e-04 - val_loss: 7.8575e-04 - 19s/epoch - 201us/sample
Epoch 47/90
95501/95501 - 19s - loss: 8.7555e-04 - val_loss: 7.9378e-04 - 19s/epoch - 201us/sample
Epoch 48/90
95501/95501 - 19s - loss: 9.6544e-04 - val_loss: 7.8832e-04 - 19s/epoch - 203us/sample
Epoch 49/90
95501/95501 - 19s - loss: 8.8423e-04 - val_loss: 7.8123e-04 - 19s/epoch - 202us/sample
Epoch 50/90
95501/95501 - 19s - loss: 8.7600e-04 - val_loss: 7.8563e-04 - 19s/epoch - 201us/sample
Epoch 51/90
95501/95501 - 19s - loss: 8.7843e-04 - val_loss: 7.8612e-04 - 19s/epoch - 202us/sample
Epoch 52/90
95501/95501 - 19s - loss: 8.6583e-04 - val_loss: 7.7986e-04 - 19s/epoch - 202us/sample
Epoch 53/90
95501/95501 - 19s - loss: 8.6221e-04 - val_loss: 7.8496e-04 - 19s/epoch - 202us/sample
Epoch 54/90
95501/95501 - 19s - loss: 8.6598e-04 - val_loss: 7.8660e-04 - 19s/epoch - 201us/sample
Epoch 55/90
95501/95501 - 19s - loss: 8.6164e-04 - val_loss: 7.7365e-04 - 19s/epoch - 202us/sample
Epoch 56/90
95501/95501 - 19s - loss: 8.5875e-04 - val_loss: 7.6675e-04 - 19s/epoch - 201us/sample
Epoch 57/90
95501/95501 - 19s - loss: 8.5057e-04 - val_loss: 7.7322e-04 - 19s/epoch - 203us/sample
Epoch 58/90
95501/95501 - 19s - loss: 8.5679e-04 - val_loss: 7.6470e-04 - 19s/epoch - 202us/sample
Epoch 59/90
95501/95501 - 19s - loss: 8.7590e-04 - val_loss: 7.8270e-04 - 19s/epoch - 201us/sample
Epoch 60/90
95501/95501 - 19s - loss: 8.4494e-04 - val_loss: 7.5413e-04 - 19s/epoch - 201us/sample
Epoch 61/90
95501/95501 - 19s - loss: 8.4809e-04 - val_loss: 7.5873e-04 - 19s/epoch - 204us/sample
Epoch 62/90
95501/95501 - 19s - loss: 8.6013e-04 - val_loss: 8.2916e-04 - 19s/epoch - 201us/sample
Epoch 63/90
95501/95501 - 19s - loss: 8.4432e-04 - val_loss: 7.5958e-04 - 19s/epoch - 201us/sample
Epoch 64/90
95501/95501 - 19s - loss: 8.4258e-04 - val_loss: 7.5446e-04 - 19s/epoch - 202us/sample
Epoch 65/90
95501/95501 - 19s - loss: 8.3393e-04 - val_loss: 7.6686e-04 - 19s/epoch - 202us/sample
Epoch 66/90
95501/95501 - 19s - loss: 8.4204e-04 - val_loss: 7.6134e-04 - 19s/epoch - 201us/sample
Epoch 67/90
95501/95501 - 19s - loss: 8.3865e-04 - val_loss: 7.6369e-04 - 19s/epoch - 202us/sample
Epoch 68/90
95501/95501 - 19s - loss: 8.3073e-04 - val_loss: 7.5973e-04 - 19s/epoch - 201us/sample
Epoch 69/90
95501/95501 - 19s - loss: 8.3474e-04 - val_loss: 7.6263e-04 - 19s/epoch - 202us/sample
Epoch 70/90
95501/95501 - 19s - loss: 8.2637e-04 - val_loss: 7.4610e-04 - 19s/epoch - 203us/sample
Epoch 71/90
95501/95501 - 19s - loss: 8.2951e-04 - val_loss: 7.5056e-04 - 19s/epoch - 201us/sample
Epoch 72/90
95501/95501 - 19s - loss: 8.3963e-04 - val_loss: 7.7885e-04 - 19s/epoch - 201us/sample
Epoch 73/90
95501/95501 - 19s - loss: 8.4338e-04 - val_loss: 7.5416e-04 - 19s/epoch - 201us/sample
Epoch 74/90
95501/95501 - 19s - loss: 8.5057e-04 - val_loss: 7.6186e-04 - 19s/epoch - 203us/sample
Epoch 75/90
95501/95501 - 19s - loss: 8.4419e-04 - val_loss: 7.5477e-04 - 19s/epoch - 201us/sample
Epoch 76/90
95501/95501 - 19s - loss: 8.2184e-04 - val_loss: 7.4330e-04 - 19s/epoch - 201us/sample
Epoch 77/90
95501/95501 - 19s - loss: 8.2411e-04 - val_loss: 7.4813e-04 - 19s/epoch - 202us/sample
Epoch 78/90
95501/95501 - 19s - loss: 8.2311e-04 - val_loss: 8.3282e-04 - 19s/epoch - 202us/sample
Epoch 79/90
95501/95501 - 19s - loss: 8.1715e-04 - val_loss: 7.3791e-04 - 19s/epoch - 201us/sample
Epoch 80/90
95501/95501 - 19s - loss: 8.2435e-04 - val_loss: 9.0945e-04 - 19s/epoch - 202us/sample
Epoch 81/90
95501/95501 - 19s - loss: 8.2072e-04 - val_loss: 7.4375e-04 - 19s/epoch - 201us/sample
Epoch 82/90
95501/95501 - 19s - loss: 8.1467e-04 - val_loss: 7.3638e-04 - 19s/epoch - 202us/sample
Epoch 83/90
95501/95501 - 19s - loss: 8.0756e-04 - val_loss: 7.4322e-04 - 19s/epoch - 202us/sample
Epoch 84/90
95501/95501 - 19s - loss: 8.1192e-04 - val_loss: 7.2331e-04 - 19s/epoch - 201us/sample
Epoch 85/90
95501/95501 - 19s - loss: 8.2394e-04 - val_loss: 7.8766e-04 - 19s/epoch - 201us/sample
Epoch 86/90
95501/95501 - 19s - loss: 8.1004e-04 - val_loss: 7.2513e-04 - 19s/epoch - 203us/sample
Epoch 87/90
95501/95501 - 19s - loss: 8.2412e-04 - val_loss: 7.3035e-04 - 19s/epoch - 201us/sample
Epoch 88/90
95501/95501 - 19s - loss: 8.0743e-04 - val_loss: 7.7487e-04 - 19s/epoch - 201us/sample
Epoch 89/90
95501/95501 - 19s - loss: 8.0796e-04 - val_loss: 7.3135e-04 - 19s/epoch - 202us/sample
Epoch 90/90
95501/95501 - 19s - loss: 8.0155e-04 - val_loss: 7.2403e-04 - 19s/epoch - 203us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0007240347657861702
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 01:15:29.617347: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_17/outputlayer/BiasAdd' id:22337 op device:{requested: '', assigned: ''} def:{{{node decoder_model_17/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_17/outputlayer/MatMul, decoder_model_17/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.005405883001822864
cosine 0.004273957664500171
MAE: 0.011219613
RMSE: 0.021677183
r2: 0.9695168899929039
RMSE zero-vector: 0.23411466903540806
['2.5custom_VAE', 'mse', 32, 90, 0.0012000000000000001, 0.6, 758, 0.0008015516679460937, 0.0007240347657861702, 0.005405883001822864, 0.004273957664500171, 0.011219613254070282, 0.021677182987332344, 0.9695168899929039, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 1
Fitness    = 98.76685138538917
Last generation's best solutions = [1.6 145 0.0014 64 1] with fitness 98.76685138538917.
Best solutions :  [array([1.5, 145, 0.0012, 64, 1], dtype=object), array([1.6, 145, 0.0014, 64, 1], dtype=object)]
Best solutions fitness :  [96.28315495666305]
[2.6 90 0.0012000000000000001 64 0] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3286)         4156790     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_54 (BatchN  (None, 3286)        13144       ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_54 (ReLU)                (None, 3286)         0           ['batch_normalization_54[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2491546     ['re_lu_54[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2491546     ['re_lu_54[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         7240340     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 16,393,366
Trainable params: 16,378,706
Non-trainable params: 14,660
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/90
2023-02-15 01:15:41.561867: W tensorflow/c/c_api.cc:291] Operation '{name:'training_36/Adam/bottleneck_zlog_18/kernel/m/Assign' id:24205 op device:{requested: '', assigned: ''} def:{{{node training_36/Adam/bottleneck_zlog_18/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_36/Adam/bottleneck_zlog_18/kernel/m, training_36/Adam/bottleneck_zlog_18/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 01:15:56.101967: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_18/mul' id:23646 op device:{requested: '', assigned: ''} def:{{{node loss_18/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_18/mul/x, loss_18/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 18s - loss: 2.6563 - val_loss: 1.5378 - 18s/epoch - 192us/sample
Epoch 2/90
95501/95501 - 13s - loss: 1.4446 - val_loss: 1.4306 - 13s/epoch - 131us/sample
Epoch 3/90
95501/95501 - 12s - loss: 1.4264 - val_loss: 1.4228 - 12s/epoch - 131us/sample
Epoch 4/90
95501/95501 - 13s - loss: 1.8817 - val_loss: 1.4158 - 13s/epoch - 132us/sample
Epoch 5/90
95501/95501 - 12s - loss: 1.4215 - val_loss: 1.4213 - 12s/epoch - 131us/sample
Epoch 6/90
95501/95501 - 12s - loss: 2.3348 - val_loss: 1.4522 - 12s/epoch - 130us/sample
Epoch 7/90
95501/95501 - 12s - loss: 1.4622 - val_loss: 1.4193 - 12s/epoch - 131us/sample
Epoch 8/90
95501/95501 - 12s - loss: 1.5334 - val_loss: 5.0387 - 12s/epoch - 131us/sample
Epoch 9/90
95501/95501 - 12s - loss: 1.4392 - val_loss: 1.4376 - 12s/epoch - 130us/sample
Epoch 10/90
95501/95501 - 12s - loss: 1.4299 - val_loss: 1.5563 - 12s/epoch - 130us/sample
Epoch 11/90
95501/95501 - 12s - loss: 1.4378 - val_loss: 1.4320 - 12s/epoch - 131us/sample
Epoch 12/90
95501/95501 - 13s - loss: 1.4183 - val_loss: 1.4149 - 13s/epoch - 131us/sample
Epoch 13/90
95501/95501 - 13s - loss: 1.4152 - val_loss: 1.4242 - 13s/epoch - 131us/sample
Epoch 14/90
95501/95501 - 12s - loss: 1.5162 - val_loss: 1.4929 - 12s/epoch - 131us/sample
Epoch 15/90
95501/95501 - 12s - loss: 1.4334 - val_loss: 1.4310 - 12s/epoch - 131us/sample
Epoch 16/90
95501/95501 - 12s - loss: 1.4294 - val_loss: 1.4255 - 12s/epoch - 131us/sample
Epoch 17/90
95501/95501 - 12s - loss: 1.4250 - val_loss: 1.4156 - 12s/epoch - 130us/sample
Epoch 18/90
95501/95501 - 13s - loss: 1.4304 - val_loss: 1.4204 - 13s/epoch - 131us/sample
Epoch 19/90
95501/95501 - 13s - loss: 1.4185 - val_loss: 1.4207 - 13s/epoch - 131us/sample
Epoch 20/90
95501/95501 - 12s - loss: 1.4268 - val_loss: 1.4492 - 12s/epoch - 131us/sample
Epoch 21/90
95501/95501 - 12s - loss: 1.4155 - val_loss: 1.4123 - 12s/epoch - 130us/sample
Epoch 22/90
95501/95501 - 12s - loss: 1.4149 - val_loss: 2.5842 - 12s/epoch - 131us/sample
Epoch 23/90
95501/95501 - 12s - loss: 1.4211 - val_loss: 1.4161 - 12s/epoch - 131us/sample
Epoch 24/90
95501/95501 - 12s - loss: 1.4120 - val_loss: 1.4127 - 12s/epoch - 130us/sample
Epoch 25/90
95501/95501 - 12s - loss: 1.4121 - val_loss: 1.4103 - 12s/epoch - 131us/sample
Epoch 26/90
95501/95501 - 13s - loss: 1.4116 - val_loss: 1.4091 - 13s/epoch - 131us/sample
Epoch 27/90
95501/95501 - 12s - loss: 1.4212 - val_loss: 1.4407 - 12s/epoch - 131us/sample
Epoch 28/90
95501/95501 - 13s - loss: 1.4117 - val_loss: 1.4081 - 13s/epoch - 131us/sample
Epoch 29/90
95501/95501 - 12s - loss: 1.4163 - val_loss: 1.4217 - 12s/epoch - 131us/sample
Epoch 30/90
95501/95501 - 12s - loss: 1.4122 - val_loss: 1.4137 - 12s/epoch - 131us/sample
Epoch 31/90
95501/95501 - 12s - loss: 1.4175 - val_loss: 1.4627 - 12s/epoch - 131us/sample
Epoch 32/90
95501/95501 - 12s - loss: 1.4273 - val_loss: 1.4199 - 12s/epoch - 131us/sample
Epoch 33/90
95501/95501 - 13s - loss: 1.4201 - val_loss: 1.4676 - 13s/epoch - 132us/sample
Epoch 34/90
95501/95501 - 13s - loss: 1.4191 - val_loss: 1.4218 - 13s/epoch - 131us/sample
Epoch 35/90
95501/95501 - 12s - loss: 1.4101 - val_loss: 1.4098 - 12s/epoch - 131us/sample
Epoch 36/90
95501/95501 - 12s - loss: 1.4212 - val_loss: 1.4177 - 12s/epoch - 131us/sample
Epoch 37/90
95501/95501 - 12s - loss: 1.4118 - val_loss: 1.4106 - 12s/epoch - 131us/sample
Epoch 38/90
95501/95501 - 13s - loss: 1.4084 - val_loss: 1.4097 - 13s/epoch - 131us/sample
Epoch 39/90
95501/95501 - 12s - loss: 1.4204 - val_loss: 1.4202 - 12s/epoch - 131us/sample
Epoch 40/90
95501/95501 - 13s - loss: 1.4107 - val_loss: 1.4107 - 13s/epoch - 132us/sample
Epoch 41/90
95501/95501 - 12s - loss: 1.4106 - val_loss: 1.4131 - 12s/epoch - 131us/sample
Epoch 42/90
95501/95501 - 12s - loss: 1.4171 - val_loss: 1.4099 - 12s/epoch - 131us/sample
Epoch 43/90
95501/95501 - 12s - loss: 1.4099 - val_loss: 1.4093 - 12s/epoch - 131us/sample
Epoch 44/90
95501/95501 - 12s - loss: 1.4088 - val_loss: 1.4119 - 12s/epoch - 131us/sample
Epoch 45/90
95501/95501 - 12s - loss: 1.4107 - val_loss: 1.4116 - 12s/epoch - 131us/sample
Epoch 46/90
95501/95501 - 12s - loss: 1.4148 - val_loss: 1.4229 - 12s/epoch - 131us/sample
Epoch 47/90
95501/95501 - 13s - loss: 1.4109 - val_loss: 1.4122 - 13s/epoch - 132us/sample
Epoch 48/90
95501/95501 - 13s - loss: 1.4108 - val_loss: 1.4098 - 13s/epoch - 131us/sample
Epoch 49/90
95501/95501 - 13s - loss: 1.4084 - val_loss: 1.4084 - 13s/epoch - 131us/sample
Epoch 50/90
95501/95501 - 12s - loss: 1.4073 - val_loss: 1.4143 - 12s/epoch - 131us/sample
Epoch 51/90
95501/95501 - 12s - loss: 1.4059 - val_loss: 1.4076 - 12s/epoch - 131us/sample
Epoch 52/90
95501/95501 - 12s - loss: 1.4080 - val_loss: 1.4063 - 12s/epoch - 131us/sample
Epoch 53/90
95501/95501 - 13s - loss: 1.4039 - val_loss: 1.4050 - 13s/epoch - 131us/sample
Epoch 54/90
95501/95501 - 13s - loss: 1.4039 - val_loss: 1.4049 - 13s/epoch - 131us/sample
Epoch 55/90
95501/95501 - 13s - loss: 1.4062 - val_loss: 1.4054 - 13s/epoch - 132us/sample
Epoch 56/90
95501/95501 - 12s - loss: 1.4047 - val_loss: 1.4078 - 12s/epoch - 131us/sample
Epoch 57/90
95501/95501 - 12s - loss: 1.4121 - val_loss: 1.4077 - 12s/epoch - 131us/sample
Epoch 58/90
95501/95501 - 13s - loss: 1.4101 - val_loss: 1.4088 - 13s/epoch - 131us/sample
Epoch 59/90
95501/95501 - 12s - loss: 1.4064 - val_loss: 1.4075 - 12s/epoch - 131us/sample
Epoch 60/90
95501/95501 - 12s - loss: 1.4056 - val_loss: 1.4066 - 12s/epoch - 131us/sample
Epoch 61/90
95501/95501 - 13s - loss: 1.4061 - val_loss: 1.4095 - 13s/epoch - 131us/sample
Epoch 62/90
95501/95501 - 13s - loss: 1.4047 - val_loss: 1.4055 - 13s/epoch - 132us/sample
Epoch 63/90
95501/95501 - 13s - loss: 1.4069 - val_loss: 1.4066 - 13s/epoch - 131us/sample
Epoch 64/90
95501/95501 - 13s - loss: 1.4049 - val_loss: 1.4062 - 13s/epoch - 131us/sample
Epoch 65/90
95501/95501 - 12s - loss: 1.4063 - val_loss: 1.4059 - 12s/epoch - 131us/sample
Epoch 66/90
95501/95501 - 13s - loss: 1.4086 - val_loss: 1.4084 - 13s/epoch - 131us/sample
Epoch 67/90
95501/95501 - 12s - loss: 1.4059 - val_loss: 1.4060 - 12s/epoch - 131us/sample
Epoch 68/90
95501/95501 - 12s - loss: 1.4034 - val_loss: 1.4047 - 12s/epoch - 131us/sample
Epoch 69/90
95501/95501 - 13s - loss: 6.8633 - val_loss: 1.4055 - 13s/epoch - 132us/sample
Epoch 70/90
95501/95501 - 13s - loss: 1.4053 - val_loss: 1.4076 - 13s/epoch - 131us/sample
Epoch 71/90
95501/95501 - 12s - loss: 1.4053 - val_loss: 1.4060 - 12s/epoch - 131us/sample
Epoch 72/90
95501/95501 - 12s - loss: 1.4047 - val_loss: 1.4094 - 12s/epoch - 131us/sample
Epoch 73/90
95501/95501 - 12s - loss: 1.4078 - val_loss: 1.4077 - 12s/epoch - 131us/sample
Epoch 74/90
95501/95501 - 13s - loss: 1.4040 - val_loss: 1.4057 - 13s/epoch - 131us/sample
Epoch 75/90
95501/95501 - 13s - loss: 1.4033 - val_loss: 1.4080 - 13s/epoch - 131us/sample
Epoch 76/90
95501/95501 - 13s - loss: 1.4038 - val_loss: 1.4044 - 13s/epoch - 132us/sample
Epoch 77/90
95501/95501 - 13s - loss: 1.4038 - val_loss: 1.4055 - 13s/epoch - 131us/sample
Epoch 78/90
95501/95501 - 13s - loss: 1.4037 - val_loss: 1.4052 - 13s/epoch - 131us/sample
Epoch 79/90
95501/95501 - 13s - loss: 1.4031 - val_loss: 1.4057 - 13s/epoch - 131us/sample
Epoch 80/90
95501/95501 - 13s - loss: 1.4054 - val_loss: 1.4099 - 13s/epoch - 131us/sample
Epoch 81/90
95501/95501 - 13s - loss: 1.4047 - val_loss: 1.4053 - 13s/epoch - 131us/sample
Epoch 82/90
95501/95501 - 13s - loss: 1.4034 - val_loss: 1.4050 - 13s/epoch - 131us/sample
Epoch 83/90
95501/95501 - 13s - loss: 1.4078 - val_loss: 1.4084 - 13s/epoch - 131us/sample
Epoch 84/90
95501/95501 - 13s - loss: 1.4076 - val_loss: 1.4126 - 13s/epoch - 131us/sample
Epoch 85/90
95501/95501 - 12s - loss: 1.4054 - val_loss: 1.4052 - 12s/epoch - 131us/sample
Epoch 86/90
95501/95501 - 12s - loss: 1.4033 - val_loss: 1.4044 - 12s/epoch - 131us/sample
Epoch 87/90
95501/95501 - 12s - loss: 1.4058 - val_loss: 1.4136 - 12s/epoch - 131us/sample
Epoch 88/90
95501/95501 - 12s - loss: 1.4058 - val_loss: 1.4054 - 12s/epoch - 131us/sample
Epoch 89/90
95501/95501 - 13s - loss: 1.4075 - val_loss: 1.4131 - 13s/epoch - 131us/sample
Epoch 90/90
95501/95501 - 13s - loss: 1.4066 - val_loss: 1.4059 - 13s/epoch - 132us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 1.4058684117830882
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 01:34:31.097356: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_18/outputlayer/BiasAdd' id:23598 op device:{requested: '', assigned: ''} def:{{{node decoder_model_18/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_18/outputlayer/MatMul, decoder_model_18/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.726437278174052
cosine 0.9216934979546578
MAE: 10.299182
RMSE: 22.142845
r2: -31796.731816807674
RMSE zero-vector: 0.23411466903540806
['2.6custom_VAE', 'binary_crossentropy', 64, 90, 0.0012000000000000001, 0.6, 758, 1.4065860074357404, 1.4058684117830882, 0.726437278174052, 0.9216934979546578, 10.299181938171387, 22.142845153808594, -31796.731816807674, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6 145 0.0014 64 0] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2022)         2557830     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_57 (BatchN  (None, 2022)        8088        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_57 (ReLU)                (None, 2022)         0           ['batch_normalization_57[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1533434     ['re_lu_57[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1533434     ['re_lu_57[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4678212     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 10,310,998
Trainable params: 10,301,394
Non-trainable params: 9,604
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-15 01:34:43.514723: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_59/beta/Assign' id:24787 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_59/beta/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_59/beta, batch_normalization_59/beta/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 01:34:57.737835: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_19/mul' id:24992 op device:{requested: '', assigned: ''} def:{{{node loss_19/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_19/mul/x, loss_19/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 18s - loss: 2.6340 - val_loss: 1.4210 - 18s/epoch - 191us/sample
Epoch 2/145
95501/95501 - 12s - loss: 1.4139 - val_loss: 1.4169 - 12s/epoch - 128us/sample
Epoch 3/145
95501/95501 - 12s - loss: 1.4133 - val_loss: 1.4129 - 12s/epoch - 128us/sample
Epoch 4/145
95501/95501 - 12s - loss: 1.4178 - val_loss: 1.4146 - 12s/epoch - 128us/sample
Epoch 5/145
95501/95501 - 12s - loss: 1.5452 - val_loss: 1.4282 - 12s/epoch - 129us/sample
Epoch 6/145
95501/95501 - 12s - loss: 1.4363 - val_loss: 1.4196 - 12s/epoch - 128us/sample
Epoch 7/145
95501/95501 - 12s - loss: 1.4201 - val_loss: 1.4196 - 12s/epoch - 128us/sample
Epoch 8/145
95501/95501 - 12s - loss: 1.4393 - val_loss: 1.4215 - 12s/epoch - 128us/sample
Epoch 9/145
95501/95501 - 12s - loss: 1.4338 - val_loss: 1.4226 - 12s/epoch - 128us/sample
Epoch 10/145
95501/95501 - 12s - loss: 1.4158 - val_loss: 1.4295 - 12s/epoch - 128us/sample
Epoch 11/145
95501/95501 - 12s - loss: 1.4198 - val_loss: 1.4135 - 12s/epoch - 128us/sample
Epoch 12/145
95501/95501 - 12s - loss: 1.4221 - val_loss: 1.4162 - 12s/epoch - 129us/sample
Epoch 13/145
95501/95501 - 12s - loss: 1.4117 - val_loss: 1.4108 - 12s/epoch - 128us/sample
Epoch 14/145
95501/95501 - 12s - loss: 1.4127 - val_loss: 1.4100 - 12s/epoch - 128us/sample
Epoch 15/145
95501/95501 - 12s - loss: 1.4131 - val_loss: 1.4386 - 12s/epoch - 128us/sample
Epoch 16/145
95501/95501 - 12s - loss: 1.4164 - val_loss: 1.4256 - 12s/epoch - 128us/sample
Epoch 17/145
95501/95501 - 12s - loss: 1.4169 - val_loss: 1.4146 - 12s/epoch - 127us/sample
Epoch 18/145
95501/95501 - 12s - loss: 1.4131 - val_loss: 1.4125 - 12s/epoch - 128us/sample
Epoch 19/145
95501/95501 - 12s - loss: 1.4146 - val_loss: 1.4143 - 12s/epoch - 129us/sample
Epoch 20/145
95501/95501 - 12s - loss: 1.4134 - val_loss: 1.4167 - 12s/epoch - 128us/sample
Epoch 21/145
95501/95501 - 12s - loss: 1.4161 - val_loss: 1.4147 - 12s/epoch - 128us/sample
Epoch 22/145
95501/95501 - 12s - loss: 1.4114 - val_loss: 1.4111 - 12s/epoch - 128us/sample
Epoch 23/145
95501/95501 - 12s - loss: 1.4111 - val_loss: 1.4146 - 12s/epoch - 127us/sample
Epoch 24/145
95501/95501 - 12s - loss: 1.4113 - val_loss: 1.4127 - 12s/epoch - 128us/sample
Epoch 25/145
95501/95501 - 12s - loss: 1.4085 - val_loss: 1.4113 - 12s/epoch - 128us/sample
Epoch 26/145
95501/95501 - 12s - loss: 1.4123 - val_loss: 1.4295 - 12s/epoch - 129us/sample
Epoch 27/145
95501/95501 - 12s - loss: 1.4187 - val_loss: 1.4182 - 12s/epoch - 128us/sample
Epoch 28/145
95501/95501 - 12s - loss: 1.4131 - val_loss: 1.4133 - 12s/epoch - 128us/sample
Epoch 29/145
95501/95501 - 12s - loss: 1.4101 - val_loss: 1.4111 - 12s/epoch - 128us/sample
Epoch 30/145
95501/95501 - 12s - loss: 1.4090 - val_loss: 1.4115 - 12s/epoch - 128us/sample
Epoch 31/145
95501/95501 - 12s - loss: 1.4120 - val_loss: 1.4158 - 12s/epoch - 128us/sample
Epoch 32/145
95501/95501 - 12s - loss: 1.4120 - val_loss: 1.4115 - 12s/epoch - 129us/sample
Epoch 33/145
95501/95501 - 12s - loss: 1.4106 - val_loss: 1.4150 - 12s/epoch - 128us/sample
Epoch 34/145
95501/95501 - 12s - loss: 1.4109 - val_loss: 1.4123 - 12s/epoch - 128us/sample
Epoch 35/145
95501/95501 - 12s - loss: 1.4112 - val_loss: 1.4142 - 12s/epoch - 128us/sample
Epoch 36/145
95501/95501 - 12s - loss: 1.4120 - val_loss: 1.4118 - 12s/epoch - 128us/sample
Epoch 37/145
95501/95501 - 12s - loss: 1.4123 - val_loss: 1.4132 - 12s/epoch - 127us/sample
Epoch 38/145
95501/95501 - 12s - loss: 1.4106 - val_loss: 1.4107 - 12s/epoch - 128us/sample
Epoch 39/145
95501/95501 - 12s - loss: 1.4100 - val_loss: 1.4120 - 12s/epoch - 129us/sample
Epoch 40/145
95501/95501 - 12s - loss: 1.4087 - val_loss: 1.4098 - 12s/epoch - 128us/sample
Epoch 41/145
95501/95501 - 12s - loss: 1.4091 - val_loss: 1.4117 - 12s/epoch - 128us/sample
Epoch 42/145
95501/95501 - 12s - loss: 1.4105 - val_loss: 1.4150 - 12s/epoch - 128us/sample
Epoch 43/145
95501/95501 - 12s - loss: 1.4099 - val_loss: 1.4107 - 12s/epoch - 128us/sample
Epoch 44/145
95501/95501 - 12s - loss: 1.4098 - val_loss: 1.4108 - 12s/epoch - 128us/sample
Epoch 45/145
95501/95501 - 12s - loss: 1.4086 - val_loss: 1.4096 - 12s/epoch - 129us/sample
Epoch 46/145
95501/95501 - 12s - loss: 1.4083 - val_loss: 1.4093 - 12s/epoch - 128us/sample
Epoch 47/145
95501/95501 - 12s - loss: 1.4077 - val_loss: 1.4090 - 12s/epoch - 128us/sample
Epoch 48/145
95501/95501 - 12s - loss: 1.4071 - val_loss: 1.4087 - 12s/epoch - 128us/sample
Epoch 49/145
95501/95501 - 12s - loss: 1.4078 - val_loss: 1.4111 - 12s/epoch - 127us/sample
Epoch 50/145
95501/95501 - 12s - loss: 1.4084 - val_loss: 1.4088 - 12s/epoch - 128us/sample
Epoch 51/145
95501/95501 - 13s - loss: 1.4113 - val_loss: 1.4101 - 13s/epoch - 134us/sample
Epoch 52/145
95501/95501 - 13s - loss: 1.4082 - val_loss: 1.4095 - 13s/epoch - 135us/sample
Epoch 53/145
95501/95501 - 13s - loss: 1.4088 - val_loss: 1.4108 - 13s/epoch - 134us/sample
Epoch 54/145
95501/95501 - 13s - loss: 1.4074 - val_loss: 1.4115 - 13s/epoch - 134us/sample
Epoch 55/145
95501/95501 - 13s - loss: 1.4072 - val_loss: 1.4094 - 13s/epoch - 134us/sample
Epoch 56/145
95501/95501 - 13s - loss: 1.4069 - val_loss: 1.4102 - 13s/epoch - 134us/sample
Epoch 57/145
95501/95501 - 13s - loss: 1.4064 - val_loss: 1.4079 - 13s/epoch - 134us/sample
Epoch 58/145
95501/95501 - 13s - loss: 1.4066 - val_loss: 1.4103 - 13s/epoch - 134us/sample
Epoch 59/145
95501/95501 - 13s - loss: 1.4073 - val_loss: 1.4109 - 13s/epoch - 135us/sample
Epoch 60/145
95501/95501 - 13s - loss: 1.4089 - val_loss: 1.4096 - 13s/epoch - 134us/sample
Epoch 61/145
95501/95501 - 13s - loss: 1.4070 - val_loss: 1.4084 - 13s/epoch - 134us/sample
Epoch 62/145
95501/95501 - 13s - loss: 1.4069 - val_loss: 1.4104 - 13s/epoch - 134us/sample
Epoch 63/145
95501/95501 - 13s - loss: 1.4076 - val_loss: 1.4085 - 13s/epoch - 134us/sample
Epoch 64/145
95501/95501 - 13s - loss: 1.4060 - val_loss: 1.4076 - 13s/epoch - 134us/sample
Epoch 65/145
95501/95501 - 13s - loss: 1.4066 - val_loss: 1.4083 - 13s/epoch - 135us/sample
Epoch 66/145
95501/95501 - 13s - loss: 1.4073 - val_loss: 1.4092 - 13s/epoch - 135us/sample
Epoch 67/145
95501/95501 - 13s - loss: 1.4065 - val_loss: 1.4084 - 13s/epoch - 134us/sample
Epoch 68/145
95501/95501 - 13s - loss: 1.4060 - val_loss: 1.4079 - 13s/epoch - 134us/sample
Epoch 69/145
95501/95501 - 13s - loss: 1.4058 - val_loss: 1.4079 - 13s/epoch - 134us/sample
Epoch 70/145
95501/95501 - 13s - loss: 1.4075 - val_loss: 1.4084 - 13s/epoch - 134us/sample
Epoch 71/145
95501/95501 - 13s - loss: 1.4064 - val_loss: 1.4082 - 13s/epoch - 135us/sample
Epoch 72/145
95501/95501 - 13s - loss: 1.4062 - val_loss: 1.4081 - 13s/epoch - 134us/sample
Epoch 73/145
95501/95501 - 13s - loss: 1.4072 - val_loss: 1.4090 - 13s/epoch - 134us/sample
Epoch 74/145
95501/95501 - 13s - loss: 1.4066 - val_loss: 1.4088 - 13s/epoch - 134us/sample
Epoch 75/145
95501/95501 - 13s - loss: 1.4061 - val_loss: 1.4082 - 13s/epoch - 134us/sample
Epoch 76/145
95501/95501 - 13s - loss: 1.4066 - val_loss: 1.4095 - 13s/epoch - 134us/sample
Epoch 77/145
95501/95501 - 13s - loss: 1.4074 - val_loss: 1.4090 - 13s/epoch - 134us/sample
Epoch 78/145
95501/95501 - 13s - loss: 1.4063 - val_loss: 1.4090 - 13s/epoch - 135us/sample
Epoch 79/145
95501/95501 - 13s - loss: 1.4061 - val_loss: 1.4080 - 13s/epoch - 134us/sample
Epoch 80/145
95501/95501 - 13s - loss: 1.4056 - val_loss: 1.4076 - 13s/epoch - 134us/sample
Epoch 81/145
95501/95501 - 13s - loss: 1.4053 - val_loss: 1.4072 - 13s/epoch - 134us/sample
Epoch 82/145
95501/95501 - 13s - loss: 1.4064 - val_loss: 1.4088 - 13s/epoch - 134us/sample
Epoch 83/145
95501/95501 - 13s - loss: 1.4065 - val_loss: 1.4082 - 13s/epoch - 134us/sample
Epoch 84/145
95501/95501 - 13s - loss: 1.4060 - val_loss: 1.4084 - 13s/epoch - 136us/sample
Epoch 85/145
95501/95501 - 13s - loss: 1.4058 - val_loss: 1.4075 - 13s/epoch - 134us/sample
Epoch 86/145
95501/95501 - 13s - loss: 1.4055 - val_loss: 1.4070 - 13s/epoch - 134us/sample
Epoch 87/145
95501/95501 - 13s - loss: 1.4057 - val_loss: 1.4079 - 13s/epoch - 134us/sample
Epoch 88/145
95501/95501 - 13s - loss: 1.4055 - val_loss: 1.4071 - 13s/epoch - 134us/sample
Epoch 89/145
95501/95501 - 13s - loss: 1.4056 - val_loss: 1.4081 - 13s/epoch - 134us/sample
Epoch 90/145
95501/95501 - 13s - loss: 1.4060 - val_loss: 1.4084 - 13s/epoch - 135us/sample
Epoch 91/145
95501/95501 - 13s - loss: 1.4074 - val_loss: 1.4087 - 13s/epoch - 135us/sample
Epoch 92/145
95501/95501 - 13s - loss: 1.4066 - val_loss: 1.4086 - 13s/epoch - 134us/sample
Epoch 93/145
95501/95501 - 13s - loss: 1.4064 - val_loss: 1.4080 - 13s/epoch - 134us/sample
Epoch 94/145
95501/95501 - 13s - loss: 1.4069 - val_loss: 1.4089 - 13s/epoch - 134us/sample
Epoch 95/145
95501/95501 - 13s - loss: 1.4067 - val_loss: 1.4086 - 13s/epoch - 134us/sample
Epoch 96/145
95501/95501 - 13s - loss: 1.4071 - val_loss: 1.4083 - 13s/epoch - 134us/sample
Epoch 97/145
95501/95501 - 13s - loss: 1.4062 - val_loss: 1.4076 - 13s/epoch - 135us/sample
Epoch 98/145
95501/95501 - 13s - loss: 1.4058 - val_loss: 1.4079 - 13s/epoch - 134us/sample
Epoch 99/145
95501/95501 - 13s - loss: 1.4053 - val_loss: 1.4070 - 13s/epoch - 134us/sample
Epoch 100/145
95501/95501 - 13s - loss: 1.4067 - val_loss: 1.4081 - 13s/epoch - 134us/sample
Epoch 101/145
95501/95501 - 13s - loss: 1.4066 - val_loss: 1.4080 - 13s/epoch - 134us/sample
Epoch 102/145
95501/95501 - 13s - loss: 1.4067 - val_loss: 1.4078 - 13s/epoch - 134us/sample
Epoch 103/145
95501/95501 - 13s - loss: 1.4057 - val_loss: 1.4075 - 13s/epoch - 135us/sample
Epoch 104/145
95501/95501 - 13s - loss: 1.4055 - val_loss: 1.4087 - 13s/epoch - 135us/sample
Epoch 105/145
95501/95501 - 13s - loss: 1.4066 - val_loss: 1.4085 - 13s/epoch - 134us/sample
Epoch 106/145
95501/95501 - 13s - loss: 1.4060 - val_loss: 1.4079 - 13s/epoch - 134us/sample
Epoch 107/145
95501/95501 - 13s - loss: 1.4059 - val_loss: 1.4077 - 13s/epoch - 134us/sample
Epoch 108/145
95501/95501 - 13s - loss: 1.4055 - val_loss: 1.4073 - 13s/epoch - 134us/sample
Epoch 109/145
95501/95501 - 13s - loss: 1.4053 - val_loss: 1.4073 - 13s/epoch - 135us/sample
Epoch 110/145
95501/95501 - 13s - loss: 1.4071 - val_loss: 1.4091 - 13s/epoch - 134us/sample
Epoch 111/145
95501/95501 - 13s - loss: 1.4073 - val_loss: 1.4082 - 13s/epoch - 134us/sample
Epoch 112/145
95501/95501 - 13s - loss: 1.4064 - val_loss: 1.4085 - 13s/epoch - 134us/sample
Epoch 113/145
95501/95501 - 13s - loss: 1.4056 - val_loss: 1.4073 - 13s/epoch - 134us/sample
Epoch 114/145
95501/95501 - 13s - loss: 1.4062 - val_loss: 1.4086 - 13s/epoch - 134us/sample
Epoch 115/145
95501/95501 - 13s - loss: 1.4069 - val_loss: 1.4082 - 13s/epoch - 135us/sample
Epoch 116/145
95501/95501 - 13s - loss: 1.4062 - val_loss: 1.4076 - 13s/epoch - 135us/sample
Epoch 117/145
95501/95501 - 13s - loss: 1.4058 - val_loss: 1.4097 - 13s/epoch - 134us/sample
Epoch 118/145
95501/95501 - 13s - loss: 1.4062 - val_loss: 1.4080 - 13s/epoch - 134us/sample
Epoch 119/145
95501/95501 - 13s - loss: 1.4056 - val_loss: 1.4070 - 13s/epoch - 134us/sample
Epoch 120/145
95501/95501 - 13s - loss: 1.4051 - val_loss: 1.4068 - 13s/epoch - 134us/sample
Epoch 121/145
95501/95501 - 13s - loss: 1.4050 - val_loss: 1.4072 - 13s/epoch - 134us/sample
Epoch 122/145
95501/95501 - 13s - loss: 1.4053 - val_loss: 1.4072 - 13s/epoch - 135us/sample
Epoch 123/145
95501/95501 - 13s - loss: 1.4050 - val_loss: 1.4069 - 13s/epoch - 134us/sample
Epoch 124/145
95501/95501 - 13s - loss: 1.4047 - val_loss: 1.4066 - 13s/epoch - 134us/sample
Epoch 125/145
95501/95501 - 13s - loss: 1.4048 - val_loss: 1.4078 - 13s/epoch - 134us/sample
Epoch 126/145
95501/95501 - 13s - loss: 1.4052 - val_loss: 1.4072 - 13s/epoch - 134us/sample
Epoch 127/145
95501/95501 - 13s - loss: 1.4050 - val_loss: 1.4069 - 13s/epoch - 134us/sample
Epoch 128/145
95501/95501 - 13s - loss: 1.4063 - val_loss: 1.4090 - 13s/epoch - 135us/sample
Epoch 129/145
95501/95501 - 13s - loss: 1.4063 - val_loss: 1.4078 - 13s/epoch - 134us/sample
Epoch 130/145
95501/95501 - 13s - loss: 1.4057 - val_loss: 1.4079 - 13s/epoch - 134us/sample
Epoch 131/145
95501/95501 - 13s - loss: 1.4058 - val_loss: 1.4078 - 13s/epoch - 134us/sample
Epoch 132/145
95501/95501 - 13s - loss: 1.4054 - val_loss: 1.4071 - 13s/epoch - 134us/sample
Epoch 133/145
95501/95501 - 13s - loss: 1.4052 - val_loss: 1.4073 - 13s/epoch - 134us/sample
Epoch 134/145
95501/95501 - 13s - loss: 1.4055 - val_loss: 1.4074 - 13s/epoch - 135us/sample
Epoch 135/145
95501/95501 - 13s - loss: 1.4063 - val_loss: 1.4087 - 13s/epoch - 134us/sample
Epoch 136/145
95501/95501 - 13s - loss: 1.4062 - val_loss: 1.4078 - 13s/epoch - 134us/sample
Epoch 137/145
95501/95501 - 13s - loss: 1.4064 - val_loss: 1.4077 - 13s/epoch - 134us/sample
Epoch 138/145
95501/95501 - 13s - loss: 1.4053 - val_loss: 1.4067 - 13s/epoch - 134us/sample
Epoch 139/145
95501/95501 - 13s - loss: 1.4047 - val_loss: 1.4067 - 13s/epoch - 134us/sample
Epoch 140/145
95501/95501 - 13s - loss: 1.4051 - val_loss: 1.4076 - 13s/epoch - 135us/sample
Epoch 141/145
95501/95501 - 13s - loss: 1.4051 - val_loss: 1.4072 - 13s/epoch - 134us/sample
Epoch 142/145
95501/95501 - 13s - loss: 1.4054 - val_loss: 1.4076 - 13s/epoch - 134us/sample
Epoch 143/145
95501/95501 - 13s - loss: 1.4053 - val_loss: 1.4075 - 13s/epoch - 134us/sample
Epoch 144/145
95501/95501 - 13s - loss: 1.4050 - val_loss: 1.4070 - 13s/epoch - 134us/sample
Epoch 145/145
95501/95501 - 13s - loss: 1.4048 - val_loss: 1.4071 - 13s/epoch - 134us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 1.4071331058768923
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 02:05:17.043928: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_19/outputlayer/BiasAdd' id:24944 op device:{requested: '', assigned: ''} def:{{{node decoder_model_19/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_19/outputlayer/MatMul, decoder_model_19/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.7507504161729527
cosine 0.9217853555526727
MAE: 7.6128364
RMSE: 17.237745
r2: -19271.601345871826
RMSE zero-vector: 0.23411466903540806
['1.6custom_VAE', 'binary_crossentropy', 64, 145, 0.0014, 0.6, 758, 1.404821552122687, 1.4071331058768923, 0.7507504161729527, 0.9217853555526727, 7.6128363609313965, 17.23774528503418, -19271.601345871826, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.7000000000000002 145 0.0014 8 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2148)         2717220     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_60 (BatchN  (None, 2148)        8592        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_60 (ReLU)                (None, 2148)         0           ['batch_normalization_60[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1628942     ['re_lu_60[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1628942     ['re_lu_60[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4933614     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 10,917,310
Trainable params: 10,907,202
Non-trainable params: 10,108
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-15 02:05:30.087206: W tensorflow/c/c_api.cc:291] Operation '{name:'training_40/Adam/dense_enc0_20/kernel/m/Assign' id:26772 op device:{requested: '', assigned: ''} def:{{{node training_40/Adam/dense_enc0_20/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_40/Adam/dense_enc0_20/kernel/m, training_40/Adam/dense_enc0_20/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 02:06:43.879165: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_20/mul' id:26316 op device:{requested: '', assigned: ''} def:{{{node loss_20/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_20/mul/x, loss_20/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 80s - loss: 0.0104 - val_loss: 0.0073 - 80s/epoch - 842us/sample
Epoch 2/145
95501/95501 - 73s - loss: 0.0051 - val_loss: 0.0100 - 73s/epoch - 767us/sample
Epoch 3/145
95501/95501 - 73s - loss: 0.0043 - val_loss: 0.0116 - 73s/epoch - 766us/sample
Epoch 4/145
95501/95501 - 73s - loss: 0.0039 - val_loss: 0.0067 - 73s/epoch - 767us/sample
Epoch 5/145
95501/95501 - 73s - loss: 0.0036 - val_loss: 0.0077 - 73s/epoch - 768us/sample
Epoch 6/145
95501/95501 - 73s - loss: 0.0034 - val_loss: 0.0080 - 73s/epoch - 766us/sample
Epoch 7/145
95501/95501 - 72s - loss: 0.0033 - val_loss: 0.0101 - 72s/epoch - 752us/sample
Epoch 8/145
95501/95501 - 69s - loss: 0.0031 - val_loss: 0.0073 - 69s/epoch - 720us/sample
Epoch 9/145
95501/95501 - 69s - loss: 0.0031 - val_loss: 0.0103 - 69s/epoch - 720us/sample
Epoch 10/145
95501/95501 - 69s - loss: 0.0030 - val_loss: 0.0084 - 69s/epoch - 721us/sample
Epoch 11/145
95501/95501 - 69s - loss: 0.0029 - val_loss: 0.0077 - 69s/epoch - 721us/sample
Epoch 12/145
95501/95501 - 69s - loss: 0.0029 - val_loss: 0.0072 - 69s/epoch - 721us/sample
Epoch 13/145
95501/95501 - 69s - loss: 0.0029 - val_loss: 0.0069 - 69s/epoch - 721us/sample
Epoch 14/145
95501/95501 - 69s - loss: 0.0028 - val_loss: 0.0071 - 69s/epoch - 721us/sample
Epoch 15/145
95501/95501 - 69s - loss: 0.0027 - val_loss: 0.0071 - 69s/epoch - 719us/sample
Epoch 16/145
95501/95501 - 69s - loss: 0.0027 - val_loss: 0.0066 - 69s/epoch - 722us/sample
Epoch 17/145
95501/95501 - 69s - loss: 0.0027 - val_loss: 0.0073 - 69s/epoch - 720us/sample
Epoch 18/145
95501/95501 - 69s - loss: 0.0027 - val_loss: 0.0081 - 69s/epoch - 720us/sample
Epoch 19/145
95501/95501 - 69s - loss: 0.0027 - val_loss: 0.0070 - 69s/epoch - 720us/sample
Epoch 20/145
95501/95501 - 69s - loss: 0.0026 - val_loss: 0.0069 - 69s/epoch - 720us/sample
Epoch 21/145
95501/95501 - 69s - loss: 0.0026 - val_loss: 0.0075 - 69s/epoch - 719us/sample
Epoch 22/145
95501/95501 - 69s - loss: 0.0026 - val_loss: 0.0067 - 69s/epoch - 719us/sample
Epoch 23/145
95501/95501 - 69s - loss: 0.0025 - val_loss: 0.0087 - 69s/epoch - 722us/sample
Epoch 24/145
95501/95501 - 69s - loss: 0.0025 - val_loss: 0.0066 - 69s/epoch - 723us/sample
Epoch 25/145
95501/95501 - 69s - loss: 0.0025 - val_loss: 0.0067 - 69s/epoch - 722us/sample
Epoch 26/145
95501/95501 - 69s - loss: 0.0025 - val_loss: 0.0071 - 69s/epoch - 722us/sample
Epoch 27/145
95501/95501 - 69s - loss: 0.0024 - val_loss: 0.0064 - 69s/epoch - 722us/sample
Epoch 28/145
95501/95501 - 69s - loss: 0.0024 - val_loss: 0.0057 - 69s/epoch - 723us/sample
Epoch 29/145
95501/95501 - 69s - loss: 0.0024 - val_loss: 0.0059 - 69s/epoch - 723us/sample
Epoch 30/145
95501/95501 - 69s - loss: 0.0024 - val_loss: 0.0064 - 69s/epoch - 721us/sample
Epoch 31/145
95501/95501 - 69s - loss: 0.0024 - val_loss: 0.0059 - 69s/epoch - 721us/sample
Epoch 32/145
95501/95501 - 69s - loss: 0.0024 - val_loss: 0.0053 - 69s/epoch - 722us/sample
Epoch 33/145
95501/95501 - 69s - loss: 0.0023 - val_loss: 0.0073 - 69s/epoch - 721us/sample
Epoch 34/145
95501/95501 - 69s - loss: 0.0024 - val_loss: 0.0064 - 69s/epoch - 721us/sample
Epoch 35/145
95501/95501 - 69s - loss: 0.0024 - val_loss: 0.0058 - 69s/epoch - 723us/sample
Epoch 36/145
95501/95501 - 69s - loss: 0.0023 - val_loss: 0.0071 - 69s/epoch - 723us/sample
Epoch 37/145
95501/95501 - 69s - loss: 0.0023 - val_loss: 0.0060 - 69s/epoch - 722us/sample
Epoch 38/145
95501/95501 - 69s - loss: 0.0023 - val_loss: 0.0062 - 69s/epoch - 720us/sample
Epoch 39/145
95501/95501 - 69s - loss: 0.0023 - val_loss: 0.0067 - 69s/epoch - 722us/sample
Epoch 40/145
95501/95501 - 69s - loss: 0.0023 - val_loss: 0.0061 - 69s/epoch - 723us/sample
Epoch 41/145
95501/95501 - 69s - loss: 0.0023 - val_loss: 0.0059 - 69s/epoch - 722us/sample
Epoch 42/145
95501/95501 - 69s - loss: 0.0023 - val_loss: 0.0064 - 69s/epoch - 722us/sample
Epoch 43/145
95501/95501 - 69s - loss: 0.0023 - val_loss: 0.0060 - 69s/epoch - 721us/sample
Epoch 44/145
95501/95501 - 69s - loss: 0.0023 - val_loss: 0.0069 - 69s/epoch - 721us/sample
Epoch 45/145
95501/95501 - 69s - loss: 0.0022 - val_loss: 0.0065 - 69s/epoch - 721us/sample
Epoch 46/145
95501/95501 - 69s - loss: 0.0022 - val_loss: 0.0066 - 69s/epoch - 718us/sample
Epoch 47/145
95501/95501 - 62s - loss: 0.0022 - val_loss: 0.0066 - 62s/epoch - 648us/sample
Epoch 48/145
95501/95501 - 62s - loss: 0.0022 - val_loss: 0.0067 - 62s/epoch - 648us/sample
Epoch 49/145
95501/95501 - 62s - loss: 0.0022 - val_loss: 0.0071 - 62s/epoch - 648us/sample
Epoch 50/145
95501/95501 - 62s - loss: 0.0022 - val_loss: 0.0067 - 62s/epoch - 647us/sample
Epoch 51/145
95501/95501 - 62s - loss: 0.0022 - val_loss: 0.0063 - 62s/epoch - 646us/sample
Epoch 52/145
95501/95501 - 62s - loss: 0.0022 - val_loss: 0.0058 - 62s/epoch - 647us/sample
Epoch 53/145
95501/95501 - 62s - loss: 0.0023 - val_loss: 0.0056 - 62s/epoch - 647us/sample
Epoch 54/145
95501/95501 - 62s - loss: 0.0022 - val_loss: 0.0064 - 62s/epoch - 647us/sample
Epoch 55/145
95501/95501 - 62s - loss: 0.0022 - val_loss: 0.0062 - 62s/epoch - 645us/sample
Epoch 56/145
95501/95501 - 62s - loss: 0.0022 - val_loss: 0.0066 - 62s/epoch - 648us/sample
Epoch 57/145
95501/95501 - 62s - loss: 0.0022 - val_loss: 0.0059 - 62s/epoch - 647us/sample
Epoch 58/145
95501/95501 - 62s - loss: 0.0022 - val_loss: 0.0067 - 62s/epoch - 646us/sample
Epoch 59/145
95501/95501 - 62s - loss: 0.0022 - val_loss: 0.0065 - 62s/epoch - 647us/sample
Epoch 60/145
95501/95501 - 62s - loss: 0.0022 - val_loss: 0.0056 - 62s/epoch - 648us/sample
Epoch 61/145
95501/95501 - 62s - loss: 0.0022 - val_loss: 0.0062 - 62s/epoch - 648us/sample
Epoch 62/145
95501/95501 - 62s - loss: 0.0022 - val_loss: 0.0064 - 62s/epoch - 647us/sample
Epoch 63/145
95501/95501 - 62s - loss: 0.0022 - val_loss: 0.0058 - 62s/epoch - 647us/sample
Epoch 64/145
95501/95501 - 62s - loss: 0.0022 - val_loss: 0.0059 - 62s/epoch - 646us/sample
Epoch 65/145
95501/95501 - 62s - loss: 0.0022 - val_loss: 0.0071 - 62s/epoch - 647us/sample
Epoch 66/145
95501/95501 - 62s - loss: 0.0022 - val_loss: 0.0064 - 62s/epoch - 648us/sample
Epoch 67/145
95501/95501 - 62s - loss: 0.0022 - val_loss: 0.0057 - 62s/epoch - 647us/sample
Epoch 68/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0066 - 62s/epoch - 646us/sample
Epoch 69/145
95501/95501 - 62s - loss: 0.0022 - val_loss: 0.0061 - 62s/epoch - 646us/sample
Epoch 70/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0069 - 62s/epoch - 647us/sample
Epoch 71/145
95501/95501 - 62s - loss: 0.0022 - val_loss: 0.0066 - 62s/epoch - 646us/sample
Epoch 72/145
95501/95501 - 62s - loss: 0.0022 - val_loss: 0.0066 - 62s/epoch - 647us/sample
Epoch 73/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0071 - 62s/epoch - 648us/sample
Epoch 74/145
95501/95501 - 62s - loss: 0.0022 - val_loss: 0.0067 - 62s/epoch - 648us/sample
Epoch 75/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0061 - 62s/epoch - 646us/sample
Epoch 76/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0058 - 62s/epoch - 647us/sample
Epoch 77/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0072 - 62s/epoch - 647us/sample
Epoch 78/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0065 - 62s/epoch - 647us/sample
Epoch 79/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0067 - 62s/epoch - 646us/sample
Epoch 80/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0065 - 62s/epoch - 648us/sample
Epoch 81/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0077 - 62s/epoch - 648us/sample
Epoch 82/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0065 - 62s/epoch - 646us/sample
Epoch 83/145
95501/95501 - 62s - loss: 0.0022 - val_loss: 0.0062 - 62s/epoch - 646us/sample
Epoch 84/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0068 - 62s/epoch - 647us/sample
Epoch 85/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0068 - 62s/epoch - 646us/sample
Epoch 86/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0067 - 62s/epoch - 647us/sample
Epoch 87/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0069 - 62s/epoch - 646us/sample
Epoch 88/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0065 - 62s/epoch - 648us/sample
Epoch 89/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0065 - 62s/epoch - 646us/sample
Epoch 90/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0076 - 62s/epoch - 648us/sample
Epoch 91/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0074 - 62s/epoch - 646us/sample
Epoch 92/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0073 - 62s/epoch - 646us/sample
Epoch 93/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0066 - 62s/epoch - 648us/sample
Epoch 94/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0075 - 62s/epoch - 647us/sample
Epoch 95/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0074 - 62s/epoch - 646us/sample
Epoch 96/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0065 - 62s/epoch - 647us/sample
Epoch 97/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0070 - 62s/epoch - 647us/sample
Epoch 98/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0071 - 62s/epoch - 648us/sample
Epoch 99/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0082 - 62s/epoch - 645us/sample
Epoch 100/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0071 - 62s/epoch - 648us/sample
Epoch 101/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0088 - 62s/epoch - 647us/sample
Epoch 102/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0074 - 62s/epoch - 647us/sample
Epoch 103/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0068 - 62s/epoch - 645us/sample
Epoch 104/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0078 - 62s/epoch - 647us/sample
Epoch 105/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0062 - 62s/epoch - 647us/sample
Epoch 106/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0070 - 62s/epoch - 647us/sample
Epoch 107/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0074 - 62s/epoch - 646us/sample
Epoch 108/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0069 - 62s/epoch - 648us/sample
Epoch 109/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0067 - 62s/epoch - 646us/sample
Epoch 110/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0073 - 62s/epoch - 647us/sample
Epoch 111/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0073 - 62s/epoch - 646us/sample
Epoch 112/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0073 - 62s/epoch - 648us/sample
Epoch 113/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0065 - 62s/epoch - 647us/sample
Epoch 114/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0070 - 62s/epoch - 647us/sample
Epoch 115/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0082 - 62s/epoch - 646us/sample
Epoch 116/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0095 - 62s/epoch - 648us/sample
Epoch 117/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0080 - 62s/epoch - 646us/sample
Epoch 118/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0083 - 62s/epoch - 648us/sample
Epoch 119/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0070 - 62s/epoch - 647us/sample
Epoch 120/145
95501/95501 - 62s - loss: 0.0021 - val_loss: 0.0073 - 62s/epoch - 646us/sample
Epoch 121/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0076 - 62s/epoch - 650us/sample
Epoch 122/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0076 - 62s/epoch - 649us/sample
Epoch 123/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0068 - 62s/epoch - 647us/sample
Epoch 124/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0085 - 62s/epoch - 648us/sample
Epoch 125/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0081 - 62s/epoch - 646us/sample
Epoch 126/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0077 - 62s/epoch - 646us/sample
Epoch 127/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0082 - 62s/epoch - 647us/sample
Epoch 128/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0098 - 62s/epoch - 647us/sample
Epoch 129/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0067 - 62s/epoch - 646us/sample
Epoch 130/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0077 - 62s/epoch - 647us/sample
Epoch 131/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0063 - 62s/epoch - 647us/sample
Epoch 132/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0085 - 62s/epoch - 646us/sample
Epoch 133/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0067 - 62s/epoch - 647us/sample
Epoch 134/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0089 - 62s/epoch - 647us/sample
Epoch 135/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0070 - 62s/epoch - 646us/sample
Epoch 136/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0081 - 62s/epoch - 646us/sample
Epoch 137/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0083 - 62s/epoch - 647us/sample
Epoch 138/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0080 - 62s/epoch - 647us/sample
Epoch 139/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0083 - 62s/epoch - 646us/sample
Epoch 140/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0085 - 62s/epoch - 646us/sample
Epoch 141/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0118 - 62s/epoch - 647us/sample
Epoch 142/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0085 - 62s/epoch - 647us/sample
Epoch 143/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0076 - 62s/epoch - 646us/sample
Epoch 144/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0086 - 62s/epoch - 648us/sample
Epoch 145/145
95501/95501 - 62s - loss: 0.0020 - val_loss: 0.0083 - 62s/epoch - 646us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.008291511204640624
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 04:40:48.518420: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_20/outputlayer/BiasAdd' id:26287 op device:{requested: '', assigned: ''} def:{{{node decoder_model_20/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_20/outputlayer/MatMul, decoder_model_20/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.0369649353190341
cosine 0.03135513277395105
MAE: 0.027515981
RMSE: 0.08915332
r2: 0.4843707810996535
RMSE zero-vector: 0.23411466903540806
['1.7000000000000002custom_VAE', 'mse', 8, 145, 0.0014, 0.6, 758, 0.002001043074587401, 0.008291511204640624, 0.0369649353190341, 0.03135513277395105, 0.02751598134636879, 0.08915331959724426, 0.4843707810996535, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 85 0.001 64 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_63 (BatchN  (None, 1769)        7076        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_63 (ReLU)                (None, 1769)         0           ['batch_normalization_63[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1341660     ['re_lu_63[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1341660     ['re_lu_63[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4165381     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9,093,562
Trainable params: 9,084,970
Non-trainable params: 8,592
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/85
2023-02-15 04:41:01.742781: W tensorflow/c/c_api.cc:291] Operation '{name:'training_42/Adam/bottleneck_zlog_21/kernel/v/Assign' id:28189 op device:{requested: '', assigned: ''} def:{{{node training_42/Adam/bottleneck_zlog_21/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_42/Adam/bottleneck_zlog_21/kernel/v, training_42/Adam/bottleneck_zlog_21/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 04:41:16.292748: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_21/mul' id:27580 op device:{requested: '', assigned: ''} def:{{{node loss_21/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_21/mul/x, loss_21/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 19s - loss: 0.0104 - val_loss: 0.0041 - 19s/epoch - 200us/sample
Epoch 2/85
95501/95501 - 12s - loss: 0.0044 - val_loss: 0.0038 - 12s/epoch - 126us/sample
Epoch 3/85
95501/95501 - 12s - loss: 0.0038 - val_loss: 0.0031 - 12s/epoch - 126us/sample
Epoch 4/85
95501/95501 - 12s - loss: 0.0032 - val_loss: 0.0025 - 12s/epoch - 126us/sample
Epoch 5/85
95501/95501 - 12s - loss: 0.0028 - val_loss: 0.0026 - 12s/epoch - 126us/sample
Epoch 6/85
95501/95501 - 12s - loss: 0.0025 - val_loss: 0.0020 - 12s/epoch - 126us/sample
Epoch 7/85
95501/95501 - 12s - loss: 0.0021 - val_loss: 0.0017 - 12s/epoch - 127us/sample
Epoch 8/85
95501/95501 - 12s - loss: 0.0019 - val_loss: 0.0018 - 12s/epoch - 127us/sample
Epoch 9/85
95501/95501 - 12s - loss: 0.0018 - val_loss: 0.0014 - 12s/epoch - 126us/sample
Epoch 10/85
95501/95501 - 12s - loss: 0.0016 - val_loss: 0.0014 - 12s/epoch - 126us/sample
Epoch 11/85
95501/95501 - 12s - loss: 0.0014 - val_loss: 0.0012 - 12s/epoch - 126us/sample
Epoch 12/85
95501/95501 - 12s - loss: 0.0013 - val_loss: 0.0012 - 12s/epoch - 126us/sample
Epoch 13/85
95501/95501 - 12s - loss: 0.0013 - val_loss: 0.0011 - 12s/epoch - 126us/sample
Epoch 14/85
95501/95501 - 12s - loss: 0.0012 - val_loss: 0.0011 - 12s/epoch - 127us/sample
Epoch 15/85
95501/95501 - 12s - loss: 0.0012 - val_loss: 0.0011 - 12s/epoch - 126us/sample
Epoch 16/85
95501/95501 - 12s - loss: 0.0011 - val_loss: 0.0010 - 12s/epoch - 126us/sample
Epoch 17/85
95501/95501 - 12s - loss: 0.0011 - val_loss: 0.0015 - 12s/epoch - 126us/sample
Epoch 18/85
95501/95501 - 12s - loss: 0.0012 - val_loss: 0.0012 - 12s/epoch - 126us/sample
Epoch 19/85
95501/95501 - 12s - loss: 0.0012 - val_loss: 9.6847e-04 - 12s/epoch - 126us/sample
Epoch 20/85
95501/95501 - 12s - loss: 0.0010 - val_loss: 0.0012 - 12s/epoch - 126us/sample
Epoch 21/85
95501/95501 - 12s - loss: 0.0011 - val_loss: 9.3034e-04 - 12s/epoch - 127us/sample
Epoch 22/85
95501/95501 - 12s - loss: 0.0010 - val_loss: 9.1840e-04 - 12s/epoch - 126us/sample
Epoch 23/85
95501/95501 - 12s - loss: 9.8018e-04 - val_loss: 9.4488e-04 - 12s/epoch - 126us/sample
Epoch 24/85
95501/95501 - 12s - loss: 0.0010 - val_loss: 9.2882e-04 - 12s/epoch - 127us/sample
Epoch 25/85
95501/95501 - 12s - loss: 9.5952e-04 - val_loss: 8.7318e-04 - 12s/epoch - 126us/sample
Epoch 26/85
95501/95501 - 12s - loss: 9.4619e-04 - val_loss: 9.0811e-04 - 12s/epoch - 126us/sample
Epoch 27/85
95501/95501 - 12s - loss: 9.3822e-04 - val_loss: 9.4544e-04 - 12s/epoch - 127us/sample
Epoch 28/85
95501/95501 - 12s - loss: 9.1450e-04 - val_loss: 8.5405e-04 - 12s/epoch - 127us/sample
Epoch 29/85
95501/95501 - 12s - loss: 9.1780e-04 - val_loss: 8.3349e-04 - 12s/epoch - 126us/sample
Epoch 30/85
95501/95501 - 12s - loss: 9.0041e-04 - val_loss: 8.5019e-04 - 12s/epoch - 127us/sample
Epoch 31/85
95501/95501 - 12s - loss: 9.1348e-04 - val_loss: 8.3574e-04 - 12s/epoch - 126us/sample
Epoch 32/85
95501/95501 - 12s - loss: 8.8245e-04 - val_loss: 0.0010 - 12s/epoch - 126us/sample
Epoch 33/85
95501/95501 - 12s - loss: 0.0010 - val_loss: 8.3862e-04 - 12s/epoch - 126us/sample
Epoch 34/85
95501/95501 - 12s - loss: 8.7518e-04 - val_loss: 8.6440e-04 - 12s/epoch - 127us/sample
Epoch 35/85
95501/95501 - 12s - loss: 8.8537e-04 - val_loss: 8.0103e-04 - 12s/epoch - 127us/sample
Epoch 36/85
95501/95501 - 12s - loss: 8.6742e-04 - val_loss: 8.3446e-04 - 12s/epoch - 126us/sample
Epoch 37/85
95501/95501 - 12s - loss: 8.8366e-04 - val_loss: 7.8806e-04 - 12s/epoch - 126us/sample
Epoch 38/85
95501/95501 - 12s - loss: 8.5040e-04 - val_loss: 8.6412e-04 - 12s/epoch - 126us/sample
Epoch 39/85
95501/95501 - 12s - loss: 9.4102e-04 - val_loss: 8.8792e-04 - 12s/epoch - 126us/sample
Epoch 40/85
95501/95501 - 12s - loss: 9.3503e-04 - val_loss: 7.9936e-04 - 12s/epoch - 127us/sample
Epoch 41/85
95501/95501 - 12s - loss: 8.4628e-04 - val_loss: 7.8076e-04 - 12s/epoch - 127us/sample
Epoch 42/85
95501/95501 - 12s - loss: 8.4774e-04 - val_loss: 7.8282e-04 - 12s/epoch - 126us/sample
Epoch 43/85
95501/95501 - 12s - loss: 8.3256e-04 - val_loss: 7.8858e-04 - 12s/epoch - 126us/sample
Epoch 44/85
95501/95501 - 12s - loss: 8.3264e-04 - val_loss: 0.0011 - 12s/epoch - 126us/sample
Epoch 45/85
95501/95501 - 12s - loss: 0.0010 - val_loss: 7.8084e-04 - 12s/epoch - 126us/sample
Epoch 46/85
95501/95501 - 12s - loss: 8.3645e-04 - val_loss: 7.6540e-04 - 12s/epoch - 126us/sample
Epoch 47/85
95501/95501 - 12s - loss: 8.5219e-04 - val_loss: 7.7434e-04 - 12s/epoch - 128us/sample
Epoch 48/85
95501/95501 - 12s - loss: 8.1688e-04 - val_loss: 7.6167e-04 - 12s/epoch - 126us/sample
Epoch 49/85
95501/95501 - 12s - loss: 8.1372e-04 - val_loss: 7.7797e-04 - 12s/epoch - 126us/sample
Epoch 50/85
95501/95501 - 12s - loss: 8.1704e-04 - val_loss: 7.5660e-04 - 12s/epoch - 126us/sample
Epoch 51/85
95501/95501 - 12s - loss: 8.0442e-04 - val_loss: 7.4265e-04 - 12s/epoch - 126us/sample
Epoch 52/85
95501/95501 - 12s - loss: 8.0420e-04 - val_loss: 7.5039e-04 - 12s/epoch - 126us/sample
Epoch 53/85
95501/95501 - 12s - loss: 8.1259e-04 - val_loss: 8.4284e-04 - 12s/epoch - 127us/sample
Epoch 54/85
95501/95501 - 12s - loss: 8.8254e-04 - val_loss: 7.5600e-04 - 12s/epoch - 127us/sample
Epoch 55/85
95501/95501 - 12s - loss: 8.0527e-04 - val_loss: 7.4690e-04 - 12s/epoch - 126us/sample
Epoch 56/85
95501/95501 - 12s - loss: 8.0153e-04 - val_loss: 0.0012 - 12s/epoch - 126us/sample
Epoch 57/85
95501/95501 - 12s - loss: 9.6154e-04 - val_loss: 7.5355e-04 - 12s/epoch - 126us/sample
Epoch 58/85
95501/95501 - 12s - loss: 8.0375e-04 - val_loss: 7.4208e-04 - 12s/epoch - 126us/sample
Epoch 59/85
95501/95501 - 12s - loss: 7.9256e-04 - val_loss: 7.7280e-04 - 12s/epoch - 127us/sample
Epoch 60/85
95501/95501 - 12s - loss: 8.2589e-04 - val_loss: 9.1007e-04 - 12s/epoch - 127us/sample
Epoch 61/85
95501/95501 - 12s - loss: 9.5288e-04 - val_loss: 8.7896e-04 - 12s/epoch - 127us/sample
Epoch 62/85
95501/95501 - 12s - loss: 8.9490e-04 - val_loss: 8.6604e-04 - 12s/epoch - 126us/sample
Epoch 63/85
95501/95501 - 12s - loss: 9.3230e-04 - val_loss: 9.7807e-04 - 12s/epoch - 126us/sample
Epoch 64/85
95501/95501 - 12s - loss: 9.1396e-04 - val_loss: 7.6245e-04 - 12s/epoch - 127us/sample
Epoch 65/85
95501/95501 - 12s - loss: 8.1849e-04 - val_loss: 7.4826e-04 - 12s/epoch - 126us/sample
Epoch 66/85
95501/95501 - 12s - loss: 7.9198e-04 - val_loss: 7.4285e-04 - 12s/epoch - 126us/sample
Epoch 67/85
95501/95501 - 12s - loss: 7.8431e-04 - val_loss: 8.2018e-04 - 12s/epoch - 127us/sample
Epoch 68/85
95501/95501 - 12s - loss: 8.5571e-04 - val_loss: 8.2139e-04 - 12s/epoch - 127us/sample
Epoch 69/85
95501/95501 - 12s - loss: 8.6571e-04 - val_loss: 7.9645e-04 - 12s/epoch - 126us/sample
Epoch 70/85
95501/95501 - 12s - loss: 8.2470e-04 - val_loss: 7.6636e-04 - 12s/epoch - 127us/sample
Epoch 71/85
95501/95501 - 12s - loss: 8.1746e-04 - val_loss: 7.2784e-04 - 12s/epoch - 126us/sample
Epoch 72/85
95501/95501 - 12s - loss: 7.7995e-04 - val_loss: 7.3696e-04 - 12s/epoch - 126us/sample
Epoch 73/85
95501/95501 - 12s - loss: 7.9115e-04 - val_loss: 7.3281e-04 - 12s/epoch - 126us/sample
Epoch 74/85
95501/95501 - 12s - loss: 8.2377e-04 - val_loss: 7.2495e-04 - 12s/epoch - 127us/sample
Epoch 75/85
95501/95501 - 12s - loss: 7.7075e-04 - val_loss: 7.6668e-04 - 12s/epoch - 126us/sample
Epoch 76/85
95501/95501 - 12s - loss: 7.9618e-04 - val_loss: 7.1992e-04 - 12s/epoch - 127us/sample
Epoch 77/85
95501/95501 - 12s - loss: 7.6806e-04 - val_loss: 7.2560e-04 - 12s/epoch - 126us/sample
Epoch 78/85
95501/95501 - 12s - loss: 7.5877e-04 - val_loss: 7.2572e-04 - 12s/epoch - 126us/sample
Epoch 79/85
95501/95501 - 12s - loss: 7.8272e-04 - val_loss: 7.9580e-04 - 12s/epoch - 126us/sample
Epoch 80/85
95501/95501 - 12s - loss: 8.2177e-04 - val_loss: 7.1284e-04 - 12s/epoch - 127us/sample
Epoch 81/85
95501/95501 - 12s - loss: 7.5897e-04 - val_loss: 7.0051e-04 - 12s/epoch - 127us/sample
Epoch 82/85
95501/95501 - 12s - loss: 7.5785e-04 - val_loss: 7.1897e-04 - 12s/epoch - 127us/sample
Epoch 83/85
95501/95501 - 12s - loss: 7.5234e-04 - val_loss: 7.0798e-04 - 12s/epoch - 126us/sample
Epoch 84/85
95501/95501 - 12s - loss: 7.5450e-04 - val_loss: 7.0672e-04 - 12s/epoch - 126us/sample
Epoch 85/85
95501/95501 - 12s - loss: 7.4742e-04 - val_loss: 7.4412e-04 - 12s/epoch - 126us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0007441170250389991
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 04:58:12.895061: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_21/outputlayer/BiasAdd' id:27551 op device:{requested: '', assigned: ''} def:{{{node decoder_model_21/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_21/outputlayer/MatMul, decoder_model_21/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.005090295987627077
cosine 0.004023298789679308
MAE: 0.011471025
RMSE: 0.02105853
r2: 0.9712320228800034
RMSE zero-vector: 0.23411466903540806
['1.4custom_VAE', 'mse', 64, 85, 0.001, 0.6, 758, 0.0007474184401622068, 0.0007441170250389991, 0.005090295987627077, 0.004023298789679308, 0.011471024714410305, 0.02105852961540222, 0.9712320228800034, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 85 0.0012 64 0] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_66 (BatchN  (None, 3160)        12640       ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_66 (ReLU)                (None, 3160)         0           ['batch_normalization_66[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2396038     ['re_lu_66[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2396038     ['re_lu_66[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         6984938     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 15,787,054
Trainable params: 15,772,898
Non-trainable params: 14,156
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/85
2023-02-15 04:58:26.316893: W tensorflow/c/c_api.cc:291] Operation '{name:'training_44/Adam/bottleneck_zmean_22/kernel/v/Assign' id:29520 op device:{requested: '', assigned: ''} def:{{{node training_44/Adam/bottleneck_zmean_22/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_44/Adam/bottleneck_zmean_22/kernel/v, training_44/Adam/bottleneck_zmean_22/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 04:58:41.695914: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_22/mul' id:28860 op device:{requested: '', assigned: ''} def:{{{node loss_22/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_22/mul/x, loss_22/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 20s - loss: 3.8917 - val_loss: 1.4444 - 20s/epoch - 211us/sample
Epoch 2/85
95501/95501 - 13s - loss: 1.4412 - val_loss: 1.4365 - 13s/epoch - 135us/sample
Epoch 3/85
95501/95501 - 13s - loss: 1.4418 - val_loss: 1.4488 - 13s/epoch - 135us/sample
Epoch 4/85
95501/95501 - 13s - loss: 1.4529 - val_loss: 1.5662 - 13s/epoch - 135us/sample
Epoch 5/85
95501/95501 - 13s - loss: 1.4556 - val_loss: 1.4779 - 13s/epoch - 135us/sample
Epoch 6/85
95501/95501 - 13s - loss: 1.4901 - val_loss: 1.4371 - 13s/epoch - 135us/sample
Epoch 7/85
95501/95501 - 13s - loss: 1.4407 - val_loss: 1.4449 - 13s/epoch - 135us/sample
Epoch 8/85
95501/95501 - 13s - loss: 1.4454 - val_loss: 1.4344 - 13s/epoch - 135us/sample
Epoch 9/85
95501/95501 - 13s - loss: 1.4327 - val_loss: 1.4376 - 13s/epoch - 134us/sample
Epoch 10/85
95501/95501 - 13s - loss: 1.4535 - val_loss: 1.4305 - 13s/epoch - 135us/sample
Epoch 11/85
95501/95501 - 13s - loss: 1.4399 - val_loss: 1.4348 - 13s/epoch - 135us/sample
Epoch 12/85
95501/95501 - 13s - loss: 1.4482 - val_loss: 1.4367 - 13s/epoch - 135us/sample
Epoch 13/85
95501/95501 - 13s - loss: 2.2543 - val_loss: 1.4512 - 13s/epoch - 135us/sample
Epoch 14/85
95501/95501 - 13s - loss: 1.4384 - val_loss: 1.4396 - 13s/epoch - 136us/sample
Epoch 15/85
95501/95501 - 13s - loss: 1.4563 - val_loss: 1.4785 - 13s/epoch - 135us/sample
Epoch 16/85
95501/95501 - 13s - loss: 1.4385 - val_loss: 1.4267 - 13s/epoch - 135us/sample
Epoch 17/85
95501/95501 - 13s - loss: 1.4361 - val_loss: 1.4308 - 13s/epoch - 135us/sample
Epoch 18/85
95501/95501 - 13s - loss: 1.4268 - val_loss: 1.4291 - 13s/epoch - 135us/sample
Epoch 19/85
95501/95501 - 13s - loss: 1.4699 - val_loss: 62.0846 - 13s/epoch - 135us/sample
Epoch 20/85
95501/95501 - 13s - loss: 1.4238 - val_loss: 1.4481 - 13s/epoch - 136us/sample
Epoch 21/85
95501/95501 - 13s - loss: 1.4248 - val_loss: 1.4217 - 13s/epoch - 135us/sample
Epoch 22/85
95501/95501 - 13s - loss: 1.4294 - val_loss: 1.4286 - 13s/epoch - 135us/sample
Epoch 23/85
95501/95501 - 13s - loss: 1.4366 - val_loss: 1.4236 - 13s/epoch - 135us/sample
Epoch 24/85
95501/95501 - 13s - loss: 1.4213 - val_loss: 1.4213 - 13s/epoch - 135us/sample
Epoch 25/85
95501/95501 - 13s - loss: 1.4333 - val_loss: 1.4647 - 13s/epoch - 135us/sample
Epoch 26/85
95501/95501 - 13s - loss: 1.4607 - val_loss: 1.4359 - 13s/epoch - 135us/sample
Epoch 27/85
95501/95501 - 13s - loss: 1.4285 - val_loss: 1.4248 - 13s/epoch - 135us/sample
Epoch 28/85
95501/95501 - 13s - loss: 1.4246 - val_loss: 1.4221 - 13s/epoch - 135us/sample
Epoch 29/85
95501/95501 - 13s - loss: 1.4242 - val_loss: 1.4224 - 13s/epoch - 135us/sample
Epoch 30/85
95501/95501 - 13s - loss: 1.4249 - val_loss: 1.4300 - 13s/epoch - 135us/sample
Epoch 31/85
95501/95501 - 13s - loss: 1.4233 - val_loss: 1.4227 - 13s/epoch - 135us/sample
Epoch 32/85
95501/95501 - 13s - loss: 1.4519 - val_loss: 1.4261 - 13s/epoch - 135us/sample
Epoch 33/85
95501/95501 - 13s - loss: 1.4235 - val_loss: 1.4228 - 13s/epoch - 136us/sample
Epoch 34/85
95501/95501 - 13s - loss: 1.4225 - val_loss: 1.4196 - 13s/epoch - 135us/sample
Epoch 35/85
95501/95501 - 13s - loss: 1.4450 - val_loss: 1.4207 - 13s/epoch - 135us/sample
Epoch 36/85
95501/95501 - 13s - loss: 1.4224 - val_loss: 1.4225 - 13s/epoch - 135us/sample
Epoch 37/85
95501/95501 - 13s - loss: 1.4626 - val_loss: 1.4224 - 13s/epoch - 135us/sample
Epoch 38/85
95501/95501 - 13s - loss: 1.4194 - val_loss: 1.4200 - 13s/epoch - 135us/sample
Epoch 39/85
95501/95501 - 13s - loss: 1.4245 - val_loss: 1.6038 - 13s/epoch - 136us/sample
Epoch 40/85
95501/95501 - 13s - loss: 1.4371 - val_loss: 1.4234 - 13s/epoch - 135us/sample
Epoch 41/85
95501/95501 - 13s - loss: 1.4228 - val_loss: 1.4240 - 13s/epoch - 135us/sample
Epoch 42/85
95501/95501 - 13s - loss: 1.4221 - val_loss: 1.4235 - 13s/epoch - 135us/sample
Epoch 43/85
95501/95501 - 13s - loss: 1.4220 - val_loss: 1.4241 - 13s/epoch - 135us/sample
Epoch 44/85
95501/95501 - 13s - loss: 1.4514 - val_loss: 1.4281 - 13s/epoch - 135us/sample
Epoch 45/85
95501/95501 - 13s - loss: 1.4232 - val_loss: 1.4220 - 13s/epoch - 135us/sample
Epoch 46/85
95501/95501 - 13s - loss: 1.4227 - val_loss: 1.4217 - 13s/epoch - 136us/sample
Epoch 47/85
95501/95501 - 13s - loss: 2.2099 - val_loss: 1.4366 - 13s/epoch - 135us/sample
Epoch 48/85
95501/95501 - 13s - loss: 1.4211 - val_loss: 1.4210 - 13s/epoch - 135us/sample
Epoch 49/85
95501/95501 - 13s - loss: 1.4223 - val_loss: 1.4204 - 13s/epoch - 135us/sample
Epoch 50/85
95501/95501 - 13s - loss: 1.4261 - val_loss: 1.4293 - 13s/epoch - 135us/sample
Epoch 51/85
95501/95501 - 13s - loss: 1.4238 - val_loss: 1.4697 - 13s/epoch - 135us/sample
Epoch 52/85
95501/95501 - 13s - loss: 1.4232 - val_loss: 1.4356 - 13s/epoch - 136us/sample
Epoch 53/85
95501/95501 - 13s - loss: 3.9212 - val_loss: 1.4205 - 13s/epoch - 135us/sample
Epoch 54/85
95501/95501 - 13s - loss: 1.4189 - val_loss: 1.4202 - 13s/epoch - 135us/sample
Epoch 55/85
95501/95501 - 13s - loss: 1.4284 - val_loss: 1.4514 - 13s/epoch - 135us/sample
Epoch 56/85
95501/95501 - 13s - loss: 2.6993 - val_loss: 1.4195 - 13s/epoch - 135us/sample
Epoch 57/85
95501/95501 - 13s - loss: 1.5253 - val_loss: 1.4261 - 13s/epoch - 135us/sample
Epoch 58/85
95501/95501 - 13s - loss: 1.4244 - val_loss: 1.4197 - 13s/epoch - 135us/sample
Epoch 59/85
95501/95501 - 13s - loss: 1.4214 - val_loss: 1.4209 - 13s/epoch - 136us/sample
Epoch 60/85
95501/95501 - 13s - loss: 1.4317 - val_loss: 1.4306 - 13s/epoch - 135us/sample
Epoch 61/85
95501/95501 - 13s - loss: 1.4412 - val_loss: 1.4272 - 13s/epoch - 135us/sample
Epoch 62/85
95501/95501 - 13s - loss: 1.4236 - val_loss: 1.4221 - 13s/epoch - 135us/sample
Epoch 63/85
95501/95501 - 13s - loss: 1.4211 - val_loss: 1.4241 - 13s/epoch - 135us/sample
Epoch 64/85
95501/95501 - 13s - loss: 1.4303 - val_loss: 1.4288 - 13s/epoch - 135us/sample
Epoch 65/85
95501/95501 - 13s - loss: 1.4232 - val_loss: 1.4449 - 13s/epoch - 136us/sample
Epoch 66/85
95501/95501 - 13s - loss: 6.4176 - val_loss: 1.4289 - 13s/epoch - 135us/sample
Epoch 67/85
95501/95501 - 13s - loss: 1.4230 - val_loss: 1.4216 - 13s/epoch - 135us/sample
Epoch 68/85
95501/95501 - 13s - loss: 1.4222 - val_loss: 1.4245 - 13s/epoch - 135us/sample
Epoch 69/85
95501/95501 - 13s - loss: 1.4374 - val_loss: 1.4672 - 13s/epoch - 135us/sample
Epoch 70/85
95501/95501 - 13s - loss: 1.4318 - val_loss: 1.4249 - 13s/epoch - 135us/sample
Epoch 71/85
95501/95501 - 13s - loss: 1.4201 - val_loss: 1.4640 - 13s/epoch - 136us/sample
Epoch 72/85
95501/95501 - 13s - loss: 1.4258 - val_loss: 1.4200 - 13s/epoch - 135us/sample
Epoch 73/85
95501/95501 - 13s - loss: 1.4195 - val_loss: 1.4285 - 13s/epoch - 135us/sample
Epoch 74/85
95501/95501 - 13s - loss: 1.4248 - val_loss: 1.4244 - 13s/epoch - 135us/sample
Epoch 75/85
95501/95501 - 13s - loss: 1.4266 - val_loss: 1.4236 - 13s/epoch - 135us/sample
Epoch 76/85
95501/95501 - 13s - loss: 1.4464 - val_loss: 1.4218 - 13s/epoch - 135us/sample
Epoch 77/85
95501/95501 - 13s - loss: 1.6309 - val_loss: 1.4357 - 13s/epoch - 135us/sample
Epoch 78/85
95501/95501 - 13s - loss: 1.4232 - val_loss: 1.4212 - 13s/epoch - 136us/sample
Epoch 79/85
95501/95501 - 13s - loss: 1.4196 - val_loss: 1.4198 - 13s/epoch - 135us/sample
Epoch 80/85
95501/95501 - 13s - loss: 1.6503 - val_loss: 1.4205 - 13s/epoch - 135us/sample
Epoch 81/85
95501/95501 - 13s - loss: 1.4236 - val_loss: 1.4223 - 13s/epoch - 135us/sample
Epoch 82/85
95501/95501 - 13s - loss: 217.5891 - val_loss: 1.4411 - 13s/epoch - 135us/sample
Epoch 83/85
95501/95501 - 13s - loss: 1.4208 - val_loss: 1.4187 - 13s/epoch - 135us/sample
Epoch 84/85
95501/95501 - 13s - loss: 1.4205 - val_loss: 1.4259 - 13s/epoch - 135us/sample
Epoch 85/85
95501/95501 - 13s - loss: 1.5330 - val_loss: 1.4229 - 13s/epoch - 136us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 1.4228953773733628
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 05:16:46.927562: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_22/outputlayer/BiasAdd' id:28812 op device:{requested: '', assigned: ''} def:{{{node decoder_model_22/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_22/outputlayer/MatMul, decoder_model_22/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.687087016182966
cosine 0.8960834540926469
MAE: 9.160555
RMSE: 18.572508
r2: -22369.266210208625
RMSE zero-vector: 0.23411466903540806
['2.5custom_VAE', 'binary_crossentropy', 64, 85, 0.0012, 0.6, 758, 1.533016634969052, 1.4228953773733628, 0.687087016182966, 0.8960834540926469, 9.160554885864258, 18.572507858276367, -22369.266210208625, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 145 0.0014 16 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_69 (BatchN  (None, 1769)        7076        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_69 (ReLU)                (None, 1769)         0           ['batch_normalization_69[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1341660     ['re_lu_69[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1341660     ['re_lu_69[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4165381     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9,093,562
Trainable params: 9,084,970
Non-trainable params: 8,592
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-15 05:17:01.006741: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_70/gamma/Assign' id:29909 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_70/gamma/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_70/gamma, batch_normalization_70/gamma/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 05:17:39.105674: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_23/mul' id:30184 op device:{requested: '', assigned: ''} def:{{{node loss_23/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_23/mul/x, loss_23/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 44s - loss: 0.0089 - val_loss: 0.0038 - 44s/epoch - 458us/sample
Epoch 2/145
95501/95501 - 37s - loss: 0.0043 - val_loss: 0.0039 - 37s/epoch - 383us/sample
Epoch 3/145
95501/95501 - 36s - loss: 0.0032 - val_loss: 0.0024 - 36s/epoch - 381us/sample
Epoch 4/145
95501/95501 - 37s - loss: 0.0027 - val_loss: 0.0022 - 37s/epoch - 383us/sample
Epoch 5/145
95501/95501 - 36s - loss: 0.0024 - val_loss: 0.0020 - 36s/epoch - 381us/sample
Epoch 6/145
95501/95501 - 37s - loss: 0.0022 - val_loss: 0.0018 - 37s/epoch - 383us/sample
Epoch 7/145
95501/95501 - 37s - loss: 0.0020 - val_loss: 0.0017 - 37s/epoch - 382us/sample
Epoch 8/145
95501/95501 - 36s - loss: 0.0019 - val_loss: 0.0016 - 36s/epoch - 382us/sample
Epoch 9/145
95501/95501 - 37s - loss: 0.0018 - val_loss: 0.0015 - 37s/epoch - 383us/sample
Epoch 10/145
95501/95501 - 37s - loss: 0.0018 - val_loss: 0.0015 - 37s/epoch - 382us/sample
Epoch 11/145
95501/95501 - 37s - loss: 0.0017 - val_loss: 0.0014 - 37s/epoch - 383us/sample
Epoch 12/145
95501/95501 - 36s - loss: 0.0017 - val_loss: 0.0014 - 36s/epoch - 382us/sample
Epoch 13/145
95501/95501 - 37s - loss: 0.0017 - val_loss: 0.0014 - 37s/epoch - 383us/sample
Epoch 14/145
95501/95501 - 36s - loss: 0.0016 - val_loss: 0.0013 - 36s/epoch - 382us/sample
Epoch 15/145
95501/95501 - 37s - loss: 0.0016 - val_loss: 0.0013 - 37s/epoch - 383us/sample
Epoch 16/145
95501/95501 - 36s - loss: 0.0016 - val_loss: 0.0013 - 36s/epoch - 382us/sample
Epoch 17/145
95501/95501 - 37s - loss: 0.0015 - val_loss: 0.0013 - 37s/epoch - 383us/sample
Epoch 18/145
95501/95501 - 37s - loss: 0.0015 - val_loss: 0.0013 - 37s/epoch - 382us/sample
Epoch 19/145
95501/95501 - 37s - loss: 0.0015 - val_loss: 0.0012 - 37s/epoch - 383us/sample
Epoch 20/145
95501/95501 - 36s - loss: 0.0015 - val_loss: 0.0012 - 36s/epoch - 380us/sample
Epoch 21/145
95501/95501 - 37s - loss: 0.0014 - val_loss: 0.0012 - 37s/epoch - 388us/sample
Epoch 22/145
95501/95501 - 37s - loss: 0.0014 - val_loss: 0.0012 - 37s/epoch - 383us/sample
Epoch 23/145
95501/95501 - 37s - loss: 0.0014 - val_loss: 0.0012 - 37s/epoch - 386us/sample
Epoch 24/145
95501/95501 - 37s - loss: 0.0014 - val_loss: 0.0012 - 37s/epoch - 383us/sample
Epoch 25/145
95501/95501 - 37s - loss: 0.0014 - val_loss: 0.0012 - 37s/epoch - 384us/sample
Epoch 26/145
95501/95501 - 37s - loss: 0.0014 - val_loss: 0.0012 - 37s/epoch - 382us/sample
Epoch 27/145
95501/95501 - 37s - loss: 0.0014 - val_loss: 0.0012 - 37s/epoch - 384us/sample
Epoch 28/145
95501/95501 - 37s - loss: 0.0014 - val_loss: 0.0011 - 37s/epoch - 384us/sample
Epoch 29/145
95501/95501 - 37s - loss: 0.0014 - val_loss: 0.0012 - 37s/epoch - 382us/sample
Epoch 30/145
95501/95501 - 37s - loss: 0.0014 - val_loss: 0.0011 - 37s/epoch - 384us/sample
Epoch 31/145
95501/95501 - 36s - loss: 0.0013 - val_loss: 0.0011 - 36s/epoch - 382us/sample
Epoch 32/145
95501/95501 - 37s - loss: 0.0013 - val_loss: 0.0011 - 37s/epoch - 384us/sample
Epoch 33/145
95501/95501 - 36s - loss: 0.0013 - val_loss: 0.0011 - 36s/epoch - 382us/sample
Epoch 34/145
95501/95501 - 37s - loss: 0.0013 - val_loss: 0.0011 - 37s/epoch - 385us/sample
Epoch 35/145
95501/95501 - 37s - loss: 0.0013 - val_loss: 0.0011 - 37s/epoch - 383us/sample
Epoch 36/145
95501/95501 - 37s - loss: 0.0013 - val_loss: 0.0011 - 37s/epoch - 383us/sample
Epoch 37/145
95501/95501 - 37s - loss: 0.0013 - val_loss: 0.0011 - 37s/epoch - 382us/sample
Epoch 38/145
95501/95501 - 37s - loss: 0.0013 - val_loss: 0.0011 - 37s/epoch - 382us/sample
Epoch 39/145
95501/95501 - 37s - loss: 0.0013 - val_loss: 0.0011 - 37s/epoch - 384us/sample
Epoch 40/145
95501/95501 - 37s - loss: 0.0013 - val_loss: 0.0011 - 37s/epoch - 382us/sample
Epoch 41/145
95501/95501 - 37s - loss: 0.0013 - val_loss: 0.0011 - 37s/epoch - 384us/sample
Epoch 42/145
95501/95501 - 37s - loss: 0.0013 - val_loss: 0.0011 - 37s/epoch - 383us/sample
Epoch 43/145
95501/95501 - 37s - loss: 0.0013 - val_loss: 0.0011 - 37s/epoch - 384us/sample
Epoch 44/145
95501/95501 - 37s - loss: 0.0013 - val_loss: 0.0011 - 37s/epoch - 383us/sample
Epoch 45/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 0.0011 - 37s/epoch - 383us/sample
Epoch 46/145
95501/95501 - 37s - loss: 0.0013 - val_loss: 0.0011 - 37s/epoch - 383us/sample
Epoch 47/145
95501/95501 - 36s - loss: 0.0013 - val_loss: 0.0011 - 36s/epoch - 382us/sample
Epoch 48/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 0.0011 - 37s/epoch - 385us/sample
Epoch 49/145
95501/95501 - 37s - loss: 0.0013 - val_loss: 0.0011 - 37s/epoch - 383us/sample
Epoch 50/145
95501/95501 - 37s - loss: 0.0013 - val_loss: 0.0011 - 37s/epoch - 384us/sample
Epoch 51/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 0.0010 - 37s/epoch - 383us/sample
Epoch 52/145
95501/95501 - 37s - loss: 0.0013 - val_loss: 0.0011 - 37s/epoch - 384us/sample
Epoch 53/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 0.0011 - 37s/epoch - 383us/sample
Epoch 54/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 0.0010 - 37s/epoch - 383us/sample
Epoch 55/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 0.0011 - 37s/epoch - 385us/sample
Epoch 56/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 0.0010 - 37s/epoch - 383us/sample
Epoch 57/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 0.0010 - 37s/epoch - 384us/sample
Epoch 58/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 0.0011 - 37s/epoch - 383us/sample
Epoch 59/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 0.0010 - 37s/epoch - 384us/sample
Epoch 60/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 0.0011 - 37s/epoch - 383us/sample
Epoch 61/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 0.0010 - 37s/epoch - 384us/sample
Epoch 62/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 0.0010 - 37s/epoch - 383us/sample
Epoch 63/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 0.0010 - 37s/epoch - 385us/sample
Epoch 64/145
95501/95501 - 36s - loss: 0.0012 - val_loss: 0.0010 - 36s/epoch - 382us/sample
Epoch 65/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 9.9377e-04 - 37s/epoch - 385us/sample
Epoch 66/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 0.0011 - 37s/epoch - 383us/sample
Epoch 67/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 0.0010 - 37s/epoch - 384us/sample
Epoch 68/145
95501/95501 - 36s - loss: 0.0012 - val_loss: 0.0010 - 36s/epoch - 382us/sample
Epoch 69/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 9.9682e-04 - 37s/epoch - 385us/sample
Epoch 70/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 0.0010 - 37s/epoch - 383us/sample
Epoch 71/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 0.0010 - 37s/epoch - 383us/sample
Epoch 72/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 9.8914e-04 - 37s/epoch - 382us/sample
Epoch 73/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 0.0010 - 37s/epoch - 383us/sample
Epoch 74/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 0.0010 - 37s/epoch - 383us/sample
Epoch 75/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 9.7375e-04 - 37s/epoch - 384us/sample
Epoch 76/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 9.8543e-04 - 37s/epoch - 383us/sample
Epoch 77/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 0.0010 - 37s/epoch - 384us/sample
Epoch 78/145
95501/95501 - 36s - loss: 0.0012 - val_loss: 9.8290e-04 - 36s/epoch - 382us/sample
Epoch 79/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 9.9453e-04 - 37s/epoch - 384us/sample
Epoch 80/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 0.0010 - 37s/epoch - 382us/sample
Epoch 81/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 9.6732e-04 - 37s/epoch - 384us/sample
Epoch 82/145
95501/95501 - 36s - loss: 0.0012 - val_loss: 9.8614e-04 - 36s/epoch - 382us/sample
Epoch 83/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 9.8790e-04 - 37s/epoch - 384us/sample
Epoch 84/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 9.8125e-04 - 37s/epoch - 384us/sample
Epoch 85/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 9.8280e-04 - 37s/epoch - 382us/sample
Epoch 86/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 9.6960e-04 - 37s/epoch - 384us/sample
Epoch 87/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 0.0011 - 37s/epoch - 383us/sample
Epoch 88/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 9.5219e-04 - 37s/epoch - 384us/sample
Epoch 89/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 9.6630e-04 - 37s/epoch - 383us/sample
Epoch 90/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 9.7890e-04 - 37s/epoch - 384us/sample
Epoch 91/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.6607e-04 - 37s/epoch - 383us/sample
Epoch 92/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 9.7008e-04 - 37s/epoch - 383us/sample
Epoch 93/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 0.0010 - 37s/epoch - 383us/sample
Epoch 94/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 0.0010 - 37s/epoch - 383us/sample
Epoch 95/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.5252e-04 - 37s/epoch - 383us/sample
Epoch 96/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.6609e-04 - 37s/epoch - 384us/sample
Epoch 97/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.6759e-04 - 37s/epoch - 383us/sample
Epoch 98/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.4286e-04 - 37s/epoch - 385us/sample
Epoch 99/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 9.6673e-04 - 37s/epoch - 382us/sample
Epoch 100/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 0.0011 - 37s/epoch - 385us/sample
Epoch 101/145
95501/95501 - 36s - loss: 0.0011 - val_loss: 9.8472e-04 - 36s/epoch - 382us/sample
Epoch 102/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.3598e-04 - 37s/epoch - 384us/sample
Epoch 103/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.3278e-04 - 37s/epoch - 383us/sample
Epoch 104/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.7740e-04 - 37s/epoch - 384us/sample
Epoch 105/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.4441e-04 - 37s/epoch - 383us/sample
Epoch 106/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.6475e-04 - 37s/epoch - 383us/sample
Epoch 107/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.5374e-04 - 37s/epoch - 383us/sample
Epoch 108/145
95501/95501 - 36s - loss: 0.0011 - val_loss: 9.6156e-04 - 36s/epoch - 382us/sample
Epoch 109/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.6246e-04 - 37s/epoch - 385us/sample
Epoch 110/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 9.8694e-04 - 37s/epoch - 383us/sample
Epoch 111/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 9.4710e-04 - 37s/epoch - 385us/sample
Epoch 112/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.4583e-04 - 37s/epoch - 383us/sample
Epoch 113/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.3824e-04 - 37s/epoch - 384us/sample
Epoch 114/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.5932e-04 - 37s/epoch - 383us/sample
Epoch 115/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.4112e-04 - 37s/epoch - 384us/sample
Epoch 116/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.8579e-04 - 37s/epoch - 383us/sample
Epoch 117/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.3175e-04 - 37s/epoch - 385us/sample
Epoch 118/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.1837e-04 - 37s/epoch - 383us/sample
Epoch 119/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 0.0010 - 37s/epoch - 384us/sample
Epoch 120/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.5676e-04 - 37s/epoch - 382us/sample
Epoch 121/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 9.5670e-04 - 37s/epoch - 383us/sample
Epoch 122/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.3710e-04 - 37s/epoch - 383us/sample
Epoch 123/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.9505e-04 - 37s/epoch - 384us/sample
Epoch 124/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.3156e-04 - 37s/epoch - 385us/sample
Epoch 125/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.1447e-04 - 37s/epoch - 382us/sample
Epoch 126/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.9813e-04 - 37s/epoch - 385us/sample
Epoch 127/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.3312e-04 - 37s/epoch - 382us/sample
Epoch 128/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.3977e-04 - 37s/epoch - 385us/sample
Epoch 129/145
95501/95501 - 36s - loss: 0.0012 - val_loss: 9.5728e-04 - 36s/epoch - 382us/sample
Epoch 130/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 0.0010 - 37s/epoch - 385us/sample
Epoch 131/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.3719e-04 - 37s/epoch - 385us/sample
Epoch 132/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.2956e-04 - 37s/epoch - 384us/sample
Epoch 133/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 9.3464e-04 - 37s/epoch - 383us/sample
Epoch 134/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.2202e-04 - 37s/epoch - 383us/sample
Epoch 135/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.1962e-04 - 37s/epoch - 383us/sample
Epoch 136/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 0.0012 - 37s/epoch - 384us/sample
Epoch 137/145
95501/95501 - 37s - loss: 0.0012 - val_loss: 9.3239e-04 - 37s/epoch - 383us/sample
Epoch 138/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.3667e-04 - 37s/epoch - 385us/sample
Epoch 139/145
95501/95501 - 36s - loss: 0.0011 - val_loss: 9.4516e-04 - 36s/epoch - 382us/sample
Epoch 140/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.3234e-04 - 37s/epoch - 384us/sample
Epoch 141/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 0.0010 - 37s/epoch - 382us/sample
Epoch 142/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.5765e-04 - 37s/epoch - 384us/sample
Epoch 143/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.2233e-04 - 37s/epoch - 382us/sample
Epoch 144/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.1466e-04 - 37s/epoch - 384us/sample
Epoch 145/145
95501/95501 - 37s - loss: 0.0011 - val_loss: 9.2054e-04 - 37s/epoch - 383us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0009205380485463667
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 06:45:33.372956: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_23/outputlayer/BiasAdd' id:30155 op device:{requested: '', assigned: ''} def:{{{node decoder_model_23/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_23/outputlayer/MatMul, decoder_model_23/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.007525657724534346
cosine 0.005980278438682427
MAE: 0.01336847
RMSE: 0.025674995
r2: 0.9572362525655789
RMSE zero-vector: 0.23411466903540806
['1.4custom_VAE', 'mse', 16, 145, 0.0014, 0.6, 758, 0.001100594143005677, 0.0009205380485463667, 0.007525657724534346, 0.005980278438682427, 0.013368469662964344, 0.02567499503493309, 0.9572362525655789, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.7000000000000002 145 0.0012 64 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2148)         2717220     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_72 (BatchN  (None, 2148)        8592        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_72 (ReLU)                (None, 2148)         0           ['batch_normalization_72[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1628942     ['re_lu_72[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1628942     ['re_lu_72[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4933614     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 10,917,310
Trainable params: 10,907,202
Non-trainable params: 10,108
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-15 06:45:47.569325: W tensorflow/c/c_api.cc:291] Operation '{name:'training_48/Adam/beta_2/Assign' id:31881 op device:{requested: '', assigned: ''} def:{{{node training_48/Adam/beta_2/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_48/Adam/beta_2, training_48/Adam/beta_2/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 06:46:02.863028: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_24/mul' id:31445 op device:{requested: '', assigned: ''} def:{{{node loss_24/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_24/mul/x, loss_24/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 20s - loss: 0.0111 - val_loss: 0.0056 - 20s/epoch - 214us/sample
Epoch 2/145
95501/95501 - 13s - loss: 0.0044 - val_loss: 0.0040 - 13s/epoch - 132us/sample
Epoch 3/145
95501/95501 - 13s - loss: 0.0037 - val_loss: 0.0038 - 13s/epoch - 132us/sample
Epoch 4/145
95501/95501 - 13s - loss: 0.0037 - val_loss: 0.0033 - 13s/epoch - 132us/sample
Epoch 5/145
95501/95501 - 13s - loss: 0.0028 - val_loss: 0.0024 - 13s/epoch - 132us/sample
Epoch 6/145
95501/95501 - 13s - loss: 0.0024 - val_loss: 0.0064 - 13s/epoch - 132us/sample
Epoch 7/145
95501/95501 - 13s - loss: 0.0029 - val_loss: 0.0018 - 13s/epoch - 133us/sample
Epoch 8/145
95501/95501 - 13s - loss: 0.0023 - val_loss: 0.0019 - 13s/epoch - 132us/sample
Epoch 9/145
95501/95501 - 13s - loss: 0.0026 - val_loss: 0.0017 - 13s/epoch - 132us/sample
Epoch 10/145
95501/95501 - 13s - loss: 0.0017 - val_loss: 0.0016 - 13s/epoch - 132us/sample
Epoch 11/145
95501/95501 - 13s - loss: 0.0015 - val_loss: 0.0030 - 13s/epoch - 132us/sample
Epoch 12/145
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0014 - 13s/epoch - 132us/sample
Epoch 13/145
95501/95501 - 13s - loss: 0.0014 - val_loss: 0.0013 - 13s/epoch - 133us/sample
Epoch 14/145
95501/95501 - 13s - loss: 0.0013 - val_loss: 0.0022 - 13s/epoch - 132us/sample
Epoch 15/145
95501/95501 - 13s - loss: 0.0014 - val_loss: 0.0011 - 13s/epoch - 132us/sample
Epoch 16/145
95501/95501 - 13s - loss: 0.0012 - val_loss: 0.0011 - 13s/epoch - 132us/sample
Epoch 17/145
95501/95501 - 13s - loss: 0.0012 - val_loss: 0.0014 - 13s/epoch - 132us/sample
Epoch 18/145
95501/95501 - 13s - loss: 0.0013 - val_loss: 0.0010 - 13s/epoch - 132us/sample
Epoch 19/145
95501/95501 - 13s - loss: 0.0011 - val_loss: 0.0018 - 13s/epoch - 132us/sample
Epoch 20/145
95501/95501 - 13s - loss: 0.0012 - val_loss: 0.0012 - 13s/epoch - 133us/sample
Epoch 21/145
95501/95501 - 13s - loss: 0.0011 - val_loss: 9.6010e-04 - 13s/epoch - 132us/sample
Epoch 22/145
95501/95501 - 13s - loss: 0.0010 - val_loss: 9.3724e-04 - 13s/epoch - 132us/sample
Epoch 23/145
95501/95501 - 13s - loss: 9.9350e-04 - val_loss: 9.8865e-04 - 13s/epoch - 132us/sample
Epoch 24/145
95501/95501 - 13s - loss: 9.8569e-04 - val_loss: 9.1658e-04 - 13s/epoch - 132us/sample
Epoch 25/145
95501/95501 - 13s - loss: 9.4776e-04 - val_loss: 8.8213e-04 - 13s/epoch - 132us/sample
Epoch 26/145
95501/95501 - 13s - loss: 9.4744e-04 - val_loss: 8.8051e-04 - 13s/epoch - 133us/sample
Epoch 27/145
95501/95501 - 13s - loss: 9.2075e-04 - val_loss: 8.7302e-04 - 13s/epoch - 132us/sample
Epoch 28/145
95501/95501 - 13s - loss: 9.0903e-04 - val_loss: 8.6495e-04 - 13s/epoch - 132us/sample
Epoch 29/145
95501/95501 - 13s - loss: 8.9948e-04 - val_loss: 8.3206e-04 - 13s/epoch - 132us/sample
Epoch 30/145
95501/95501 - 13s - loss: 8.8717e-04 - val_loss: 9.6226e-04 - 13s/epoch - 132us/sample
Epoch 31/145
95501/95501 - 13s - loss: 8.8626e-04 - val_loss: 8.3491e-04 - 13s/epoch - 132us/sample
Epoch 32/145
95501/95501 - 13s - loss: 9.0006e-04 - val_loss: 8.7032e-04 - 13s/epoch - 134us/sample
Epoch 33/145
95501/95501 - 13s - loss: 8.7876e-04 - val_loss: 8.8564e-04 - 13s/epoch - 132us/sample
Epoch 34/145
95501/95501 - 13s - loss: 9.1839e-04 - val_loss: 8.0770e-04 - 13s/epoch - 132us/sample
Epoch 35/145
95501/95501 - 13s - loss: 8.5679e-04 - val_loss: 8.8332e-04 - 13s/epoch - 132us/sample
Epoch 36/145
95501/95501 - 13s - loss: 9.1397e-04 - val_loss: 8.6043e-04 - 13s/epoch - 132us/sample
Epoch 37/145
95501/95501 - 13s - loss: 8.8875e-04 - val_loss: 7.8835e-04 - 13s/epoch - 132us/sample
Epoch 38/145
95501/95501 - 13s - loss: 8.5853e-04 - val_loss: 9.2726e-04 - 13s/epoch - 134us/sample
Epoch 39/145
95501/95501 - 13s - loss: 9.4303e-04 - val_loss: 0.0016 - 13s/epoch - 132us/sample
Epoch 40/145
95501/95501 - 13s - loss: 9.4751e-04 - val_loss: 8.9956e-04 - 13s/epoch - 132us/sample
Epoch 41/145
95501/95501 - 13s - loss: 9.2403e-04 - val_loss: 8.6814e-04 - 13s/epoch - 132us/sample
Epoch 42/145
95501/95501 - 13s - loss: 9.2181e-04 - val_loss: 7.8142e-04 - 13s/epoch - 132us/sample
Epoch 43/145
95501/95501 - 13s - loss: 8.3281e-04 - val_loss: 9.5967e-04 - 13s/epoch - 132us/sample
Epoch 44/145
95501/95501 - 13s - loss: 9.1756e-04 - val_loss: 7.7665e-04 - 13s/epoch - 132us/sample
Epoch 45/145
95501/95501 - 13s - loss: 8.4553e-04 - val_loss: 7.6438e-04 - 13s/epoch - 133us/sample
Epoch 46/145
95501/95501 - 13s - loss: 8.1738e-04 - val_loss: 8.3430e-04 - 13s/epoch - 132us/sample
Epoch 47/145
95501/95501 - 13s - loss: 9.1052e-04 - val_loss: 7.6782e-04 - 13s/epoch - 132us/sample
Epoch 48/145
95501/95501 - 13s - loss: 8.1112e-04 - val_loss: 7.7175e-04 - 13s/epoch - 132us/sample
Epoch 49/145
95501/95501 - 13s - loss: 8.3179e-04 - val_loss: 7.4719e-04 - 13s/epoch - 132us/sample
Epoch 50/145
95501/95501 - 13s - loss: 7.9999e-04 - val_loss: 7.6543e-04 - 13s/epoch - 132us/sample
Epoch 51/145
95501/95501 - 13s - loss: 7.9524e-04 - val_loss: 8.3641e-04 - 13s/epoch - 133us/sample
Epoch 52/145
95501/95501 - 13s - loss: 8.3711e-04 - val_loss: 7.5442e-04 - 13s/epoch - 132us/sample
Epoch 53/145
95501/95501 - 13s - loss: 7.8761e-04 - val_loss: 7.3881e-04 - 13s/epoch - 132us/sample
Epoch 54/145
95501/95501 - 13s - loss: 7.9056e-04 - val_loss: 7.2886e-04 - 13s/epoch - 132us/sample
Epoch 55/145
95501/95501 - 13s - loss: 7.8084e-04 - val_loss: 7.6183e-04 - 13s/epoch - 132us/sample
Epoch 56/145
95501/95501 - 13s - loss: 8.0106e-04 - val_loss: 7.3420e-04 - 13s/epoch - 132us/sample
Epoch 57/145
95501/95501 - 13s - loss: 7.7688e-04 - val_loss: 7.7732e-04 - 13s/epoch - 133us/sample
Epoch 58/145
95501/95501 - 13s - loss: 8.0843e-04 - val_loss: 8.3715e-04 - 13s/epoch - 132us/sample
Epoch 59/145
95501/95501 - 13s - loss: 8.5820e-04 - val_loss: 9.0385e-04 - 13s/epoch - 132us/sample
Epoch 60/145
95501/95501 - 13s - loss: 8.8439e-04 - val_loss: 7.4017e-04 - 13s/epoch - 132us/sample
Epoch 61/145
95501/95501 - 13s - loss: 7.7781e-04 - val_loss: 7.2748e-04 - 13s/epoch - 132us/sample
Epoch 62/145
95501/95501 - 13s - loss: 7.7000e-04 - val_loss: 7.3311e-04 - 13s/epoch - 132us/sample
Epoch 63/145
95501/95501 - 13s - loss: 7.7310e-04 - val_loss: 9.0062e-04 - 13s/epoch - 133us/sample
Epoch 64/145
95501/95501 - 13s - loss: 8.9257e-04 - val_loss: 9.3609e-04 - 13s/epoch - 133us/sample
Epoch 65/145
95501/95501 - 13s - loss: 8.9540e-04 - val_loss: 7.4256e-04 - 13s/epoch - 132us/sample
Epoch 66/145
95501/95501 - 13s - loss: 7.7663e-04 - val_loss: 7.2624e-04 - 13s/epoch - 132us/sample
Epoch 67/145
95501/95501 - 13s - loss: 7.7118e-04 - val_loss: 7.1993e-04 - 13s/epoch - 132us/sample
Epoch 68/145
95501/95501 - 13s - loss: 7.6196e-04 - val_loss: 8.1444e-04 - 13s/epoch - 132us/sample
Epoch 69/145
95501/95501 - 13s - loss: 8.6595e-04 - val_loss: 7.4751e-04 - 13s/epoch - 132us/sample
Epoch 70/145
95501/95501 - 13s - loss: 7.9441e-04 - val_loss: 8.9453e-04 - 13s/epoch - 133us/sample
Epoch 71/145
95501/95501 - 13s - loss: 9.2290e-04 - val_loss: 7.4550e-04 - 13s/epoch - 132us/sample
Epoch 72/145
95501/95501 - 13s - loss: 7.7358e-04 - val_loss: 7.2177e-04 - 13s/epoch - 132us/sample
Epoch 73/145
95501/95501 - 13s - loss: 7.6457e-04 - val_loss: 7.3920e-04 - 13s/epoch - 132us/sample
Epoch 74/145
95501/95501 - 13s - loss: 7.7229e-04 - val_loss: 7.1804e-04 - 13s/epoch - 132us/sample
Epoch 75/145
95501/95501 - 13s - loss: 7.4776e-04 - val_loss: 7.1475e-04 - 13s/epoch - 132us/sample
Epoch 76/145
95501/95501 - 13s - loss: 7.4979e-04 - val_loss: 7.1551e-04 - 13s/epoch - 133us/sample
Epoch 77/145
95501/95501 - 13s - loss: 8.0832e-04 - val_loss: 7.1100e-04 - 13s/epoch - 132us/sample
Epoch 78/145
95501/95501 - 13s - loss: 7.4642e-04 - val_loss: 7.9928e-04 - 13s/epoch - 132us/sample
Epoch 79/145
95501/95501 - 13s - loss: 8.6489e-04 - val_loss: 8.1951e-04 - 13s/epoch - 132us/sample
Epoch 80/145
95501/95501 - 13s - loss: 7.5989e-04 - val_loss: 7.3680e-04 - 13s/epoch - 132us/sample
Epoch 81/145
95501/95501 - 13s - loss: 7.8537e-04 - val_loss: 7.0258e-04 - 13s/epoch - 132us/sample
Epoch 82/145
95501/95501 - 13s - loss: 7.4420e-04 - val_loss: 7.2622e-04 - 13s/epoch - 132us/sample
Epoch 83/145
95501/95501 - 13s - loss: 7.6074e-04 - val_loss: 7.4870e-04 - 13s/epoch - 133us/sample
Epoch 84/145
95501/95501 - 13s - loss: 7.9708e-04 - val_loss: 7.0436e-04 - 13s/epoch - 132us/sample
Epoch 85/145
95501/95501 - 13s - loss: 7.4087e-04 - val_loss: 7.3745e-04 - 13s/epoch - 132us/sample
Epoch 86/145
95501/95501 - 13s - loss: 7.7355e-04 - val_loss: 6.9670e-04 - 13s/epoch - 132us/sample
Epoch 87/145
95501/95501 - 13s - loss: 7.4022e-04 - val_loss: 7.3087e-04 - 13s/epoch - 132us/sample
Epoch 88/145
95501/95501 - 13s - loss: 7.3205e-04 - val_loss: 6.8839e-04 - 13s/epoch - 132us/sample
Epoch 89/145
95501/95501 - 13s - loss: 7.2963e-04 - val_loss: 6.9546e-04 - 13s/epoch - 133us/sample
Epoch 90/145
95501/95501 - 13s - loss: 7.3033e-04 - val_loss: 8.3642e-04 - 13s/epoch - 132us/sample
Epoch 91/145
95501/95501 - 13s - loss: 8.2223e-04 - val_loss: 8.0342e-04 - 13s/epoch - 132us/sample
Epoch 92/145
95501/95501 - 13s - loss: 8.1213e-04 - val_loss: 7.3739e-04 - 13s/epoch - 132us/sample
Epoch 93/145
95501/95501 - 13s - loss: 8.1785e-04 - val_loss: 7.4509e-04 - 13s/epoch - 132us/sample
Epoch 94/145
95501/95501 - 13s - loss: 7.4570e-04 - val_loss: 6.9340e-04 - 13s/epoch - 132us/sample
Epoch 95/145
95501/95501 - 13s - loss: 7.3217e-04 - val_loss: 6.8624e-04 - 13s/epoch - 133us/sample
Epoch 96/145
95501/95501 - 13s - loss: 7.2666e-04 - val_loss: 7.3904e-04 - 13s/epoch - 132us/sample
Epoch 97/145
95501/95501 - 13s - loss: 7.6372e-04 - val_loss: 6.8854e-04 - 13s/epoch - 132us/sample
Epoch 98/145
95501/95501 - 13s - loss: 7.3099e-04 - val_loss: 8.1541e-04 - 13s/epoch - 132us/sample
Epoch 99/145
95501/95501 - 13s - loss: 8.2347e-04 - val_loss: 6.8977e-04 - 13s/epoch - 132us/sample
Epoch 100/145
95501/95501 - 13s - loss: 7.3189e-04 - val_loss: 6.9061e-04 - 13s/epoch - 132us/sample
Epoch 101/145
95501/95501 - 13s - loss: 7.2926e-04 - val_loss: 7.2960e-04 - 13s/epoch - 133us/sample
Epoch 102/145
95501/95501 - 13s - loss: 7.5059e-04 - val_loss: 6.8966e-04 - 13s/epoch - 132us/sample
Epoch 103/145
95501/95501 - 13s - loss: 7.1697e-04 - val_loss: 6.8438e-04 - 13s/epoch - 132us/sample
Epoch 104/145
95501/95501 - 13s - loss: 7.1622e-04 - val_loss: 6.8933e-04 - 13s/epoch - 132us/sample
Epoch 105/145
95501/95501 - 13s - loss: 7.2440e-04 - val_loss: 7.2055e-04 - 13s/epoch - 132us/sample
Epoch 106/145
95501/95501 - 13s - loss: 7.4914e-04 - val_loss: 6.8792e-04 - 13s/epoch - 132us/sample
Epoch 107/145
95501/95501 - 13s - loss: 7.1827e-04 - val_loss: 6.7866e-04 - 13s/epoch - 132us/sample
Epoch 108/145
95501/95501 - 13s - loss: 7.0759e-04 - val_loss: 6.6617e-04 - 13s/epoch - 132us/sample
Epoch 109/145
95501/95501 - 13s - loss: 7.1563e-04 - val_loss: 8.3457e-04 - 13s/epoch - 132us/sample
Epoch 110/145
95501/95501 - 13s - loss: 8.2181e-04 - val_loss: 6.8445e-04 - 13s/epoch - 132us/sample
Epoch 111/145
95501/95501 - 13s - loss: 7.1943e-04 - val_loss: 6.7817e-04 - 13s/epoch - 132us/sample
Epoch 112/145
95501/95501 - 13s - loss: 7.0783e-04 - val_loss: 6.7152e-04 - 13s/epoch - 132us/sample
Epoch 113/145
95501/95501 - 13s - loss: 7.1296e-04 - val_loss: 8.5129e-04 - 13s/epoch - 132us/sample
Epoch 114/145
95501/95501 - 13s - loss: 8.3267e-04 - val_loss: 6.8377e-04 - 13s/epoch - 133us/sample
Epoch 115/145
95501/95501 - 13s - loss: 7.1698e-04 - val_loss: 6.7048e-04 - 13s/epoch - 132us/sample
Epoch 116/145
95501/95501 - 13s - loss: 7.0800e-04 - val_loss: 6.6829e-04 - 13s/epoch - 132us/sample
Epoch 117/145
95501/95501 - 13s - loss: 7.0461e-04 - val_loss: 6.6888e-04 - 13s/epoch - 132us/sample
Epoch 118/145
95501/95501 - 13s - loss: 7.1185e-04 - val_loss: 9.1383e-04 - 13s/epoch - 132us/sample
Epoch 119/145
95501/95501 - 13s - loss: 8.5078e-04 - val_loss: 6.7955e-04 - 13s/epoch - 132us/sample
Epoch 120/145
95501/95501 - 13s - loss: 7.2551e-04 - val_loss: 6.7474e-04 - 13s/epoch - 133us/sample
Epoch 121/145
95501/95501 - 13s - loss: 7.0794e-04 - val_loss: 6.6255e-04 - 13s/epoch - 132us/sample
Epoch 122/145
95501/95501 - 13s - loss: 7.0412e-04 - val_loss: 6.8034e-04 - 13s/epoch - 132us/sample
Epoch 123/145
95501/95501 - 13s - loss: 7.1723e-04 - val_loss: 6.8761e-04 - 13s/epoch - 132us/sample
Epoch 124/145
95501/95501 - 13s - loss: 7.1445e-04 - val_loss: 6.9516e-04 - 13s/epoch - 132us/sample
Epoch 125/145
95501/95501 - 13s - loss: 7.2608e-04 - val_loss: 6.8612e-04 - 13s/epoch - 132us/sample
Epoch 126/145
95501/95501 - 13s - loss: 7.1938e-04 - val_loss: 6.7183e-04 - 13s/epoch - 133us/sample
Epoch 127/145
95501/95501 - 13s - loss: 6.9852e-04 - val_loss: 6.6767e-04 - 13s/epoch - 132us/sample
Epoch 128/145
95501/95501 - 13s - loss: 7.1108e-04 - val_loss: 6.6199e-04 - 13s/epoch - 132us/sample
Epoch 129/145
95501/95501 - 13s - loss: 6.9734e-04 - val_loss: 7.8277e-04 - 13s/epoch - 132us/sample
Epoch 130/145
95501/95501 - 13s - loss: 7.6222e-04 - val_loss: 6.6231e-04 - 13s/epoch - 132us/sample
Epoch 131/145
95501/95501 - 13s - loss: 6.9888e-04 - val_loss: 6.9729e-04 - 13s/epoch - 132us/sample
Epoch 132/145
95501/95501 - 13s - loss: 7.0987e-04 - val_loss: 6.6857e-04 - 13s/epoch - 133us/sample
Epoch 133/145
95501/95501 - 13s - loss: 6.9681e-04 - val_loss: 7.3284e-04 - 13s/epoch - 132us/sample
Epoch 134/145
95501/95501 - 13s - loss: 7.5748e-04 - val_loss: 6.6507e-04 - 13s/epoch - 132us/sample
Epoch 135/145
95501/95501 - 13s - loss: 6.9726e-04 - val_loss: 7.0180e-04 - 13s/epoch - 132us/sample
Epoch 136/145
95501/95501 - 13s - loss: 7.1990e-04 - val_loss: 6.5852e-04 - 13s/epoch - 132us/sample
Epoch 137/145
95501/95501 - 13s - loss: 6.9269e-04 - val_loss: 6.8923e-04 - 13s/epoch - 132us/sample
Epoch 138/145
95501/95501 - 13s - loss: 7.2042e-04 - val_loss: 6.5592e-04 - 13s/epoch - 133us/sample
Epoch 139/145
95501/95501 - 13s - loss: 6.8935e-04 - val_loss: 6.9855e-04 - 13s/epoch - 132us/sample
Epoch 140/145
95501/95501 - 13s - loss: 7.3126e-04 - val_loss: 7.0119e-04 - 13s/epoch - 132us/sample
Epoch 141/145
95501/95501 - 13s - loss: 7.4353e-04 - val_loss: 7.8968e-04 - 13s/epoch - 132us/sample
Epoch 142/145
95501/95501 - 13s - loss: 8.1603e-04 - val_loss: 6.8725e-04 - 13s/epoch - 132us/sample
Epoch 143/145
95501/95501 - 13s - loss: 7.0853e-04 - val_loss: 8.0792e-04 - 13s/epoch - 132us/sample
Epoch 144/145
95501/95501 - 13s - loss: 7.8758e-04 - val_loss: 9.0822e-04 - 13s/epoch - 133us/sample
Epoch 145/145
95501/95501 - 13s - loss: 7.1397e-04 - val_loss: 7.9063e-04 - 13s/epoch - 133us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0007906271822659648
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 07:16:22.053685: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_24/outputlayer/BiasAdd' id:31416 op device:{requested: '', assigned: ''} def:{{{node decoder_model_24/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_24/outputlayer/MatMul, decoder_model_24/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.005779989100248464
cosine 0.004561362027717545
MAE: 0.013801903
RMSE: 0.022391386
r2: 0.9674755978591741
RMSE zero-vector: 0.23411466903540806
['1.7000000000000002custom_VAE', 'mse', 64, 145, 0.0012, 0.6, 758, 0.0007139722681688632, 0.0007906271822659648, 0.005779989100248464, 0.004561362027717545, 0.01380190346390009, 0.022391386330127716, 0.9674755978591741, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6 85 0.001 64 2] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2022)         2557830     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_75 (BatchN  (None, 2022)        8088        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_75 (ReLU)                (None, 2022)         0           ['batch_normalization_75[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1533434     ['re_lu_75[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1533434     ['re_lu_75[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4678212     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 10,310,998
Trainable params: 10,301,394
Non-trainable params: 9,604
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/85
2023-02-15 07:16:36.599138: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_76/moving_mean/Assign' id:32441 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_76/moving_mean/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_76/moving_mean, batch_normalization_76/moving_mean/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 07:16:52.062031: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_25/mul' id:32713 op device:{requested: '', assigned: ''} def:{{{node loss_25/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_25/mul/x, loss_25/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 21s - loss: 0.0057 - val_loss: 0.0048 - 21s/epoch - 216us/sample
Epoch 2/85
95501/95501 - 13s - loss: 0.0028 - val_loss: 0.0026 - 13s/epoch - 135us/sample
Epoch 3/85
95501/95501 - 13s - loss: 0.0024 - val_loss: 0.0019 - 13s/epoch - 135us/sample
Epoch 4/85
95501/95501 - 13s - loss: 0.0018 - val_loss: 0.0015 - 13s/epoch - 136us/sample
Epoch 5/85
95501/95501 - 13s - loss: 0.0015 - val_loss: 0.0012 - 13s/epoch - 134us/sample
Epoch 6/85
95501/95501 - 13s - loss: 0.0013 - val_loss: 0.0011 - 13s/epoch - 135us/sample
Epoch 7/85
95501/95501 - 13s - loss: 0.0011 - val_loss: 0.0010 - 13s/epoch - 134us/sample
Epoch 8/85
95501/95501 - 13s - loss: 0.0010 - val_loss: 9.1058e-04 - 13s/epoch - 135us/sample
Epoch 9/85
95501/95501 - 13s - loss: 9.2502e-04 - val_loss: 8.3108e-04 - 13s/epoch - 135us/sample
Epoch 10/85
95501/95501 - 13s - loss: 8.6883e-04 - val_loss: 7.6867e-04 - 13s/epoch - 135us/sample
Epoch 11/85
95501/95501 - 13s - loss: 8.2743e-04 - val_loss: 0.0011 - 13s/epoch - 135us/sample
Epoch 12/85
95501/95501 - 13s - loss: 9.1192e-04 - val_loss: 7.1526e-04 - 13s/epoch - 134us/sample
Epoch 13/85
95501/95501 - 13s - loss: 7.5486e-04 - val_loss: 6.7049e-04 - 13s/epoch - 134us/sample
Epoch 14/85
95501/95501 - 13s - loss: 7.2475e-04 - val_loss: 6.8625e-04 - 13s/epoch - 135us/sample
Epoch 15/85
95501/95501 - 13s - loss: 7.1969e-04 - val_loss: 6.4745e-04 - 13s/epoch - 134us/sample
Epoch 16/85
95501/95501 - 13s - loss: 6.8644e-04 - val_loss: 6.4459e-04 - 13s/epoch - 135us/sample
Epoch 17/85
95501/95501 - 13s - loss: 6.8520e-04 - val_loss: 7.0884e-04 - 13s/epoch - 135us/sample
Epoch 18/85
95501/95501 - 13s - loss: 6.9788e-04 - val_loss: 6.0321e-04 - 13s/epoch - 135us/sample
Epoch 19/85
95501/95501 - 13s - loss: 6.4833e-04 - val_loss: 6.0104e-04 - 13s/epoch - 134us/sample
Epoch 20/85
95501/95501 - 13s - loss: 6.3611e-04 - val_loss: 6.0901e-04 - 13s/epoch - 135us/sample
Epoch 21/85
95501/95501 - 13s - loss: 6.3836e-04 - val_loss: 6.4356e-04 - 13s/epoch - 135us/sample
Epoch 22/85
95501/95501 - 13s - loss: 6.5474e-04 - val_loss: 5.7465e-04 - 13s/epoch - 135us/sample
Epoch 23/85
95501/95501 - 13s - loss: 6.1920e-04 - val_loss: 5.8055e-04 - 13s/epoch - 136us/sample
Epoch 24/85
95501/95501 - 13s - loss: 6.0281e-04 - val_loss: 5.6453e-04 - 13s/epoch - 135us/sample
Epoch 25/85
95501/95501 - 13s - loss: 5.9236e-04 - val_loss: 5.5426e-04 - 13s/epoch - 135us/sample
Epoch 26/85
95501/95501 - 13s - loss: 6.0774e-04 - val_loss: 5.5973e-04 - 13s/epoch - 134us/sample
Epoch 27/85
95501/95501 - 13s - loss: 5.8596e-04 - val_loss: 6.0549e-04 - 13s/epoch - 134us/sample
Epoch 28/85
95501/95501 - 13s - loss: 6.0030e-04 - val_loss: 6.4475e-04 - 13s/epoch - 134us/sample
Epoch 29/85
95501/95501 - 13s - loss: 6.2046e-04 - val_loss: 5.3991e-04 - 13s/epoch - 136us/sample
Epoch 30/85
95501/95501 - 13s - loss: 5.8115e-04 - val_loss: 5.9402e-04 - 13s/epoch - 135us/sample
Epoch 31/85
95501/95501 - 13s - loss: 6.2802e-04 - val_loss: 6.4783e-04 - 13s/epoch - 135us/sample
Epoch 32/85
95501/95501 - 13s - loss: 6.2429e-04 - val_loss: 5.6158e-04 - 13s/epoch - 134us/sample
Epoch 33/85
95501/95501 - 13s - loss: 5.9400e-04 - val_loss: 5.2986e-04 - 13s/epoch - 134us/sample
Epoch 34/85
95501/95501 - 13s - loss: 5.6277e-04 - val_loss: 5.2421e-04 - 13s/epoch - 134us/sample
Epoch 35/85
95501/95501 - 13s - loss: 5.5559e-04 - val_loss: 5.2300e-04 - 13s/epoch - 135us/sample
Epoch 36/85
95501/95501 - 13s - loss: 5.5419e-04 - val_loss: 5.2342e-04 - 13s/epoch - 136us/sample
Epoch 37/85
95501/95501 - 13s - loss: 5.7497e-04 - val_loss: 5.2103e-04 - 13s/epoch - 135us/sample
Epoch 38/85
95501/95501 - 13s - loss: 5.4806e-04 - val_loss: 5.2122e-04 - 13s/epoch - 134us/sample
Epoch 39/85
95501/95501 - 13s - loss: 5.4543e-04 - val_loss: 5.1837e-04 - 13s/epoch - 134us/sample
Epoch 40/85
95501/95501 - 13s - loss: 5.4277e-04 - val_loss: 5.1949e-04 - 13s/epoch - 134us/sample
Epoch 41/85
95501/95501 - 13s - loss: 5.3452e-04 - val_loss: 5.0476e-04 - 13s/epoch - 135us/sample
Epoch 42/85
95501/95501 - 13s - loss: 5.3922e-04 - val_loss: 5.0156e-04 - 13s/epoch - 136us/sample
Epoch 43/85
95501/95501 - 13s - loss: 5.3025e-04 - val_loss: 5.0609e-04 - 13s/epoch - 135us/sample
Epoch 44/85
95501/95501 - 13s - loss: 5.2787e-04 - val_loss: 5.0184e-04 - 13s/epoch - 135us/sample
Epoch 45/85
95501/95501 - 13s - loss: 5.3386e-04 - val_loss: 5.3159e-04 - 13s/epoch - 134us/sample
Epoch 46/85
95501/95501 - 13s - loss: 5.4698e-04 - val_loss: 4.9459e-04 - 13s/epoch - 134us/sample
Epoch 47/85
95501/95501 - 13s - loss: 5.2575e-04 - val_loss: 4.9222e-04 - 13s/epoch - 135us/sample
Epoch 48/85
95501/95501 - 13s - loss: 5.2481e-04 - val_loss: 5.0675e-04 - 13s/epoch - 136us/sample
Epoch 49/85
95501/95501 - 13s - loss: 5.2686e-04 - val_loss: 4.9552e-04 - 13s/epoch - 135us/sample
Epoch 50/85
95501/95501 - 13s - loss: 5.1716e-04 - val_loss: 4.9721e-04 - 13s/epoch - 135us/sample
Epoch 51/85
95501/95501 - 13s - loss: 5.2168e-04 - val_loss: 4.8483e-04 - 13s/epoch - 134us/sample
Epoch 52/85
95501/95501 - 13s - loss: 5.2623e-04 - val_loss: 5.5732e-04 - 13s/epoch - 135us/sample
Epoch 53/85
95501/95501 - 13s - loss: 5.5092e-04 - val_loss: 4.8867e-04 - 13s/epoch - 134us/sample
Epoch 54/85
95501/95501 - 13s - loss: 5.1322e-04 - val_loss: 4.9011e-04 - 13s/epoch - 135us/sample
Epoch 55/85
95501/95501 - 13s - loss: 5.1250e-04 - val_loss: 5.6502e-04 - 13s/epoch - 135us/sample
Epoch 56/85
95501/95501 - 13s - loss: 5.6680e-04 - val_loss: 4.8942e-04 - 13s/epoch - 135us/sample
Epoch 57/85
95501/95501 - 13s - loss: 5.1476e-04 - val_loss: 4.8217e-04 - 13s/epoch - 134us/sample
Epoch 58/85
95501/95501 - 13s - loss: 5.2977e-04 - val_loss: 5.4119e-04 - 13s/epoch - 135us/sample
Epoch 59/85
95501/95501 - 13s - loss: 5.2225e-04 - val_loss: 4.8256e-04 - 13s/epoch - 134us/sample
Epoch 60/85
95501/95501 - 13s - loss: 5.0492e-04 - val_loss: 4.8442e-04 - 13s/epoch - 135us/sample
Epoch 61/85
95501/95501 - 13s - loss: 5.0937e-04 - val_loss: 4.7781e-04 - 13s/epoch - 136us/sample
Epoch 62/85
95501/95501 - 13s - loss: 5.0409e-04 - val_loss: 4.7861e-04 - 13s/epoch - 134us/sample
Epoch 63/85
95501/95501 - 13s - loss: 5.4140e-04 - val_loss: 4.8257e-04 - 13s/epoch - 135us/sample
Epoch 64/85
95501/95501 - 13s - loss: 5.0837e-04 - val_loss: 4.7757e-04 - 13s/epoch - 135us/sample
Epoch 65/85
95501/95501 - 13s - loss: 5.0315e-04 - val_loss: 4.7145e-04 - 13s/epoch - 134us/sample
Epoch 66/85
95501/95501 - 13s - loss: 4.9832e-04 - val_loss: 4.7722e-04 - 13s/epoch - 135us/sample
Epoch 67/85
95501/95501 - 13s - loss: 5.0315e-04 - val_loss: 4.7491e-04 - 13s/epoch - 135us/sample
Epoch 68/85
95501/95501 - 13s - loss: 5.0036e-04 - val_loss: 5.0464e-04 - 13s/epoch - 135us/sample
Epoch 69/85
95501/95501 - 13s - loss: 5.2471e-04 - val_loss: 4.8448e-04 - 13s/epoch - 135us/sample
Epoch 70/85
95501/95501 - 13s - loss: 4.9854e-04 - val_loss: 4.7389e-04 - 13s/epoch - 134us/sample
Epoch 71/85
95501/95501 - 13s - loss: 4.9844e-04 - val_loss: 5.3788e-04 - 13s/epoch - 134us/sample
Epoch 72/85
95501/95501 - 13s - loss: 5.3252e-04 - val_loss: 4.7176e-04 - 13s/epoch - 135us/sample
Epoch 73/85
95501/95501 - 13s - loss: 4.9528e-04 - val_loss: 4.7515e-04 - 13s/epoch - 135us/sample
Epoch 74/85
95501/95501 - 13s - loss: 4.9228e-04 - val_loss: 4.6871e-04 - 13s/epoch - 135us/sample
Epoch 75/85
95501/95501 - 13s - loss: 4.9221e-04 - val_loss: 4.6847e-04 - 13s/epoch - 135us/sample
Epoch 76/85
95501/95501 - 13s - loss: 4.8898e-04 - val_loss: 4.6767e-04 - 13s/epoch - 134us/sample
Epoch 77/85
95501/95501 - 13s - loss: 5.0247e-04 - val_loss: 5.9698e-04 - 13s/epoch - 134us/sample
Epoch 78/85
95501/95501 - 13s - loss: 5.8745e-04 - val_loss: 5.1866e-04 - 13s/epoch - 134us/sample
Epoch 79/85
95501/95501 - 13s - loss: 5.3602e-04 - val_loss: 4.7463e-04 - 13s/epoch - 136us/sample
Epoch 80/85
95501/95501 - 13s - loss: 4.9645e-04 - val_loss: 4.6630e-04 - 13s/epoch - 135us/sample
Epoch 81/85
95501/95501 - 13s - loss: 4.9041e-04 - val_loss: 4.6691e-04 - 13s/epoch - 134us/sample
Epoch 82/85
95501/95501 - 13s - loss: 4.8845e-04 - val_loss: 4.6542e-04 - 13s/epoch - 134us/sample
Epoch 83/85
95501/95501 - 13s - loss: 4.9562e-04 - val_loss: 5.3048e-04 - 13s/epoch - 134us/sample
Epoch 84/85
95501/95501 - 13s - loss: 4.8987e-04 - val_loss: 5.0205e-04 - 13s/epoch - 134us/sample
Epoch 85/85
95501/95501 - 13s - loss: 5.1810e-04 - val_loss: 4.6996e-04 - 13s/epoch - 136us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.00046995964943753575
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 07:34:56.257991: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_25/outputlayer/BiasAdd' id:32677 op device:{requested: '', assigned: ''} def:{{{node decoder_model_25/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_25/outputlayer/MatMul, decoder_model_25/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.006413924505339449
cosine 0.005073487618218446
MAE: 0.012285861
RMSE: 0.023510763
r2: 0.9641416286585388
RMSE zero-vector: 0.23411466903540806
['1.6custom_VAE', 'logcosh', 64, 85, 0.001, 0.6, 758, 0.0005181046675495651, 0.00046995964943753575, 0.006413924505339449, 0.005073487618218446, 0.012285861186683178, 0.0235107634216547, 0.9641416286585388, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 2
Fitness    = 98.76685138538917
Last generation's best solutions = [1.6 145 0.0014 64 1] with fitness 98.76685138538917.
Best solutions :  [array([1.5, 145, 0.0012, 64, 1], dtype=object), array([1.6, 145, 0.0014, 64, 1], dtype=object), array([1.6, 145, 0.0014, 64, 1], dtype=object)]
Best solutions fitness :  [96.28315495666305, 98.76685138538917]
[1.6 140 0.0012 32 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2022)         2557830     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_78 (BatchN  (None, 2022)        8088        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_78 (ReLU)                (None, 2022)         0           ['batch_normalization_78[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1533434     ['re_lu_78[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1533434     ['re_lu_78[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4678212     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 10,310,998
Trainable params: 10,301,394
Non-trainable params: 9,604
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/140
2023-02-15 07:35:11.331909: W tensorflow/c/c_api.cc:291] Operation '{name:'training_52/Adam/batch_normalization_79/beta/v/Assign' id:34633 op device:{requested: '', assigned: ''} def:{{{node training_52/Adam/batch_normalization_79/beta/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_52/Adam/batch_normalization_79/beta/v, training_52/Adam/batch_normalization_79/beta/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 07:35:35.747142: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_26/mul' id:33997 op device:{requested: '', assigned: ''} def:{{{node loss_26/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_26/mul/x, loss_26/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 30s - loss: 0.0097 - val_loss: 0.0047 - 30s/epoch - 316us/sample
Epoch 2/140
95501/95501 - 22s - loss: 0.0047 - val_loss: 0.0034 - 22s/epoch - 230us/sample
Epoch 3/140
95501/95501 - 22s - loss: 0.0037 - val_loss: 0.0195 - 22s/epoch - 231us/sample
Epoch 4/140
95501/95501 - 22s - loss: 0.0030 - val_loss: 0.0259 - 22s/epoch - 229us/sample
Epoch 5/140
95501/95501 - 22s - loss: 0.0030 - val_loss: 0.0021 - 22s/epoch - 229us/sample
Epoch 6/140
95501/95501 - 22s - loss: 0.0020 - val_loss: 0.0017 - 22s/epoch - 230us/sample
Epoch 7/140
95501/95501 - 22s - loss: 0.0018 - val_loss: 0.0015 - 22s/epoch - 230us/sample
Epoch 8/140
95501/95501 - 22s - loss: 0.0016 - val_loss: 0.0014 - 22s/epoch - 229us/sample
Epoch 9/140
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0013 - 22s/epoch - 230us/sample
Epoch 10/140
95501/95501 - 22s - loss: 0.0015 - val_loss: 0.0012 - 22s/epoch - 229us/sample
Epoch 11/140
95501/95501 - 22s - loss: 0.0013 - val_loss: 0.0011 - 22s/epoch - 230us/sample
Epoch 12/140
95501/95501 - 22s - loss: 0.0013 - val_loss: 0.0011 - 22s/epoch - 230us/sample
Epoch 13/140
95501/95501 - 22s - loss: 0.0013 - val_loss: 0.0011 - 22s/epoch - 229us/sample
Epoch 14/140
95501/95501 - 22s - loss: 0.0012 - val_loss: 0.0011 - 22s/epoch - 229us/sample
Epoch 15/140
95501/95501 - 22s - loss: 0.0012 - val_loss: 0.0011 - 22s/epoch - 231us/sample
Epoch 16/140
95501/95501 - 22s - loss: 0.0012 - val_loss: 0.0010 - 22s/epoch - 229us/sample
Epoch 17/140
95501/95501 - 22s - loss: 0.0011 - val_loss: 0.0010 - 22s/epoch - 229us/sample
Epoch 18/140
95501/95501 - 22s - loss: 0.0011 - val_loss: 0.0010 - 22s/epoch - 230us/sample
Epoch 19/140
95501/95501 - 22s - loss: 0.0011 - val_loss: 9.7505e-04 - 22s/epoch - 230us/sample
Epoch 20/140
95501/95501 - 22s - loss: 0.0011 - val_loss: 9.3854e-04 - 22s/epoch - 230us/sample
Epoch 21/140
95501/95501 - 22s - loss: 0.0011 - val_loss: 0.0015 - 22s/epoch - 230us/sample
Epoch 22/140
95501/95501 - 22s - loss: 0.0011 - val_loss: 9.4081e-04 - 22s/epoch - 229us/sample
Epoch 23/140
95501/95501 - 22s - loss: 0.0011 - val_loss: 9.1675e-04 - 22s/epoch - 231us/sample
Epoch 24/140
95501/95501 - 22s - loss: 0.0010 - val_loss: 9.2489e-04 - 22s/epoch - 230us/sample
Epoch 25/140
95501/95501 - 22s - loss: 0.0010 - val_loss: 8.9758e-04 - 22s/epoch - 229us/sample
Epoch 26/140
95501/95501 - 22s - loss: 0.0010 - val_loss: 8.9144e-04 - 22s/epoch - 229us/sample
Epoch 27/140
95501/95501 - 22s - loss: 0.0011 - val_loss: 8.8582e-04 - 22s/epoch - 231us/sample
Epoch 28/140
95501/95501 - 22s - loss: 0.0010 - val_loss: 8.8852e-04 - 22s/epoch - 229us/sample
Epoch 29/140
95501/95501 - 22s - loss: 9.9328e-04 - val_loss: 8.6849e-04 - 22s/epoch - 229us/sample
Epoch 30/140
95501/95501 - 22s - loss: 9.8483e-04 - val_loss: 8.7076e-04 - 22s/epoch - 230us/sample
Epoch 31/140
95501/95501 - 22s - loss: 9.8512e-04 - val_loss: 8.9101e-04 - 22s/epoch - 231us/sample
Epoch 32/140
95501/95501 - 22s - loss: 9.8190e-04 - val_loss: 9.6190e-04 - 22s/epoch - 229us/sample
Epoch 33/140
95501/95501 - 22s - loss: 0.0010 - val_loss: 8.5911e-04 - 22s/epoch - 230us/sample
Epoch 34/140
95501/95501 - 22s - loss: 9.6904e-04 - val_loss: 8.5807e-04 - 22s/epoch - 229us/sample
Epoch 35/140
95501/95501 - 22s - loss: 9.7278e-04 - val_loss: 8.4447e-04 - 22s/epoch - 231us/sample
Epoch 36/140
95501/95501 - 22s - loss: 9.6204e-04 - val_loss: 8.4393e-04 - 22s/epoch - 230us/sample
Epoch 37/140
95501/95501 - 22s - loss: 9.5176e-04 - val_loss: 8.3757e-04 - 22s/epoch - 229us/sample
Epoch 38/140
95501/95501 - 22s - loss: 9.4822e-04 - val_loss: 8.3495e-04 - 22s/epoch - 229us/sample
Epoch 39/140
95501/95501 - 22s - loss: 9.4290e-04 - val_loss: 8.4210e-04 - 22s/epoch - 230us/sample
Epoch 40/140
95501/95501 - 22s - loss: 9.3605e-04 - val_loss: 8.2884e-04 - 22s/epoch - 230us/sample
Epoch 41/140
95501/95501 - 22s - loss: 9.3371e-04 - val_loss: 8.0722e-04 - 22s/epoch - 229us/sample
Epoch 42/140
95501/95501 - 22s - loss: 9.2423e-04 - val_loss: 8.2025e-04 - 22s/epoch - 230us/sample
Epoch 43/140
95501/95501 - 22s - loss: 9.2459e-04 - val_loss: 8.3170e-04 - 22s/epoch - 230us/sample
Epoch 44/140
95501/95501 - 22s - loss: 9.1625e-04 - val_loss: 8.4536e-04 - 22s/epoch - 230us/sample
Epoch 45/140
95501/95501 - 22s - loss: 9.1905e-04 - val_loss: 8.4429e-04 - 22s/epoch - 230us/sample
Epoch 46/140
95501/95501 - 22s - loss: 9.3200e-04 - val_loss: 8.0655e-04 - 22s/epoch - 229us/sample
Epoch 47/140
95501/95501 - 22s - loss: 9.1140e-04 - val_loss: 8.1059e-04 - 22s/epoch - 229us/sample
Epoch 48/140
95501/95501 - 22s - loss: 9.0049e-04 - val_loss: 7.9666e-04 - 22s/epoch - 231us/sample
Epoch 49/140
95501/95501 - 22s - loss: 9.1221e-04 - val_loss: 8.1228e-04 - 22s/epoch - 229us/sample
Epoch 50/140
95501/95501 - 22s - loss: 9.0268e-04 - val_loss: 7.8543e-04 - 22s/epoch - 229us/sample
Epoch 51/140
95501/95501 - 22s - loss: 9.0625e-04 - val_loss: 7.9737e-04 - 22s/epoch - 230us/sample
Epoch 52/140
95501/95501 - 22s - loss: 8.9351e-04 - val_loss: 7.9930e-04 - 22s/epoch - 230us/sample
Epoch 53/140
95501/95501 - 22s - loss: 8.9260e-04 - val_loss: 7.9469e-04 - 22s/epoch - 229us/sample
Epoch 54/140
95501/95501 - 22s - loss: 8.8891e-04 - val_loss: 7.8737e-04 - 22s/epoch - 229us/sample
Epoch 55/140
95501/95501 - 22s - loss: 8.9255e-04 - val_loss: 7.9333e-04 - 22s/epoch - 229us/sample
Epoch 56/140
95501/95501 - 22s - loss: 9.0060e-04 - val_loss: 7.9783e-04 - 22s/epoch - 231us/sample
Epoch 57/140
95501/95501 - 22s - loss: 8.9315e-04 - val_loss: 7.7408e-04 - 22s/epoch - 230us/sample
Epoch 58/140
95501/95501 - 22s - loss: 8.8283e-04 - val_loss: 7.7294e-04 - 22s/epoch - 229us/sample
Epoch 59/140
95501/95501 - 22s - loss: 8.9834e-04 - val_loss: 7.8380e-04 - 22s/epoch - 229us/sample
Epoch 60/140
95501/95501 - 22s - loss: 8.7787e-04 - val_loss: 9.1643e-04 - 22s/epoch - 231us/sample
Epoch 61/140
95501/95501 - 22s - loss: 8.8583e-04 - val_loss: 7.9340e-04 - 22s/epoch - 229us/sample
Epoch 62/140
95501/95501 - 22s - loss: 8.7697e-04 - val_loss: 7.7244e-04 - 22s/epoch - 229us/sample
Epoch 63/140
95501/95501 - 22s - loss: 8.7093e-04 - val_loss: 7.7638e-04 - 22s/epoch - 231us/sample
Epoch 64/140
95501/95501 - 22s - loss: 8.7212e-04 - val_loss: 7.7837e-04 - 22s/epoch - 230us/sample
Epoch 65/140
95501/95501 - 22s - loss: 8.7225e-04 - val_loss: 7.6816e-04 - 22s/epoch - 229us/sample
Epoch 66/140
95501/95501 - 22s - loss: 8.5815e-04 - val_loss: 7.5554e-04 - 22s/epoch - 230us/sample
Epoch 67/140
95501/95501 - 22s - loss: 8.6255e-04 - val_loss: 7.7708e-04 - 22s/epoch - 230us/sample
Epoch 68/140
95501/95501 - 22s - loss: 8.5998e-04 - val_loss: 7.6549e-04 - 22s/epoch - 231us/sample
Epoch 69/140
95501/95501 - 22s - loss: 8.5957e-04 - val_loss: 7.6046e-04 - 22s/epoch - 230us/sample
Epoch 70/140
95501/95501 - 22s - loss: 8.5534e-04 - val_loss: 7.4992e-04 - 22s/epoch - 230us/sample
Epoch 71/140
95501/95501 - 22s - loss: 8.5523e-04 - val_loss: 7.6408e-04 - 22s/epoch - 229us/sample
Epoch 72/140
95501/95501 - 22s - loss: 8.5938e-04 - val_loss: 7.6131e-04 - 22s/epoch - 231us/sample
Epoch 73/140
95501/95501 - 22s - loss: 8.6300e-04 - val_loss: 7.5305e-04 - 22s/epoch - 230us/sample
Epoch 74/140
95501/95501 - 22s - loss: 8.5018e-04 - val_loss: 7.5701e-04 - 22s/epoch - 229us/sample
Epoch 75/140
95501/95501 - 22s - loss: 8.4855e-04 - val_loss: 7.6092e-04 - 22s/epoch - 229us/sample
Epoch 76/140
95501/95501 - 22s - loss: 8.5701e-04 - val_loss: 8.3791e-04 - 22s/epoch - 231us/sample
Epoch 77/140
95501/95501 - 22s - loss: 8.4912e-04 - val_loss: 7.5784e-04 - 22s/epoch - 229us/sample
Epoch 78/140
95501/95501 - 22s - loss: 8.4250e-04 - val_loss: 7.7517e-04 - 22s/epoch - 229us/sample
Epoch 79/140
95501/95501 - 22s - loss: 8.7115e-04 - val_loss: 7.5259e-04 - 22s/epoch - 230us/sample
Epoch 80/140
95501/95501 - 22s - loss: 8.5258e-04 - val_loss: 7.5949e-04 - 22s/epoch - 231us/sample
Epoch 81/140
95501/95501 - 22s - loss: 8.4583e-04 - val_loss: 7.4739e-04 - 22s/epoch - 229us/sample
Epoch 82/140
95501/95501 - 22s - loss: 8.4304e-04 - val_loss: 7.5434e-04 - 22s/epoch - 230us/sample
Epoch 83/140
95501/95501 - 22s - loss: 8.4075e-04 - val_loss: 7.4887e-04 - 22s/epoch - 229us/sample
Epoch 84/140
95501/95501 - 22s - loss: 8.3670e-04 - val_loss: 7.4453e-04 - 22s/epoch - 231us/sample
Epoch 85/140
95501/95501 - 22s - loss: 8.3464e-04 - val_loss: 7.4159e-04 - 22s/epoch - 230us/sample
Epoch 86/140
95501/95501 - 22s - loss: 8.3528e-04 - val_loss: 7.5146e-04 - 22s/epoch - 229us/sample
Epoch 87/140
95501/95501 - 22s - loss: 8.4015e-04 - val_loss: 7.4898e-04 - 22s/epoch - 229us/sample
Epoch 88/140
95501/95501 - 22s - loss: 8.3474e-04 - val_loss: 8.4295e-04 - 22s/epoch - 231us/sample
Epoch 89/140
95501/95501 - 22s - loss: 8.8608e-04 - val_loss: 7.5111e-04 - 22s/epoch - 230us/sample
Epoch 90/140
95501/95501 - 22s - loss: 8.3683e-04 - val_loss: 7.3738e-04 - 22s/epoch - 229us/sample
Epoch 91/140
95501/95501 - 22s - loss: 8.4180e-04 - val_loss: 7.4497e-04 - 22s/epoch - 230us/sample
Epoch 92/140
95501/95501 - 22s - loss: 8.6839e-04 - val_loss: 7.4564e-04 - 22s/epoch - 231us/sample
Epoch 93/140
95501/95501 - 22s - loss: 8.4087e-04 - val_loss: 7.4621e-04 - 22s/epoch - 230us/sample
Epoch 94/140
95501/95501 - 22s - loss: 8.3563e-04 - val_loss: 7.4330e-04 - 22s/epoch - 230us/sample
Epoch 95/140
95501/95501 - 22s - loss: 8.3855e-04 - val_loss: 7.4517e-04 - 22s/epoch - 229us/sample
Epoch 96/140
95501/95501 - 22s - loss: 8.2850e-04 - val_loss: 7.4560e-04 - 22s/epoch - 231us/sample
Epoch 97/140
95501/95501 - 22s - loss: 8.2384e-04 - val_loss: 7.4989e-04 - 22s/epoch - 233us/sample
Epoch 98/140
95501/95501 - 23s - loss: 8.4192e-04 - val_loss: 7.3629e-04 - 23s/epoch - 237us/sample
Epoch 99/140
95501/95501 - 23s - loss: 8.2833e-04 - val_loss: 7.4958e-04 - 23s/epoch - 237us/sample
Epoch 100/140
95501/95501 - 23s - loss: 8.4182e-04 - val_loss: 7.5091e-04 - 23s/epoch - 237us/sample
Epoch 101/140
95501/95501 - 23s - loss: 8.2592e-04 - val_loss: 7.3406e-04 - 23s/epoch - 237us/sample
Epoch 102/140
95501/95501 - 23s - loss: 8.2907e-04 - val_loss: 7.4345e-04 - 23s/epoch - 236us/sample
Epoch 103/140
95501/95501 - 23s - loss: 8.3842e-04 - val_loss: 7.3408e-04 - 23s/epoch - 236us/sample
Epoch 104/140
95501/95501 - 23s - loss: 8.2317e-04 - val_loss: 7.9632e-04 - 23s/epoch - 238us/sample
Epoch 105/140
95501/95501 - 23s - loss: 8.2526e-04 - val_loss: 7.4625e-04 - 23s/epoch - 236us/sample
Epoch 106/140
95501/95501 - 23s - loss: 8.1712e-04 - val_loss: 7.4029e-04 - 23s/epoch - 237us/sample
Epoch 107/140
95501/95501 - 23s - loss: 8.2358e-04 - val_loss: 7.3843e-04 - 23s/epoch - 238us/sample
Epoch 108/140
95501/95501 - 23s - loss: 8.4684e-04 - val_loss: 7.2611e-04 - 23s/epoch - 236us/sample
Epoch 109/140
95501/95501 - 23s - loss: 8.1864e-04 - val_loss: 7.2994e-04 - 23s/epoch - 236us/sample
Epoch 110/140
95501/95501 - 23s - loss: 8.2573e-04 - val_loss: 7.3058e-04 - 23s/epoch - 236us/sample
Epoch 111/140
95501/95501 - 23s - loss: 8.2849e-04 - val_loss: 7.2604e-04 - 23s/epoch - 238us/sample
Epoch 112/140
95501/95501 - 23s - loss: 8.1718e-04 - val_loss: 7.3094e-04 - 23s/epoch - 237us/sample
Epoch 113/140
95501/95501 - 22s - loss: 8.1755e-04 - val_loss: 7.3902e-04 - 22s/epoch - 235us/sample
Epoch 114/140
95501/95501 - 23s - loss: 8.1138e-04 - val_loss: 7.3083e-04 - 23s/epoch - 236us/sample
Epoch 115/140
95501/95501 - 23s - loss: 8.1368e-04 - val_loss: 7.2720e-04 - 23s/epoch - 237us/sample
Epoch 116/140
95501/95501 - 23s - loss: 8.0550e-04 - val_loss: 7.2176e-04 - 23s/epoch - 236us/sample
Epoch 117/140
95501/95501 - 23s - loss: 8.0888e-04 - val_loss: 7.1507e-04 - 23s/epoch - 236us/sample
Epoch 118/140
95501/95501 - 23s - loss: 8.2581e-04 - val_loss: 7.2437e-04 - 23s/epoch - 236us/sample
Epoch 119/140
95501/95501 - 23s - loss: 8.1419e-04 - val_loss: 7.1356e-04 - 23s/epoch - 237us/sample
Epoch 120/140
95501/95501 - 22s - loss: 8.0669e-04 - val_loss: 7.3018e-04 - 22s/epoch - 236us/sample
Epoch 121/140
95501/95501 - 23s - loss: 8.1388e-04 - val_loss: 7.1752e-04 - 23s/epoch - 236us/sample
Epoch 122/140
95501/95501 - 23s - loss: 8.0964e-04 - val_loss: 7.2510e-04 - 23s/epoch - 236us/sample
Epoch 123/140
95501/95501 - 23s - loss: 8.1247e-04 - val_loss: 7.1720e-04 - 23s/epoch - 237us/sample
Epoch 124/140
95501/95501 - 23s - loss: 8.0509e-04 - val_loss: 7.1368e-04 - 23s/epoch - 236us/sample
Epoch 125/140
95501/95501 - 23s - loss: 8.0949e-04 - val_loss: 7.1830e-04 - 23s/epoch - 236us/sample
Epoch 126/140
95501/95501 - 23s - loss: 8.0013e-04 - val_loss: 7.1265e-04 - 23s/epoch - 237us/sample
Epoch 127/140
95501/95501 - 23s - loss: 8.0768e-04 - val_loss: 7.3583e-04 - 23s/epoch - 237us/sample
Epoch 128/140
95501/95501 - 23s - loss: 8.0414e-04 - val_loss: 7.2083e-04 - 23s/epoch - 236us/sample
Epoch 129/140
95501/95501 - 23s - loss: 7.9839e-04 - val_loss: 7.1263e-04 - 23s/epoch - 236us/sample
Epoch 130/140
95501/95501 - 23s - loss: 8.2544e-04 - val_loss: 7.1279e-04 - 23s/epoch - 238us/sample
Epoch 131/140
95501/95501 - 23s - loss: 7.9944e-04 - val_loss: 7.1084e-04 - 23s/epoch - 236us/sample
Epoch 132/140
95501/95501 - 23s - loss: 8.1180e-04 - val_loss: 7.1421e-04 - 23s/epoch - 236us/sample
Epoch 133/140
95501/95501 - 23s - loss: 8.0993e-04 - val_loss: 7.1468e-04 - 23s/epoch - 236us/sample
Epoch 134/140
95501/95501 - 23s - loss: 8.0578e-04 - val_loss: 7.8306e-04 - 23s/epoch - 238us/sample
Epoch 135/140
95501/95501 - 23s - loss: 8.0145e-04 - val_loss: 7.3341e-04 - 23s/epoch - 236us/sample
Epoch 136/140
95501/95501 - 23s - loss: 7.9635e-04 - val_loss: 7.2564e-04 - 23s/epoch - 236us/sample
Epoch 137/140
95501/95501 - 23s - loss: 8.0125e-04 - val_loss: 7.1212e-04 - 23s/epoch - 237us/sample
Epoch 138/140
95501/95501 - 23s - loss: 7.9623e-04 - val_loss: 7.0792e-04 - 23s/epoch - 237us/sample
Epoch 139/140
95501/95501 - 23s - loss: 8.1477e-04 - val_loss: 7.3530e-04 - 23s/epoch - 236us/sample
Epoch 140/140
95501/95501 - 23s - loss: 8.0454e-04 - val_loss: 7.1049e-04 - 23s/epoch - 236us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0007104862117058644
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 08:26:57.636754: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_26/outputlayer/BiasAdd' id:33968 op device:{requested: '', assigned: ''} def:{{{node decoder_model_26/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_26/outputlayer/MatMul, decoder_model_26/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.005205465433645195
cosine 0.004111701312983571
MAE: 0.011038667
RMSE: 0.021178627
r2: 0.9709030158081539
RMSE zero-vector: 0.23411466903540806
['1.6custom_VAE', 'mse', 32, 140, 0.0012, 0.6, 758, 0.0008045438476850021, 0.0007104862117058644, 0.005205465433645195, 0.004111701312983571, 0.01103866659104824, 0.021178627386689186, 0.9709030158081539, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6 145 0.0012 64 0] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2022)         2557830     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_81 (BatchN  (None, 2022)        8088        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_81 (ReLU)                (None, 2022)         0           ['batch_normalization_81[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1533434     ['re_lu_81[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1533434     ['re_lu_81[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4678212     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 10,310,998
Trainable params: 10,301,394
Non-trainable params: 9,604
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-15 08:27:13.126352: W tensorflow/c/c_api.cc:291] Operation '{name:'training_54/Adam/dense_dec0_27/bias/v/Assign' id:35990 op device:{requested: '', assigned: ''} def:{{{node training_54/Adam/dense_dec0_27/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_54/Adam/dense_dec0_27/bias/v, training_54/Adam/dense_dec0_27/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 08:27:29.744938: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_27/mul' id:35277 op device:{requested: '', assigned: ''} def:{{{node loss_27/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_27/mul/x, loss_27/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 22s - loss: 1.4819 - val_loss: 50.3517 - 22s/epoch - 233us/sample
Epoch 2/145
95501/95501 - 14s - loss: 1.3882 - val_loss: 1.4084 - 14s/epoch - 143us/sample
Epoch 3/145
95501/95501 - 14s - loss: 1.3701 - val_loss: 1.2841 - 14s/epoch - 144us/sample
Epoch 4/145
95501/95501 - 14s - loss: 0.9763 - val_loss: 0.6984 - 14s/epoch - 144us/sample
Epoch 5/145
95501/95501 - 14s - loss: 0.8284 - val_loss: 0.6950 - 14s/epoch - 144us/sample
Epoch 6/145
95501/95501 - 14s - loss: 0.6909 - val_loss: 0.6089 - 14s/epoch - 145us/sample
Epoch 7/145
95501/95501 - 14s - loss: 0.6015 - val_loss: 0.6463 - 14s/epoch - 144us/sample
Epoch 8/145
95501/95501 - 14s - loss: 0.5710 - val_loss: 0.5335 - 14s/epoch - 144us/sample
Epoch 9/145
95501/95501 - 14s - loss: 0.5443 - val_loss: 0.5049 - 14s/epoch - 144us/sample
Epoch 10/145
95501/95501 - 14s - loss: 0.5227 - val_loss: 0.4971 - 14s/epoch - 144us/sample
Epoch 11/145
95501/95501 - 14s - loss: 0.4912 - val_loss: 0.4884 - 14s/epoch - 144us/sample
Epoch 12/145
95501/95501 - 14s - loss: 0.4787 - val_loss: 0.4643 - 14s/epoch - 145us/sample
Epoch 13/145
95501/95501 - 14s - loss: 4.6342 - val_loss: 0.4503 - 14s/epoch - 144us/sample
Epoch 14/145
95501/95501 - 14s - loss: 0.4628 - val_loss: 0.4382 - 14s/epoch - 144us/sample
Epoch 15/145
95501/95501 - 14s - loss: 5.8192 - val_loss: 0.4003 - 14s/epoch - 144us/sample
Epoch 16/145
95501/95501 - 14s - loss: 0.4524 - val_loss: 0.4252 - 14s/epoch - 144us/sample
Epoch 17/145
95501/95501 - 14s - loss: 0.4251 - val_loss: 0.4365 - 14s/epoch - 144us/sample
Epoch 18/145
95501/95501 - 14s - loss: 0.4240 - val_loss: 0.4176 - 14s/epoch - 145us/sample
Epoch 19/145
95501/95501 - 14s - loss: 0.4140 - val_loss: 0.3988 - 14s/epoch - 144us/sample
Epoch 20/145
95501/95501 - 14s - loss: 0.4102 - val_loss: 0.3984 - 14s/epoch - 144us/sample
Epoch 21/145
95501/95501 - 14s - loss: 0.4265 - val_loss: 0.3957 - 14s/epoch - 144us/sample
Epoch 22/145
95501/95501 - 14s - loss: 0.4142 - val_loss: 0.3906 - 14s/epoch - 143us/sample
Epoch 23/145
95501/95501 - 14s - loss: 0.3949 - val_loss: 0.3808 - 14s/epoch - 144us/sample
Epoch 24/145
95501/95501 - 14s - loss: 0.3849 - val_loss: 0.3743 - 14s/epoch - 145us/sample
Epoch 25/145
95501/95501 - 14s - loss: 0.3844 - val_loss: 0.3763 - 14s/epoch - 144us/sample
Epoch 26/145
95501/95501 - 14s - loss: 0.3831 - val_loss: 0.3752 - 14s/epoch - 144us/sample
Epoch 27/145
95501/95501 - 14s - loss: 0.3782 - val_loss: 0.3690 - 14s/epoch - 144us/sample
Epoch 28/145
95501/95501 - 14s - loss: 0.3750 - val_loss: 0.3546 - 14s/epoch - 144us/sample
Epoch 29/145
95501/95501 - 14s - loss: 0.3718 - val_loss: 0.3525 - 14s/epoch - 145us/sample
Epoch 30/145
95501/95501 - 14s - loss: 0.3683 - val_loss: 0.3659 - 14s/epoch - 144us/sample
Epoch 31/145
95501/95501 - 14s - loss: 0.3664 - val_loss: 0.3571 - 14s/epoch - 144us/sample
Epoch 32/145
95501/95501 - 14s - loss: 0.3611 - val_loss: 0.3567 - 14s/epoch - 144us/sample
Epoch 33/145
95501/95501 - 14s - loss: 0.3532 - val_loss: 0.3542 - 14s/epoch - 144us/sample
Epoch 34/145
95501/95501 - 14s - loss: 0.3545 - val_loss: 0.3493 - 14s/epoch - 144us/sample
Epoch 35/145
95501/95501 - 14s - loss: 0.3512 - val_loss: 0.3452 - 14s/epoch - 145us/sample
Epoch 36/145
95501/95501 - 14s - loss: 0.3547 - val_loss: 0.3449 - 14s/epoch - 144us/sample
Epoch 37/145
95501/95501 - 14s - loss: 0.3462 - val_loss: 0.3441 - 14s/epoch - 144us/sample
Epoch 38/145
95501/95501 - 14s - loss: 0.3426 - val_loss: 0.3373 - 14s/epoch - 144us/sample
Epoch 39/145
95501/95501 - 14s - loss: 0.3433 - val_loss: 0.3390 - 14s/epoch - 144us/sample
Epoch 40/145
95501/95501 - 14s - loss: 214.3097 - val_loss: 0.3313 - 14s/epoch - 144us/sample
Epoch 41/145
95501/95501 - 14s - loss: 0.3313 - val_loss: 0.3253 - 14s/epoch - 145us/sample
Epoch 42/145
95501/95501 - 14s - loss: 0.3278 - val_loss: 0.3184 - 14s/epoch - 144us/sample
Epoch 43/145
95501/95501 - 14s - loss: 0.3142 - val_loss: 0.3132 - 14s/epoch - 144us/sample
Epoch 44/145
95501/95501 - 14s - loss: 0.3141 - val_loss: 0.3122 - 14s/epoch - 144us/sample
Epoch 45/145
95501/95501 - 14s - loss: 0.3809 - val_loss: 0.3104 - 14s/epoch - 143us/sample
Epoch 46/145
95501/95501 - 14s - loss: 0.3126 - val_loss: 0.3106 - 14s/epoch - 144us/sample
Epoch 47/145
95501/95501 - 14s - loss: 0.3094 - val_loss: 0.3104 - 14s/epoch - 145us/sample
Epoch 48/145
95501/95501 - 14s - loss: 0.3088 - val_loss: 0.3104 - 14s/epoch - 144us/sample
Epoch 49/145
95501/95501 - 14s - loss: 0.4076 - val_loss: 0.3100 - 14s/epoch - 144us/sample
Epoch 50/145
95501/95501 - 14s - loss: 0.3077 - val_loss: 0.3071 - 14s/epoch - 143us/sample
Epoch 51/145
95501/95501 - 14s - loss: 0.3079 - val_loss: 0.3128 - 14s/epoch - 144us/sample
Epoch 52/145
95501/95501 - 14s - loss: 0.3073 - val_loss: 0.3069 - 14s/epoch - 144us/sample
Epoch 53/145
95501/95501 - 14s - loss: 0.3062 - val_loss: 0.3042 - 14s/epoch - 145us/sample
Epoch 54/145
95501/95501 - 14s - loss: 0.3067 - val_loss: 0.3054 - 14s/epoch - 144us/sample
Epoch 55/145
95501/95501 - 14s - loss: 0.3056 - val_loss: 0.3056 - 14s/epoch - 144us/sample
Epoch 56/145
95501/95501 - 14s - loss: 0.3039 - val_loss: 0.3024 - 14s/epoch - 144us/sample
Epoch 57/145
95501/95501 - 14s - loss: 0.3040 - val_loss: 0.3018 - 14s/epoch - 143us/sample
Epoch 58/145
95501/95501 - 14s - loss: 0.3036 - val_loss: 0.3028 - 14s/epoch - 144us/sample
Epoch 59/145
95501/95501 - 14s - loss: 0.3071 - val_loss: 0.3039 - 14s/epoch - 145us/sample
Epoch 60/145
95501/95501 - 14s - loss: 0.3018 - val_loss: 0.3006 - 14s/epoch - 144us/sample
Epoch 61/145
95501/95501 - 14s - loss: 0.3050 - val_loss: 0.3027 - 14s/epoch - 144us/sample
Epoch 62/145
95501/95501 - 14s - loss: 0.3024 - val_loss: 0.3018 - 14s/epoch - 144us/sample
Epoch 63/145
95501/95501 - 14s - loss: 0.3010 - val_loss: 416.8197 - 14s/epoch - 144us/sample
Epoch 64/145
95501/95501 - 14s - loss: 0.3011 - val_loss: 0.2997 - 14s/epoch - 144us/sample
Epoch 65/145
95501/95501 - 14s - loss: 0.3010 - val_loss: 0.3003 - 14s/epoch - 145us/sample
Epoch 66/145
95501/95501 - 14s - loss: 0.3008 - val_loss: 0.3005 - 14s/epoch - 144us/sample
Epoch 67/145
95501/95501 - 14s - loss: 0.3017 - val_loss: 0.2999 - 14s/epoch - 143us/sample
Epoch 68/145
95501/95501 - 14s - loss: 0.3007 - val_loss: 0.3027 - 14s/epoch - 144us/sample
Epoch 69/145
95501/95501 - 14s - loss: 0.3007 - val_loss: 0.2996 - 14s/epoch - 144us/sample
Epoch 70/145
95501/95501 - 14s - loss: 0.2995 - val_loss: 0.3000 - 14s/epoch - 144us/sample
Epoch 71/145
95501/95501 - 14s - loss: 0.2999 - val_loss: 0.2998 - 14s/epoch - 145us/sample
Epoch 72/145
95501/95501 - 14s - loss: 0.2986 - val_loss: 0.2986 - 14s/epoch - 144us/sample
Epoch 73/145
95501/95501 - 14s - loss: 0.2986 - val_loss: 0.3091 - 14s/epoch - 144us/sample
Epoch 74/145
95501/95501 - 14s - loss: 0.2995 - val_loss: 0.2988 - 14s/epoch - 144us/sample
Epoch 75/145
95501/95501 - 14s - loss: 0.2986 - val_loss: 0.2991 - 14s/epoch - 144us/sample
Epoch 76/145
95501/95501 - 14s - loss: 0.2982 - val_loss: 0.2964 - 14s/epoch - 144us/sample
Epoch 77/145
95501/95501 - 14s - loss: 0.2975 - val_loss: 0.2968 - 14s/epoch - 144us/sample
Epoch 78/145
95501/95501 - 14s - loss: 0.2975 - val_loss: 0.2967 - 14s/epoch - 143us/sample
Epoch 79/145
95501/95501 - 14s - loss: 0.2991 - val_loss: 0.2986 - 14s/epoch - 143us/sample
Epoch 80/145
95501/95501 - 14s - loss: 0.2969 - val_loss: 0.2958 - 14s/epoch - 144us/sample
Epoch 81/145
95501/95501 - 14s - loss: 0.2963 - val_loss: 0.3056 - 14s/epoch - 143us/sample
Epoch 82/145
95501/95501 - 14s - loss: 0.2982 - val_loss: 0.2993 - 14s/epoch - 145us/sample
Epoch 83/145
95501/95501 - 14s - loss: 0.2980 - val_loss: 0.2971 - 14s/epoch - 144us/sample
Epoch 84/145
95501/95501 - 14s - loss: 0.2976 - val_loss: 0.2972 - 14s/epoch - 144us/sample
Epoch 85/145
95501/95501 - 14s - loss: 0.2964 - val_loss: 0.2961 - 14s/epoch - 143us/sample
Epoch 86/145
95501/95501 - 14s - loss: 0.2975 - val_loss: 0.2972 - 14s/epoch - 144us/sample
Epoch 87/145
95501/95501 - 14s - loss: 0.2967 - val_loss: 0.2977 - 14s/epoch - 144us/sample
Epoch 88/145
95501/95501 - 14s - loss: 0.2985 - val_loss: 0.2976 - 14s/epoch - 145us/sample
Epoch 89/145
95501/95501 - 14s - loss: 0.2970 - val_loss: 0.2967 - 14s/epoch - 144us/sample
Epoch 90/145
95501/95501 - 14s - loss: 0.2973 - val_loss: 0.2960 - 14s/epoch - 143us/sample
Epoch 91/145
95501/95501 - 14s - loss: 0.2979 - val_loss: 0.3119 - 14s/epoch - 144us/sample
Epoch 92/145
95501/95501 - 14s - loss: 0.3020 - val_loss: 0.2970 - 14s/epoch - 143us/sample
Epoch 93/145
95501/95501 - 14s - loss: 0.2975 - val_loss: 0.2969 - 14s/epoch - 144us/sample
Epoch 94/145
95501/95501 - 14s - loss: 0.2969 - val_loss: 0.3041 - 14s/epoch - 145us/sample
Epoch 95/145
95501/95501 - 14s - loss: 0.2982 - val_loss: 0.2968 - 14s/epoch - 144us/sample
Epoch 96/145
95501/95501 - 14s - loss: 0.2965 - val_loss: 0.2961 - 14s/epoch - 144us/sample
Epoch 97/145
95501/95501 - 14s - loss: 0.2976 - val_loss: 0.2980 - 14s/epoch - 144us/sample
Epoch 98/145
95501/95501 - 14s - loss: 0.2976 - val_loss: 0.3016 - 14s/epoch - 143us/sample
Epoch 99/145
95501/95501 - 14s - loss: 0.2962 - val_loss: 0.2960 - 14s/epoch - 144us/sample
Epoch 100/145
95501/95501 - 14s - loss: 0.2957 - val_loss: 0.2970 - 14s/epoch - 144us/sample
Epoch 101/145
95501/95501 - 14s - loss: 0.2959 - val_loss: 0.2956 - 14s/epoch - 143us/sample
Epoch 102/145
95501/95501 - 14s - loss: 0.2977 - val_loss: 0.2974 - 14s/epoch - 144us/sample
Epoch 103/145
95501/95501 - 14s - loss: 0.2966 - val_loss: 0.2963 - 14s/epoch - 144us/sample
Epoch 104/145
95501/95501 - 14s - loss: 0.2958 - val_loss: 0.2952 - 14s/epoch - 144us/sample
Epoch 105/145
95501/95501 - 14s - loss: 0.2955 - val_loss: 0.2950 - 14s/epoch - 145us/sample
Epoch 106/145
95501/95501 - 14s - loss: 0.2963 - val_loss: 0.2969 - 14s/epoch - 144us/sample
Epoch 107/145
95501/95501 - 14s - loss: 0.2970 - val_loss: 0.2967 - 14s/epoch - 143us/sample
Epoch 108/145
95501/95501 - 14s - loss: 0.2959 - val_loss: 0.2960 - 14s/epoch - 144us/sample
Epoch 109/145
95501/95501 - 14s - loss: 0.2960 - val_loss: 0.2972 - 14s/epoch - 143us/sample
Epoch 110/145
95501/95501 - 14s - loss: 0.2959 - val_loss: 0.2948 - 14s/epoch - 144us/sample
Epoch 111/145
95501/95501 - 14s - loss: 0.2965 - val_loss: 0.2967 - 14s/epoch - 145us/sample
Epoch 112/145
95501/95501 - 14s - loss: 0.2958 - val_loss: 0.2949 - 14s/epoch - 143us/sample
Epoch 113/145
95501/95501 - 14s - loss: 0.2965 - val_loss: 0.2967 - 14s/epoch - 143us/sample
Epoch 114/145
95501/95501 - 14s - loss: 0.2962 - val_loss: 0.2951 - 14s/epoch - 144us/sample
Epoch 115/145
95501/95501 - 14s - loss: 0.2960 - val_loss: 0.2966 - 14s/epoch - 144us/sample
Epoch 116/145
95501/95501 - 14s - loss: 0.2955 - val_loss: 0.2974 - 14s/epoch - 145us/sample
Epoch 117/145
95501/95501 - 14s - loss: 0.2957 - val_loss: 0.2953 - 14s/epoch - 144us/sample
Epoch 118/145
95501/95501 - 14s - loss: 0.2953 - val_loss: 0.2949 - 14s/epoch - 143us/sample
Epoch 119/145
95501/95501 - 14s - loss: 0.2949 - val_loss: 0.2950 - 14s/epoch - 143us/sample
Epoch 120/145
95501/95501 - 14s - loss: 0.2948 - val_loss: 0.2954 - 14s/epoch - 144us/sample
Epoch 121/145
95501/95501 - 14s - loss: 0.2950 - val_loss: 0.2954 - 14s/epoch - 144us/sample
Epoch 122/145
95501/95501 - 14s - loss: 0.2950 - val_loss: 0.2955 - 14s/epoch - 145us/sample
Epoch 123/145
95501/95501 - 14s - loss: 0.2951 - val_loss: 0.2966 - 14s/epoch - 144us/sample
Epoch 124/145
95501/95501 - 14s - loss: 0.2963 - val_loss: 0.2949 - 14s/epoch - 143us/sample
Epoch 125/145
95501/95501 - 14s - loss: 0.2945 - val_loss: 0.2943 - 14s/epoch - 144us/sample
Epoch 126/145
95501/95501 - 14s - loss: 0.2946 - val_loss: 0.2941 - 14s/epoch - 144us/sample
Epoch 127/145
95501/95501 - 14s - loss: 0.2952 - val_loss: 0.2960 - 14s/epoch - 144us/sample
Epoch 128/145
95501/95501 - 14s - loss: 0.2945 - val_loss: 0.2941 - 14s/epoch - 145us/sample
Epoch 129/145
95501/95501 - 14s - loss: 0.2951 - val_loss: 0.2948 - 14s/epoch - 144us/sample
Epoch 130/145
95501/95501 - 14s - loss: 0.2939 - val_loss: 0.2955 - 14s/epoch - 144us/sample
Epoch 131/145
95501/95501 - 14s - loss: 0.2952 - val_loss: 0.2960 - 14s/epoch - 144us/sample
Epoch 132/145
95501/95501 - 14s - loss: 0.2945 - val_loss: 0.2941 - 14s/epoch - 144us/sample
Epoch 133/145
95501/95501 - 14s - loss: 0.2949 - val_loss: 0.2943 - 14s/epoch - 144us/sample
Epoch 134/145
95501/95501 - 14s - loss: 0.2948 - val_loss: 0.2953 - 14s/epoch - 145us/sample
Epoch 135/145
95501/95501 - 14s - loss: 0.2941 - val_loss: 0.2939 - 14s/epoch - 143us/sample
Epoch 136/145
95501/95501 - 14s - loss: 0.2936 - val_loss: 0.2942 - 14s/epoch - 144us/sample
Epoch 137/145
95501/95501 - 14s - loss: 0.2937 - val_loss: 0.2942 - 14s/epoch - 144us/sample
Epoch 138/145
95501/95501 - 14s - loss: 0.2947 - val_loss: 0.2945 - 14s/epoch - 144us/sample
Epoch 139/145
95501/95501 - 14s - loss: 0.2935 - val_loss: 0.2932 - 14s/epoch - 144us/sample
Epoch 140/145
95501/95501 - 14s - loss: 0.2934 - val_loss: 0.2929 - 14s/epoch - 144us/sample
Epoch 141/145
95501/95501 - 14s - loss: 0.2941 - val_loss: 0.2952 - 14s/epoch - 143us/sample
Epoch 142/145
95501/95501 - 14s - loss: 0.2942 - val_loss: 0.2938 - 14s/epoch - 144us/sample
Epoch 143/145
95501/95501 - 14s - loss: 0.2938 - val_loss: 0.2932 - 14s/epoch - 143us/sample
Epoch 144/145
95501/95501 - 14s - loss: 0.2948 - val_loss: 0.2954 - 14s/epoch - 144us/sample
Epoch 145/145
95501/95501 - 14s - loss: 0.2940 - val_loss: 0.2946 - 14s/epoch - 144us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.2945899960121208
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 09:00:31.377446: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_27/outputlayer/BiasAdd' id:35229 op device:{requested: '', assigned: ''} def:{{{node decoder_model_27/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_27/outputlayer/MatMul, decoder_model_27/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.1629679453202207
cosine 0.20323083376462084
MAE: 0.1051527
RMSE: 0.1730122
r2: -0.9403929837626145
RMSE zero-vector: 0.23411466903540806
['1.6custom_VAE', 'binary_crossentropy', 64, 145, 0.0012, 0.6, 758, 0.29399754296373837, 0.2945899960121208, 0.1629679453202207, 0.20323083376462084, 0.10515269637107849, 0.17301219701766968, -0.9403929837626145, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.7000000000000002 85 0.001 64 2] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2148)         2717220     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_84 (BatchN  (None, 2148)        8592        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_84 (ReLU)                (None, 2148)         0           ['batch_normalization_84[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1628942     ['re_lu_84[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1628942     ['re_lu_84[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4933614     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 10,917,310
Trainable params: 10,907,202
Non-trainable params: 10,108
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/85
2023-02-15 09:00:47.398728: W tensorflow/c/c_api.cc:291] Operation '{name:'training_56/Adam/bottleneck_zmean_28/kernel/m/Assign' id:37112 op device:{requested: '', assigned: ''} def:{{{node training_56/Adam/bottleneck_zmean_28/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_56/Adam/bottleneck_zmean_28/kernel/m, training_56/Adam/bottleneck_zmean_28/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 09:01:04.099167: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_28/mul' id:36608 op device:{requested: '', assigned: ''} def:{{{node loss_28/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_28/mul/x, loss_28/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 23s - loss: 2.1811 - val_loss: 0.0029 - 23s/epoch - 236us/sample
Epoch 2/85
95501/95501 - 14s - loss: 0.0028 - val_loss: 0.0022 - 14s/epoch - 142us/sample
Epoch 3/85
95501/95501 - 14s - loss: 0.0021 - val_loss: 0.0018 - 14s/epoch - 142us/sample
Epoch 4/85
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0015 - 14s/epoch - 143us/sample
Epoch 5/85
95501/95501 - 14s - loss: 0.0016 - val_loss: 0.0014 - 14s/epoch - 142us/sample
Epoch 6/85
95501/95501 - 14s - loss: 0.0014 - val_loss: 0.0014 - 14s/epoch - 142us/sample
Epoch 7/85
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0010 - 14s/epoch - 142us/sample
Epoch 8/85
95501/95501 - 14s - loss: 0.0011 - val_loss: 9.4549e-04 - 14s/epoch - 142us/sample
Epoch 9/85
95501/95501 - 14s - loss: 0.0010 - val_loss: 8.9055e-04 - 14s/epoch - 142us/sample
Epoch 10/85
95501/95501 - 14s - loss: 9.5860e-04 - val_loss: 0.0012 - 14s/epoch - 143us/sample
Epoch 11/85
95501/95501 - 14s - loss: 9.3916e-04 - val_loss: 8.9329e-04 - 14s/epoch - 142us/sample
Epoch 12/85
95501/95501 - 14s - loss: 8.3938e-04 - val_loss: 8.2259e-04 - 14s/epoch - 142us/sample
Epoch 13/85
95501/95501 - 14s - loss: 8.3122e-04 - val_loss: 7.2733e-04 - 14s/epoch - 142us/sample
Epoch 14/85
95501/95501 - 14s - loss: 7.7826e-04 - val_loss: 8.7367e-04 - 14s/epoch - 142us/sample
Epoch 15/85
95501/95501 - 14s - loss: 7.6631e-04 - val_loss: 6.9654e-04 - 14s/epoch - 143us/sample
Epoch 16/85
95501/95501 - 14s - loss: 7.3133e-04 - val_loss: 7.1043e-04 - 14s/epoch - 143us/sample
Epoch 17/85
95501/95501 - 14s - loss: 7.0212e-04 - val_loss: 9.1887e-04 - 14s/epoch - 142us/sample
Epoch 18/85
95501/95501 - 14s - loss: 7.3561e-04 - val_loss: 6.7179e-04 - 14s/epoch - 143us/sample
Epoch 19/85
95501/95501 - 14s - loss: 6.7655e-04 - val_loss: 6.3004e-04 - 14s/epoch - 142us/sample
Epoch 20/85
95501/95501 - 14s - loss: 6.5102e-04 - val_loss: 6.0343e-04 - 14s/epoch - 142us/sample
Epoch 21/85
95501/95501 - 14s - loss: 6.3704e-04 - val_loss: 5.8852e-04 - 14s/epoch - 143us/sample
Epoch 22/85
95501/95501 - 14s - loss: 6.3778e-04 - val_loss: 6.9413e-04 - 14s/epoch - 143us/sample
Epoch 23/85
95501/95501 - 14s - loss: 6.2108e-04 - val_loss: 6.1149e-04 - 14s/epoch - 142us/sample
Epoch 24/85
95501/95501 - 14s - loss: 6.0403e-04 - val_loss: 5.6515e-04 - 14s/epoch - 143us/sample
Epoch 25/85
95501/95501 - 14s - loss: 5.9272e-04 - val_loss: 0.0011 - 14s/epoch - 142us/sample
Epoch 26/85
95501/95501 - 14s - loss: 8.6394e-04 - val_loss: 6.0871e-04 - 14s/epoch - 142us/sample
Epoch 27/85
95501/95501 - 14s - loss: 6.2487e-04 - val_loss: 5.6136e-04 - 14s/epoch - 143us/sample
Epoch 28/85
95501/95501 - 14s - loss: 5.9087e-04 - val_loss: 5.4577e-04 - 14s/epoch - 143us/sample
Epoch 29/85
95501/95501 - 14s - loss: 5.8311e-04 - val_loss: 9.3809e-04 - 14s/epoch - 142us/sample
Epoch 30/85
95501/95501 - 14s - loss: 7.5541e-04 - val_loss: 5.6169e-04 - 14s/epoch - 143us/sample
Epoch 31/85
95501/95501 - 14s - loss: 6.0614e-04 - val_loss: 5.4709e-04 - 14s/epoch - 142us/sample
Epoch 32/85
95501/95501 - 14s - loss: 5.7242e-04 - val_loss: 5.3923e-04 - 14s/epoch - 143us/sample
Epoch 33/85
95501/95501 - 14s - loss: 5.7571e-04 - val_loss: 5.4610e-04 - 14s/epoch - 144us/sample
Epoch 34/85
95501/95501 - 14s - loss: 5.5938e-04 - val_loss: 5.3258e-04 - 14s/epoch - 147us/sample
Epoch 35/85
95501/95501 - 14s - loss: 5.5641e-04 - val_loss: 5.1954e-04 - 14s/epoch - 146us/sample
Epoch 36/85
95501/95501 - 14s - loss: 5.4844e-04 - val_loss: 5.1644e-04 - 14s/epoch - 147us/sample
Epoch 37/85
95501/95501 - 14s - loss: 5.5034e-04 - val_loss: 5.3666e-04 - 14s/epoch - 147us/sample
Epoch 38/85
95501/95501 - 14s - loss: 5.4890e-04 - val_loss: 5.1354e-04 - 14s/epoch - 147us/sample
Epoch 39/85
95501/95501 - 14s - loss: 5.3717e-04 - val_loss: 5.1163e-04 - 14s/epoch - 148us/sample
Epoch 40/85
95501/95501 - 14s - loss: 5.3968e-04 - val_loss: 5.0081e-04 - 14s/epoch - 147us/sample
Epoch 41/85
95501/95501 - 14s - loss: 5.2976e-04 - val_loss: 5.0358e-04 - 14s/epoch - 147us/sample
Epoch 42/85
95501/95501 - 14s - loss: 5.2660e-04 - val_loss: 5.0370e-04 - 14s/epoch - 147us/sample
Epoch 43/85
95501/95501 - 14s - loss: 5.4798e-04 - val_loss: 5.6196e-04 - 14s/epoch - 146us/sample
Epoch 44/85
95501/95501 - 14s - loss: 5.4575e-04 - val_loss: 4.9209e-04 - 14s/epoch - 147us/sample
Epoch 45/85
95501/95501 - 14s - loss: 5.2262e-04 - val_loss: 5.3693e-04 - 14s/epoch - 148us/sample
Epoch 46/85
95501/95501 - 14s - loss: 5.5724e-04 - val_loss: 6.2807e-04 - 14s/epoch - 147us/sample
Epoch 47/85
95501/95501 - 14s - loss: 6.0782e-04 - val_loss: 5.0521e-04 - 14s/epoch - 147us/sample
Epoch 48/85
95501/95501 - 14s - loss: 5.2798e-04 - val_loss: 4.9342e-04 - 14s/epoch - 147us/sample
Epoch 49/85
95501/95501 - 14s - loss: 5.2036e-04 - val_loss: 4.9329e-04 - 14s/epoch - 147us/sample
Epoch 50/85
95501/95501 - 14s - loss: 5.2206e-04 - val_loss: 7.2846e-04 - 14s/epoch - 148us/sample
Epoch 51/85
95501/95501 - 14s - loss: 6.2092e-04 - val_loss: 6.9620e-04 - 14s/epoch - 147us/sample
Epoch 52/85
95501/95501 - 14s - loss: 6.4996e-04 - val_loss: 5.6085e-04 - 14s/epoch - 147us/sample
Epoch 53/85
95501/95501 - 14s - loss: 5.8877e-04 - val_loss: 5.0463e-04 - 14s/epoch - 147us/sample
Epoch 54/85
95501/95501 - 14s - loss: 5.2750e-04 - val_loss: 4.9404e-04 - 14s/epoch - 147us/sample
Epoch 55/85
95501/95501 - 14s - loss: 5.2107e-04 - val_loss: 4.9964e-04 - 14s/epoch - 147us/sample
Epoch 56/85
95501/95501 - 14s - loss: 5.3583e-04 - val_loss: 5.5122e-04 - 14s/epoch - 148us/sample
Epoch 57/85
95501/95501 - 14s - loss: 5.6032e-04 - val_loss: 5.1837e-04 - 14s/epoch - 147us/sample
Epoch 58/85
95501/95501 - 14s - loss: 5.3789e-04 - val_loss: 4.8498e-04 - 14s/epoch - 147us/sample
Epoch 59/85
95501/95501 - 14s - loss: 5.1242e-04 - val_loss: 4.8198e-04 - 14s/epoch - 146us/sample
Epoch 60/85
95501/95501 - 14s - loss: 5.0817e-04 - val_loss: 5.1924e-04 - 14s/epoch - 147us/sample
Epoch 61/85
95501/95501 - 14s - loss: 5.4214e-04 - val_loss: 4.7934e-04 - 14s/epoch - 147us/sample
Epoch 62/85
95501/95501 - 14s - loss: 5.0842e-04 - val_loss: 4.8084e-04 - 14s/epoch - 148us/sample
Epoch 63/85
95501/95501 - 14s - loss: 5.2870e-04 - val_loss: 7.0961e-04 - 14s/epoch - 147us/sample
Epoch 64/85
95501/95501 - 14s - loss: 5.1500e-04 - val_loss: 6.0980e-04 - 14s/epoch - 147us/sample
Epoch 65/85
95501/95501 - 14s - loss: 5.0601e-04 - val_loss: 4.7534e-04 - 14s/epoch - 147us/sample
Epoch 66/85
95501/95501 - 14s - loss: 4.9862e-04 - val_loss: 5.0361e-04 - 14s/epoch - 147us/sample
Epoch 67/85
95501/95501 - 14s - loss: 5.1355e-04 - val_loss: 4.7563e-04 - 14s/epoch - 148us/sample
Epoch 68/85
95501/95501 - 14s - loss: 4.9733e-04 - val_loss: 4.7576e-04 - 14s/epoch - 147us/sample
Epoch 69/85
95501/95501 - 14s - loss: 4.9147e-04 - val_loss: 4.8212e-04 - 14s/epoch - 147us/sample
Epoch 70/85
95501/95501 - 14s - loss: 4.9111e-04 - val_loss: 4.7124e-04 - 14s/epoch - 147us/sample
Epoch 71/85
95501/95501 - 14s - loss: 5.2485e-04 - val_loss: 4.9072e-04 - 14s/epoch - 146us/sample
Epoch 72/85
95501/95501 - 14s - loss: 5.0025e-04 - val_loss: 5.5167e-04 - 14s/epoch - 147us/sample
Epoch 73/85
95501/95501 - 14s - loss: 5.5448e-04 - val_loss: 4.8761e-04 - 14s/epoch - 148us/sample
Epoch 74/85
95501/95501 - 14s - loss: 5.1049e-04 - val_loss: 4.6799e-04 - 14s/epoch - 147us/sample
Epoch 75/85
95501/95501 - 14s - loss: 4.9222e-04 - val_loss: 4.7677e-04 - 14s/epoch - 146us/sample
Epoch 76/85
95501/95501 - 14s - loss: 4.9372e-04 - val_loss: 4.6728e-04 - 14s/epoch - 147us/sample
Epoch 77/85
95501/95501 - 14s - loss: 4.8697e-04 - val_loss: 4.8105e-04 - 14s/epoch - 147us/sample
Epoch 78/85
95501/95501 - 14s - loss: 5.0166e-04 - val_loss: 5.0181e-04 - 14s/epoch - 147us/sample
Epoch 79/85
95501/95501 - 14s - loss: 5.2035e-04 - val_loss: 4.6362e-04 - 14s/epoch - 148us/sample
Epoch 80/85
95501/95501 - 14s - loss: 4.8730e-04 - val_loss: 4.5939e-04 - 14s/epoch - 147us/sample
Epoch 81/85
95501/95501 - 14s - loss: 4.8470e-04 - val_loss: 5.0634e-04 - 14s/epoch - 147us/sample
Epoch 82/85
95501/95501 - 14s - loss: 4.9911e-04 - val_loss: 4.6323e-04 - 14s/epoch - 146us/sample
Epoch 83/85
95501/95501 - 14s - loss: 4.8247e-04 - val_loss: 4.6188e-04 - 14s/epoch - 147us/sample
Epoch 84/85
95501/95501 - 14s - loss: 4.8317e-04 - val_loss: 4.5576e-04 - 14s/epoch - 147us/sample
Epoch 85/85
95501/95501 - 14s - loss: 4.7999e-04 - val_loss: 5.0584e-04 - 14s/epoch - 147us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0005058398244253409
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 09:20:32.210715: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_28/outputlayer/BiasAdd' id:36572 op device:{requested: '', assigned: ''} def:{{{node decoder_model_28/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_28/outputlayer/MatMul, decoder_model_28/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.006914604858232362
cosine 0.005460332384030413
MAE: 0.013970997
RMSE: 0.02454636
r2: 0.9609131447767761
RMSE zero-vector: 0.23411466903540806
['1.7000000000000002custom_VAE', 'logcosh', 64, 85, 0.001, 0.6, 758, 0.00047998528630831776, 0.0005058398244253409, 0.006914604858232362, 0.005460332384030413, 0.013970997184515, 0.024546360597014427, 0.9609131447767761, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.7000000000000002 85 0.0012000000000000001 64 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2148)         2717220     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_87 (BatchN  (None, 2148)        8592        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_87 (ReLU)                (None, 2148)         0           ['batch_normalization_87[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1628942     ['re_lu_87[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1628942     ['re_lu_87[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4933614     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 10,917,310
Trainable params: 10,907,202
Non-trainable params: 10,108
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/85
2023-02-15 09:20:48.942251: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_89/moving_variance/Assign' id:37720 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_89/moving_variance/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_89/moving_variance, batch_normalization_89/moving_variance/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 09:21:06.093358: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_29/mul' id:37892 op device:{requested: '', assigned: ''} def:{{{node loss_29/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_29/mul/x, loss_29/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 23s - loss: 0.0112 - val_loss: 0.0049 - 23s/epoch - 245us/sample
Epoch 2/85
95501/95501 - 14s - loss: 0.0044 - val_loss: 0.0081 - 14s/epoch - 147us/sample
Epoch 3/85
95501/95501 - 14s - loss: 0.0041 - val_loss: 0.0052 - 14s/epoch - 147us/sample
Epoch 4/85
95501/95501 - 14s - loss: 0.0036 - val_loss: 0.0029 - 14s/epoch - 147us/sample
Epoch 5/85
95501/95501 - 14s - loss: 0.0029 - val_loss: 0.0029 - 14s/epoch - 147us/sample
Epoch 6/85
95501/95501 - 14s - loss: 0.0025 - val_loss: 0.0020 - 14s/epoch - 147us/sample
Epoch 7/85
95501/95501 - 14s - loss: 0.0022 - val_loss: 0.0017 - 14s/epoch - 147us/sample
Epoch 8/85
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0019 - 14s/epoch - 147us/sample
Epoch 9/85
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0015 - 14s/epoch - 148us/sample
Epoch 10/85
95501/95501 - 14s - loss: 0.0016 - val_loss: 0.0014 - 14s/epoch - 147us/sample
Epoch 11/85
95501/95501 - 14s - loss: 0.0014 - val_loss: 0.0013 - 14s/epoch - 147us/sample
Epoch 12/85
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0012 - 14s/epoch - 146us/sample
Epoch 13/85
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0016 - 14s/epoch - 146us/sample
Epoch 14/85
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0011 - 14s/epoch - 147us/sample
Epoch 15/85
95501/95501 - 14s - loss: 0.0011 - val_loss: 0.0011 - 14s/epoch - 147us/sample
Epoch 16/85
95501/95501 - 14s - loss: 0.0011 - val_loss: 0.0010 - 14s/epoch - 147us/sample
Epoch 17/85
95501/95501 - 14s - loss: 0.0011 - val_loss: 9.5639e-04 - 14s/epoch - 146us/sample
Epoch 18/85
95501/95501 - 14s - loss: 0.0010 - val_loss: 0.0011 - 14s/epoch - 147us/sample
Epoch 19/85
95501/95501 - 14s - loss: 0.0010 - val_loss: 0.0011 - 14s/epoch - 147us/sample
Epoch 20/85
95501/95501 - 14s - loss: 0.0011 - val_loss: 9.1434e-04 - 14s/epoch - 148us/sample
Epoch 21/85
95501/95501 - 14s - loss: 9.7734e-04 - val_loss: 9.1142e-04 - 14s/epoch - 147us/sample
Epoch 22/85
95501/95501 - 14s - loss: 9.6703e-04 - val_loss: 8.9906e-04 - 14s/epoch - 147us/sample
Epoch 23/85
95501/95501 - 14s - loss: 9.3379e-04 - val_loss: 8.5760e-04 - 14s/epoch - 147us/sample
Epoch 24/85
95501/95501 - 14s - loss: 9.2669e-04 - val_loss: 0.0013 - 14s/epoch - 147us/sample
Epoch 25/85
95501/95501 - 14s - loss: 0.0010 - val_loss: 9.0815e-04 - 14s/epoch - 147us/sample
Epoch 26/85
95501/95501 - 14s - loss: 9.5640e-04 - val_loss: 9.9170e-04 - 14s/epoch - 148us/sample
Epoch 27/85
95501/95501 - 14s - loss: 9.5814e-04 - val_loss: 9.5218e-04 - 14s/epoch - 147us/sample
Epoch 28/85
95501/95501 - 14s - loss: 0.0010 - val_loss: 0.0010 - 14s/epoch - 147us/sample
Epoch 29/85
95501/95501 - 14s - loss: 0.0011 - val_loss: 8.6404e-04 - 14s/epoch - 147us/sample
Epoch 30/85
95501/95501 - 14s - loss: 8.9238e-04 - val_loss: 8.1950e-04 - 14s/epoch - 147us/sample
Epoch 31/85
95501/95501 - 14s - loss: 8.7572e-04 - val_loss: 8.3086e-04 - 14s/epoch - 147us/sample
Epoch 32/85
95501/95501 - 14s - loss: 8.6239e-04 - val_loss: 8.1300e-04 - 14s/epoch - 147us/sample
Epoch 33/85
95501/95501 - 14s - loss: 8.6336e-04 - val_loss: 7.9916e-04 - 14s/epoch - 146us/sample
Epoch 34/85
95501/95501 - 14s - loss: 8.6786e-04 - val_loss: 0.0011 - 14s/epoch - 147us/sample
Epoch 35/85
95501/95501 - 14s - loss: 0.0011 - val_loss: 8.2539e-04 - 14s/epoch - 147us/sample
Epoch 36/85
95501/95501 - 14s - loss: 8.6052e-04 - val_loss: 7.8295e-04 - 14s/epoch - 147us/sample
Epoch 37/85
95501/95501 - 14s - loss: 8.4632e-04 - val_loss: 7.8829e-04 - 14s/epoch - 148us/sample
Epoch 38/85
95501/95501 - 14s - loss: 8.4951e-04 - val_loss: 0.0013 - 14s/epoch - 147us/sample
Epoch 39/85
95501/95501 - 14s - loss: 0.0012 - val_loss: 9.3014e-04 - 14s/epoch - 147us/sample
Epoch 40/85
95501/95501 - 14s - loss: 9.9279e-04 - val_loss: 0.0017 - 14s/epoch - 147us/sample
Epoch 41/85
95501/95501 - 14s - loss: 0.0013 - val_loss: 8.5801e-04 - 14s/epoch - 147us/sample
Epoch 42/85
95501/95501 - 14s - loss: 9.0203e-04 - val_loss: 8.1098e-04 - 14s/epoch - 147us/sample
Epoch 43/85
95501/95501 - 14s - loss: 8.7151e-04 - val_loss: 8.1763e-04 - 14s/epoch - 147us/sample
Epoch 44/85
95501/95501 - 14s - loss: 8.5399e-04 - val_loss: 7.9420e-04 - 14s/epoch - 147us/sample
Epoch 45/85
95501/95501 - 14s - loss: 8.3990e-04 - val_loss: 7.8567e-04 - 14s/epoch - 147us/sample
Epoch 46/85
95501/95501 - 14s - loss: 8.6021e-04 - val_loss: 7.6791e-04 - 14s/epoch - 147us/sample
Epoch 47/85
95501/95501 - 14s - loss: 8.1840e-04 - val_loss: 7.6858e-04 - 14s/epoch - 147us/sample
Epoch 48/85
95501/95501 - 14s - loss: 8.0972e-04 - val_loss: 7.5941e-04 - 14s/epoch - 148us/sample
Epoch 49/85
95501/95501 - 14s - loss: 8.2835e-04 - val_loss: 7.6025e-04 - 14s/epoch - 147us/sample
Epoch 50/85
95501/95501 - 14s - loss: 8.1750e-04 - val_loss: 7.4991e-04 - 14s/epoch - 147us/sample
Epoch 51/85
95501/95501 - 14s - loss: 7.9488e-04 - val_loss: 7.4564e-04 - 14s/epoch - 146us/sample
Epoch 52/85
95501/95501 - 14s - loss: 7.8630e-04 - val_loss: 8.0577e-04 - 14s/epoch - 147us/sample
Epoch 53/85
95501/95501 - 14s - loss: 8.2114e-04 - val_loss: 7.4586e-04 - 14s/epoch - 147us/sample
Epoch 54/85
95501/95501 - 14s - loss: 8.1040e-04 - val_loss: 7.3317e-04 - 14s/epoch - 148us/sample
Epoch 55/85
95501/95501 - 14s - loss: 7.8414e-04 - val_loss: 8.6224e-04 - 14s/epoch - 147us/sample
Epoch 56/85
95501/95501 - 14s - loss: 8.7074e-04 - val_loss: 7.4352e-04 - 14s/epoch - 147us/sample
Epoch 57/85
95501/95501 - 14s - loss: 7.9026e-04 - val_loss: 7.2776e-04 - 14s/epoch - 147us/sample
Epoch 58/85
95501/95501 - 14s - loss: 7.7101e-04 - val_loss: 7.4549e-04 - 14s/epoch - 147us/sample
Epoch 59/85
95501/95501 - 14s - loss: 7.8491e-04 - val_loss: 7.2999e-04 - 14s/epoch - 147us/sample
Epoch 60/85
95501/95501 - 14s - loss: 7.6811e-04 - val_loss: 7.4170e-04 - 14s/epoch - 148us/sample
Epoch 61/85
95501/95501 - 14s - loss: 7.9395e-04 - val_loss: 7.1889e-04 - 14s/epoch - 147us/sample
Epoch 62/85
95501/95501 - 14s - loss: 7.6574e-04 - val_loss: 8.3530e-04 - 14s/epoch - 147us/sample
Epoch 63/85
95501/95501 - 14s - loss: 8.6830e-04 - val_loss: 7.5649e-04 - 14s/epoch - 147us/sample
Epoch 64/85
95501/95501 - 14s - loss: 8.1035e-04 - val_loss: 7.3426e-04 - 14s/epoch - 147us/sample
Epoch 65/85
95501/95501 - 14s - loss: 7.7237e-04 - val_loss: 7.1016e-04 - 14s/epoch - 147us/sample
Epoch 66/85
95501/95501 - 14s - loss: 7.6076e-04 - val_loss: 7.8173e-04 - 14s/epoch - 147us/sample
Epoch 67/85
95501/95501 - 14s - loss: 8.0915e-04 - val_loss: 7.1547e-04 - 14s/epoch - 147us/sample
Epoch 68/85
95501/95501 - 14s - loss: 7.5261e-04 - val_loss: 7.6861e-04 - 14s/epoch - 147us/sample
Epoch 69/85
95501/95501 - 14s - loss: 7.9589e-04 - val_loss: 7.1173e-04 - 14s/epoch - 147us/sample
Epoch 70/85
95501/95501 - 14s - loss: 7.5166e-04 - val_loss: 7.2889e-04 - 14s/epoch - 147us/sample
Epoch 71/85
95501/95501 - 14s - loss: 7.4995e-04 - val_loss: 8.2934e-04 - 14s/epoch - 148us/sample
Epoch 72/85
95501/95501 - 14s - loss: 8.2850e-04 - val_loss: 7.5448e-04 - 14s/epoch - 147us/sample
Epoch 73/85
95501/95501 - 14s - loss: 7.8338e-04 - val_loss: 7.1425e-04 - 14s/epoch - 147us/sample
Epoch 74/85
95501/95501 - 14s - loss: 7.5533e-04 - val_loss: 7.0269e-04 - 14s/epoch - 147us/sample
Epoch 75/85
95501/95501 - 14s - loss: 7.4993e-04 - val_loss: 7.0891e-04 - 14s/epoch - 147us/sample
Epoch 76/85
95501/95501 - 14s - loss: 7.4000e-04 - val_loss: 7.2057e-04 - 14s/epoch - 147us/sample
Epoch 77/85
95501/95501 - 14s - loss: 7.7324e-04 - val_loss: 7.0109e-04 - 14s/epoch - 148us/sample
Epoch 78/85
95501/95501 - 14s - loss: 7.3506e-04 - val_loss: 6.9087e-04 - 14s/epoch - 147us/sample
Epoch 79/85
95501/95501 - 14s - loss: 7.3336e-04 - val_loss: 6.9036e-04 - 14s/epoch - 147us/sample
Epoch 80/85
95501/95501 - 14s - loss: 7.3653e-04 - val_loss: 8.5251e-04 - 14s/epoch - 144us/sample
Epoch 81/85
95501/95501 - 14s - loss: 8.4828e-04 - val_loss: 8.1131e-04 - 14s/epoch - 142us/sample
Epoch 82/85
95501/95501 - 13s - loss: 7.8699e-04 - val_loss: 8.0997e-04 - 13s/epoch - 141us/sample
Epoch 83/85
95501/95501 - 13s - loss: 8.0898e-04 - val_loss: 7.1106e-04 - 13s/epoch - 141us/sample
Epoch 84/85
95501/95501 - 13s - loss: 7.4240e-04 - val_loss: 7.5898e-04 - 13s/epoch - 140us/sample
Epoch 85/85
95501/95501 - 13s - loss: 7.7785e-04 - val_loss: 6.9609e-04 - 13s/epoch - 140us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0006960925152995706
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 09:40:44.350629: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_29/outputlayer/BiasAdd' id:37863 op device:{requested: '', assigned: ''} def:{{{node decoder_model_29/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_29/outputlayer/MatMul, decoder_model_29/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.004766095046317672
cosine 0.003761328136161548
MAE: 0.010742623
RMSE: 0.020273183
r2: 0.9733376843229277
RMSE zero-vector: 0.23411466903540806
['1.7000000000000002custom_VAE', 'mse', 64, 85, 0.0012000000000000001, 0.6, 758, 0.0007778482409675468, 0.0006960925152995706, 0.004766095046317672, 0.003761328136161548, 0.01074262335896492, 0.020273182541131973, 0.9733376843229277, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.7000000000000002 145 0.0014 64 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2148)         2717220     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_90 (BatchN  (None, 2148)        8592        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_90 (ReLU)                (None, 2148)         0           ['batch_normalization_90[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1628942     ['re_lu_90[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1628942     ['re_lu_90[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4933614     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 10,917,310
Trainable params: 10,907,202
Non-trainable params: 10,108
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-15 09:41:01.504770: W tensorflow/c/c_api.cc:291] Operation '{name:'training_60/Adam/batch_normalization_92/beta/m/Assign' id:39701 op device:{requested: '', assigned: ''} def:{{{node training_60/Adam/batch_normalization_92/beta/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_60/Adam/batch_normalization_92/beta/m, training_60/Adam/batch_normalization_92/beta/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 09:41:18.434900: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_30/mul' id:39153 op device:{requested: '', assigned: ''} def:{{{node loss_30/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_30/mul/x, loss_30/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 23s - loss: 0.0111 - val_loss: 0.0085 - 23s/epoch - 244us/sample
Epoch 2/145
95501/95501 - 14s - loss: 0.0764 - val_loss: 0.0056 - 14s/epoch - 143us/sample
Epoch 3/145
95501/95501 - 14s - loss: 0.0040 - val_loss: 0.0063 - 14s/epoch - 143us/sample
Epoch 4/145
95501/95501 - 14s - loss: 0.0040 - val_loss: 0.0043 - 14s/epoch - 142us/sample
Epoch 5/145
95501/95501 - 14s - loss: 0.0038 - val_loss: 0.0025 - 14s/epoch - 143us/sample
Epoch 6/145
95501/95501 - 14s - loss: 0.0335 - val_loss: 0.0023 - 14s/epoch - 143us/sample
Epoch 7/145
95501/95501 - 14s - loss: 0.0023 - val_loss: 0.0058 - 14s/epoch - 143us/sample
Epoch 8/145
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0019 - 14s/epoch - 143us/sample
Epoch 9/145
95501/95501 - 14s - loss: 0.0120 - val_loss: 0.0017 - 14s/epoch - 143us/sample
Epoch 10/145
95501/95501 - 14s - loss: 0.0017 - val_loss: 0.0227 - 14s/epoch - 143us/sample
Epoch 11/145
95501/95501 - 14s - loss: 0.0024 - val_loss: 0.0015 - 14s/epoch - 142us/sample
Epoch 12/145
95501/95501 - 14s - loss: 0.0035 - val_loss: 1.0005 - 14s/epoch - 142us/sample
Epoch 13/145
95501/95501 - 14s - loss: 0.0017 - val_loss: 0.0016 - 14s/epoch - 144us/sample
Epoch 14/145
95501/95501 - 14s - loss: 0.0015 - val_loss: 0.0020 - 14s/epoch - 143us/sample
Epoch 15/145
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0013 - 14s/epoch - 143us/sample
Epoch 16/145
95501/95501 - 14s - loss: 0.0014 - val_loss: 0.0014 - 14s/epoch - 142us/sample
Epoch 17/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0013 - 14s/epoch - 142us/sample
Epoch 18/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0018 - 14s/epoch - 143us/sample
Epoch 19/145
95501/95501 - 14s - loss: 0.0016 - val_loss: 0.0012 - 14s/epoch - 144us/sample
Epoch 20/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 21/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0016 - 14s/epoch - 143us/sample
Epoch 22/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0010 - 14s/epoch - 142us/sample
Epoch 23/145
95501/95501 - 14s - loss: 0.0011 - val_loss: 0.0026 - 14s/epoch - 142us/sample
Epoch 24/145
95501/95501 - 14s - loss: 0.0015 - val_loss: 9.8751e-04 - 14s/epoch - 143us/sample
Epoch 25/145
95501/95501 - 14s - loss: 0.0011 - val_loss: 0.0012 - 14s/epoch - 144us/sample
Epoch 26/145
95501/95501 - 14s - loss: 0.0011 - val_loss: 9.6994e-04 - 14s/epoch - 143us/sample
Epoch 27/145
95501/95501 - 14s - loss: 0.0010 - val_loss: 9.6902e-04 - 14s/epoch - 143us/sample
Epoch 28/145
95501/95501 - 14s - loss: 0.0010 - val_loss: 9.3167e-04 - 14s/epoch - 142us/sample
Epoch 29/145
95501/95501 - 14s - loss: 9.8280e-04 - val_loss: 8.9633e-04 - 14s/epoch - 142us/sample
Epoch 30/145
95501/95501 - 14s - loss: 9.4954e-04 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 31/145
95501/95501 - 14s - loss: 0.0010 - val_loss: 8.6831e-04 - 14s/epoch - 144us/sample
Epoch 32/145
95501/95501 - 14s - loss: 9.1994e-04 - val_loss: 8.6616e-04 - 14s/epoch - 143us/sample
Epoch 33/145
95501/95501 - 14s - loss: 9.1226e-04 - val_loss: 9.0149e-04 - 14s/epoch - 142us/sample
Epoch 34/145
95501/95501 - 14s - loss: 9.4078e-04 - val_loss: 8.2915e-04 - 14s/epoch - 142us/sample
Epoch 35/145
95501/95501 - 14s - loss: 9.0047e-04 - val_loss: 0.0024 - 14s/epoch - 142us/sample
Epoch 36/145
95501/95501 - 14s - loss: 0.0011 - val_loss: 8.3881e-04 - 14s/epoch - 143us/sample
Epoch 37/145
95501/95501 - 14s - loss: 8.9165e-04 - val_loss: 9.8261e-04 - 14s/epoch - 144us/sample
Epoch 38/145
95501/95501 - 14s - loss: 9.8928e-04 - val_loss: 8.2870e-04 - 14s/epoch - 143us/sample
Epoch 39/145
95501/95501 - 14s - loss: 8.8116e-04 - val_loss: 8.2049e-04 - 14s/epoch - 142us/sample
Epoch 40/145
95501/95501 - 14s - loss: 8.7381e-04 - val_loss: 0.0011 - 14s/epoch - 142us/sample
Epoch 41/145
95501/95501 - 14s - loss: 0.0011 - val_loss: 0.0015 - 14s/epoch - 142us/sample
Epoch 42/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 8.5480e-04 - 14s/epoch - 144us/sample
Epoch 43/145
95501/95501 - 14s - loss: 8.8979e-04 - val_loss: 8.2545e-04 - 14s/epoch - 143us/sample
Epoch 44/145
95501/95501 - 14s - loss: 8.7182e-04 - val_loss: 8.0310e-04 - 14s/epoch - 142us/sample
Epoch 45/145
95501/95501 - 14s - loss: 8.7219e-04 - val_loss: 8.2709e-04 - 14s/epoch - 143us/sample
Epoch 46/145
95501/95501 - 14s - loss: 8.5523e-04 - val_loss: 7.8521e-04 - 14s/epoch - 142us/sample
Epoch 47/145
95501/95501 - 14s - loss: 8.4276e-04 - val_loss: 7.9433e-04 - 14s/epoch - 143us/sample
Epoch 48/145
95501/95501 - 14s - loss: 8.2978e-04 - val_loss: 7.7916e-04 - 14s/epoch - 143us/sample
Epoch 49/145
95501/95501 - 14s - loss: 8.2458e-04 - val_loss: 7.6946e-04 - 14s/epoch - 143us/sample
Epoch 50/145
95501/95501 - 14s - loss: 8.3097e-04 - val_loss: 7.7125e-04 - 14s/epoch - 143us/sample
Epoch 51/145
95501/95501 - 14s - loss: 8.2013e-04 - val_loss: 7.7744e-04 - 14s/epoch - 143us/sample
Epoch 52/145
95501/95501 - 14s - loss: 8.1866e-04 - val_loss: 8.6373e-04 - 14s/epoch - 142us/sample
Epoch 53/145
95501/95501 - 14s - loss: 8.5127e-04 - val_loss: 7.7815e-04 - 14s/epoch - 143us/sample
Epoch 54/145
95501/95501 - 14s - loss: 8.1206e-04 - val_loss: 7.5861e-04 - 14s/epoch - 144us/sample
Epoch 55/145
95501/95501 - 14s - loss: 7.9574e-04 - val_loss: 7.9613e-04 - 14s/epoch - 142us/sample
Epoch 56/145
95501/95501 - 14s - loss: 8.2930e-04 - val_loss: 9.1295e-04 - 14s/epoch - 142us/sample
Epoch 57/145
95501/95501 - 14s - loss: 9.2769e-04 - val_loss: 7.7046e-04 - 14s/epoch - 142us/sample
Epoch 58/145
95501/95501 - 14s - loss: 8.3914e-04 - val_loss: 9.2676e-04 - 14s/epoch - 143us/sample
Epoch 59/145
95501/95501 - 14s - loss: 9.6245e-04 - val_loss: 7.6026e-04 - 14s/epoch - 143us/sample
Epoch 60/145
95501/95501 - 14s - loss: 8.0456e-04 - val_loss: 7.9650e-04 - 14s/epoch - 143us/sample
Epoch 61/145
95501/95501 - 14s - loss: 8.4537e-04 - val_loss: 7.8514e-04 - 14s/epoch - 142us/sample
Epoch 62/145
95501/95501 - 14s - loss: 8.3478e-04 - val_loss: 7.7718e-04 - 14s/epoch - 142us/sample
Epoch 63/145
95501/95501 - 14s - loss: 8.0928e-04 - val_loss: 7.3750e-04 - 14s/epoch - 143us/sample
Epoch 64/145
95501/95501 - 14s - loss: 7.8488e-04 - val_loss: 7.3892e-04 - 14s/epoch - 143us/sample
Epoch 65/145
95501/95501 - 14s - loss: 7.7783e-04 - val_loss: 7.3328e-04 - 14s/epoch - 144us/sample
Epoch 66/145
95501/95501 - 14s - loss: 7.7629e-04 - val_loss: 7.4001e-04 - 14s/epoch - 143us/sample
Epoch 67/145
95501/95501 - 14s - loss: 8.4215e-04 - val_loss: 7.9965e-04 - 14s/epoch - 143us/sample
Epoch 68/145
95501/95501 - 14s - loss: 8.3429e-04 - val_loss: 7.3254e-04 - 14s/epoch - 142us/sample
Epoch 69/145
95501/95501 - 14s - loss: 7.8010e-04 - val_loss: 7.3377e-04 - 14s/epoch - 143us/sample
Epoch 70/145
95501/95501 - 14s - loss: 7.8254e-04 - val_loss: 7.6310e-04 - 14s/epoch - 143us/sample
Epoch 71/145
95501/95501 - 14s - loss: 7.8499e-04 - val_loss: 7.3771e-04 - 14s/epoch - 144us/sample
Epoch 72/145
95501/95501 - 14s - loss: 7.7247e-04 - val_loss: 7.3582e-04 - 14s/epoch - 143us/sample
Epoch 73/145
95501/95501 - 14s - loss: 7.5918e-04 - val_loss: 7.1898e-04 - 14s/epoch - 142us/sample
Epoch 74/145
95501/95501 - 14s - loss: 7.5716e-04 - val_loss: 7.5911e-04 - 14s/epoch - 143us/sample
Epoch 75/145
95501/95501 - 14s - loss: 7.7779e-04 - val_loss: 7.1919e-04 - 14s/epoch - 142us/sample
Epoch 76/145
95501/95501 - 14s - loss: 7.6287e-04 - val_loss: 8.6110e-04 - 14s/epoch - 142us/sample
Epoch 77/145
95501/95501 - 14s - loss: 8.9175e-04 - val_loss: 7.2577e-04 - 14s/epoch - 144us/sample
Epoch 78/145
95501/95501 - 14s - loss: 7.6117e-04 - val_loss: 7.0608e-04 - 14s/epoch - 143us/sample
Epoch 79/145
95501/95501 - 14s - loss: 7.5426e-04 - val_loss: 7.3044e-04 - 14s/epoch - 143us/sample
Epoch 80/145
95501/95501 - 14s - loss: 7.5245e-04 - val_loss: 7.2939e-04 - 14s/epoch - 142us/sample
Epoch 81/145
95501/95501 - 14s - loss: 7.6297e-04 - val_loss: 7.0935e-04 - 14s/epoch - 142us/sample
Epoch 82/145
95501/95501 - 14s - loss: 7.4324e-04 - val_loss: 7.3058e-04 - 14s/epoch - 143us/sample
Epoch 83/145
95501/95501 - 14s - loss: 7.7676e-04 - val_loss: 6.9940e-04 - 14s/epoch - 144us/sample
Epoch 84/145
95501/95501 - 14s - loss: 7.4240e-04 - val_loss: 7.0581e-04 - 14s/epoch - 142us/sample
Epoch 85/145
95501/95501 - 14s - loss: 7.4261e-04 - val_loss: 8.5345e-04 - 14s/epoch - 143us/sample
Epoch 86/145
95501/95501 - 14s - loss: 8.5909e-04 - val_loss: 7.1027e-04 - 14s/epoch - 142us/sample
Epoch 87/145
95501/95501 - 14s - loss: 7.6513e-04 - val_loss: 0.0010 - 14s/epoch - 143us/sample
Epoch 88/145
95501/95501 - 14s - loss: 9.4276e-04 - val_loss: 7.2241e-04 - 14s/epoch - 143us/sample
Epoch 89/145
95501/95501 - 14s - loss: 7.6386e-04 - val_loss: 7.3996e-04 - 14s/epoch - 143us/sample
Epoch 90/145
95501/95501 - 14s - loss: 7.8208e-04 - val_loss: 8.4220e-04 - 14s/epoch - 143us/sample
Epoch 91/145
95501/95501 - 14s - loss: 8.5327e-04 - val_loss: 7.0911e-04 - 14s/epoch - 142us/sample
Epoch 92/145
95501/95501 - 14s - loss: 7.5215e-04 - val_loss: 7.0338e-04 - 14s/epoch - 143us/sample
Epoch 93/145
95501/95501 - 14s - loss: 7.5090e-04 - val_loss: 0.0011 - 14s/epoch - 143us/sample
Epoch 94/145
95501/95501 - 14s - loss: 8.7251e-04 - val_loss: 7.0715e-04 - 14s/epoch - 144us/sample
Epoch 95/145
95501/95501 - 14s - loss: 7.5320e-04 - val_loss: 7.0089e-04 - 14s/epoch - 143us/sample
Epoch 96/145
95501/95501 - 14s - loss: 7.4622e-04 - val_loss: 8.1866e-04 - 14s/epoch - 142us/sample
Epoch 97/145
95501/95501 - 14s - loss: 8.1235e-04 - val_loss: 7.3927e-04 - 14s/epoch - 142us/sample
Epoch 98/145
95501/95501 - 14s - loss: 7.3949e-04 - val_loss: 6.9963e-04 - 14s/epoch - 143us/sample
Epoch 99/145
95501/95501 - 14s - loss: 7.3695e-04 - val_loss: 7.0911e-04 - 14s/epoch - 143us/sample
Epoch 100/145
95501/95501 - 14s - loss: 7.4272e-04 - val_loss: 6.9830e-04 - 14s/epoch - 144us/sample
Epoch 101/145
95501/95501 - 14s - loss: 7.3629e-04 - val_loss: 6.9926e-04 - 14s/epoch - 143us/sample
Epoch 102/145
95501/95501 - 14s - loss: 7.2474e-04 - val_loss: 6.8410e-04 - 14s/epoch - 142us/sample
Epoch 103/145
95501/95501 - 14s - loss: 7.2421e-04 - val_loss: 7.4152e-04 - 14s/epoch - 142us/sample
Epoch 104/145
95501/95501 - 14s - loss: 7.7689e-04 - val_loss: 6.8823e-04 - 14s/epoch - 142us/sample
Epoch 105/145
95501/95501 - 14s - loss: 7.3210e-04 - val_loss: 9.4848e-04 - 14s/epoch - 143us/sample
Epoch 106/145
95501/95501 - 14s - loss: 7.6222e-04 - val_loss: 6.8893e-04 - 14s/epoch - 144us/sample
Epoch 107/145
95501/95501 - 14s - loss: 7.2614e-04 - val_loss: 7.0634e-04 - 14s/epoch - 143us/sample
Epoch 108/145
95501/95501 - 14s - loss: 7.3444e-04 - val_loss: 6.8521e-04 - 14s/epoch - 143us/sample
Epoch 109/145
95501/95501 - 14s - loss: 7.2375e-04 - val_loss: 6.8204e-04 - 14s/epoch - 143us/sample
Epoch 110/145
95501/95501 - 14s - loss: 7.1444e-04 - val_loss: 6.7888e-04 - 14s/epoch - 142us/sample
Epoch 111/145
95501/95501 - 14s - loss: 7.1439e-04 - val_loss: 6.7709e-04 - 14s/epoch - 143us/sample
Epoch 112/145
95501/95501 - 14s - loss: 7.1143e-04 - val_loss: 6.7585e-04 - 14s/epoch - 144us/sample
Epoch 113/145
95501/95501 - 14s - loss: 7.1721e-04 - val_loss: 6.7525e-04 - 14s/epoch - 142us/sample
Epoch 114/145
95501/95501 - 14s - loss: 7.0978e-04 - val_loss: 7.7194e-04 - 14s/epoch - 143us/sample
Epoch 115/145
95501/95501 - 14s - loss: 7.7450e-04 - val_loss: 7.4322e-04 - 14s/epoch - 143us/sample
Epoch 116/145
95501/95501 - 14s - loss: 7.5195e-04 - val_loss: 7.9716e-04 - 14s/epoch - 143us/sample
Epoch 117/145
95501/95501 - 14s - loss: 8.1227e-04 - val_loss: 6.8339e-04 - 14s/epoch - 144us/sample
Epoch 118/145
95501/95501 - 14s - loss: 7.2309e-04 - val_loss: 6.8317e-04 - 14s/epoch - 143us/sample
Epoch 119/145
95501/95501 - 14s - loss: 7.1762e-04 - val_loss: 6.9000e-04 - 14s/epoch - 142us/sample
Epoch 120/145
95501/95501 - 14s - loss: 7.1313e-04 - val_loss: 6.7163e-04 - 14s/epoch - 142us/sample
Epoch 121/145
95501/95501 - 14s - loss: 7.1843e-04 - val_loss: 7.0134e-04 - 14s/epoch - 142us/sample
Epoch 122/145
95501/95501 - 14s - loss: 7.4362e-04 - val_loss: 6.7978e-04 - 14s/epoch - 143us/sample
Epoch 123/145
95501/95501 - 14s - loss: 7.0667e-04 - val_loss: 7.0556e-04 - 14s/epoch - 144us/sample
Epoch 124/145
95501/95501 - 14s - loss: 7.4640e-04 - val_loss: 8.8866e-04 - 14s/epoch - 143us/sample
Epoch 125/145
95501/95501 - 14s - loss: 8.5560e-04 - val_loss: 6.9542e-04 - 14s/epoch - 142us/sample
Epoch 126/145
95501/95501 - 14s - loss: 7.2779e-04 - val_loss: 6.7397e-04 - 14s/epoch - 143us/sample
Epoch 127/145
95501/95501 - 14s - loss: 7.1834e-04 - val_loss: 6.7638e-04 - 14s/epoch - 143us/sample
Epoch 128/145
95501/95501 - 14s - loss: 7.0887e-04 - val_loss: 6.7272e-04 - 14s/epoch - 143us/sample
Epoch 129/145
95501/95501 - 14s - loss: 7.1052e-04 - val_loss: 6.7078e-04 - 14s/epoch - 143us/sample
Epoch 130/145
95501/95501 - 14s - loss: 7.0411e-04 - val_loss: 7.4569e-04 - 14s/epoch - 143us/sample
Epoch 131/145
95501/95501 - 14s - loss: 7.5242e-04 - val_loss: 7.7767e-04 - 14s/epoch - 143us/sample
Epoch 132/145
95501/95501 - 14s - loss: 7.9828e-04 - val_loss: 6.9192e-04 - 14s/epoch - 143us/sample
Epoch 133/145
95501/95501 - 14s - loss: 7.5657e-04 - val_loss: 6.8704e-04 - 14s/epoch - 143us/sample
Epoch 134/145
95501/95501 - 14s - loss: 7.1055e-04 - val_loss: 6.7316e-04 - 14s/epoch - 143us/sample
Epoch 135/145
95501/95501 - 14s - loss: 7.1473e-04 - val_loss: 6.6342e-04 - 14s/epoch - 144us/sample
Epoch 136/145
95501/95501 - 14s - loss: 7.0306e-04 - val_loss: 6.6275e-04 - 14s/epoch - 143us/sample
Epoch 137/145
95501/95501 - 14s - loss: 6.9871e-04 - val_loss: 7.0643e-04 - 14s/epoch - 143us/sample
Epoch 138/145
95501/95501 - 14s - loss: 7.4304e-04 - val_loss: 7.2018e-04 - 14s/epoch - 143us/sample
Epoch 139/145
95501/95501 - 14s - loss: 7.4043e-04 - val_loss: 6.6563e-04 - 14s/epoch - 142us/sample
Epoch 140/145
95501/95501 - 14s - loss: 7.0703e-04 - val_loss: 7.8338e-04 - 14s/epoch - 143us/sample
Epoch 141/145
95501/95501 - 14s - loss: 7.8644e-04 - val_loss: 6.6868e-04 - 14s/epoch - 144us/sample
Epoch 142/145
95501/95501 - 14s - loss: 7.0321e-04 - val_loss: 6.6392e-04 - 14s/epoch - 142us/sample
Epoch 143/145
95501/95501 - 14s - loss: 7.3157e-04 - val_loss: 6.6755e-04 - 14s/epoch - 143us/sample
Epoch 144/145
95501/95501 - 14s - loss: 6.9656e-04 - val_loss: 7.0943e-04 - 14s/epoch - 143us/sample
Epoch 145/145
95501/95501 - 14s - loss: 7.3608e-04 - val_loss: 6.9435e-04 - 14s/epoch - 142us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0006943506219538748
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 10:14:05.774621: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_30/outputlayer/BiasAdd' id:39124 op device:{requested: '', assigned: ''} def:{{{node decoder_model_30/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_30/outputlayer/MatMul, decoder_model_30/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.004730042397324382
cosine 0.0037337852297213855
MAE: 0.010703228
RMSE: 0.020234963
r2: 0.9734383439768459
RMSE zero-vector: 0.23411466903540806
['1.7000000000000002custom_VAE', 'mse', 64, 145, 0.0014, 0.6, 758, 0.0007360831605346044, 0.0006943506219538748, 0.004730042397324382, 0.0037337852297213855, 0.010703228414058685, 0.020234962925314903, 0.9734383439768459, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 85 0.001 64 2] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_93 (BatchN  (None, 1896)        7584        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_93 (ReLU)                (None, 1896)         0           ['batch_normalization_93[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu_93[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu_93[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/85
2023-02-15 10:14:23.330010: W tensorflow/c/c_api.cc:291] Operation '{name:'training_62/Adam/outputlayer_31/kernel/m/Assign' id:40999 op device:{requested: '', assigned: ''} def:{{{node training_62/Adam/outputlayer_31/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_62/Adam/outputlayer_31/kernel/m, training_62/Adam/outputlayer_31/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 10:14:40.406696: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_31/mul' id:40421 op device:{requested: '', assigned: ''} def:{{{node loss_31/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_31/mul/x, loss_31/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 24s - loss: 0.0094 - val_loss: 0.0043 - 24s/epoch - 247us/sample
Epoch 2/85
95501/95501 - 14s - loss: 0.0028 - val_loss: 0.0108 - 14s/epoch - 145us/sample
Epoch 3/85
95501/95501 - 14s - loss: 0.0023 - val_loss: 0.0021 - 14s/epoch - 145us/sample
Epoch 4/85
95501/95501 - 14s - loss: 0.0025 - val_loss: 0.0017 - 14s/epoch - 145us/sample
Epoch 5/85
95501/95501 - 14s - loss: 0.0017 - val_loss: 0.0014 - 14s/epoch - 146us/sample
Epoch 6/85
95501/95501 - 14s - loss: 0.0015 - val_loss: 0.0014 - 14s/epoch - 145us/sample
Epoch 7/85
95501/95501 - 14s - loss: 0.0014 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 8/85
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0012 - 14s/epoch - 145us/sample
Epoch 9/85
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0011 - 14s/epoch - 145us/sample
Epoch 10/85
95501/95501 - 14s - loss: 0.0011 - val_loss: 9.0034e-04 - 14s/epoch - 145us/sample
Epoch 11/85
95501/95501 - 14s - loss: 9.5474e-04 - val_loss: 8.3808e-04 - 14s/epoch - 146us/sample
Epoch 12/85
95501/95501 - 14s - loss: 9.0190e-04 - val_loss: 8.2121e-04 - 14s/epoch - 145us/sample
Epoch 13/85
95501/95501 - 14s - loss: 8.3636e-04 - val_loss: 7.7402e-04 - 14s/epoch - 145us/sample
Epoch 14/85
95501/95501 - 14s - loss: 8.1486e-04 - val_loss: 7.1751e-04 - 14s/epoch - 145us/sample
Epoch 15/85
95501/95501 - 14s - loss: 7.5912e-04 - val_loss: 7.0575e-04 - 14s/epoch - 145us/sample
Epoch 16/85
95501/95501 - 14s - loss: 7.2784e-04 - val_loss: 6.6153e-04 - 14s/epoch - 145us/sample
Epoch 17/85
95501/95501 - 14s - loss: 7.0078e-04 - val_loss: 6.5237e-04 - 14s/epoch - 146us/sample
Epoch 18/85
95501/95501 - 14s - loss: 6.7830e-04 - val_loss: 6.3641e-04 - 14s/epoch - 145us/sample
Epoch 19/85
95501/95501 - 14s - loss: 6.7069e-04 - val_loss: 6.5286e-04 - 14s/epoch - 145us/sample
Epoch 20/85
95501/95501 - 14s - loss: 6.6997e-04 - val_loss: 6.0675e-04 - 14s/epoch - 145us/sample
Epoch 21/85
95501/95501 - 14s - loss: 6.3552e-04 - val_loss: 5.9609e-04 - 14s/epoch - 145us/sample
Epoch 22/85
95501/95501 - 14s - loss: 6.3492e-04 - val_loss: 7.4174e-04 - 14s/epoch - 145us/sample
Epoch 23/85
95501/95501 - 14s - loss: 6.8924e-04 - val_loss: 5.8512e-04 - 14s/epoch - 146us/sample
Epoch 24/85
95501/95501 - 14s - loss: 6.1278e-04 - val_loss: 5.7697e-04 - 14s/epoch - 145us/sample
Epoch 25/85
95501/95501 - 14s - loss: 6.0578e-04 - val_loss: 5.6545e-04 - 14s/epoch - 145us/sample
Epoch 26/85
95501/95501 - 14s - loss: 5.9690e-04 - val_loss: 5.6483e-04 - 14s/epoch - 145us/sample
Epoch 27/85
95501/95501 - 14s - loss: 5.8961e-04 - val_loss: 5.6202e-04 - 14s/epoch - 145us/sample
Epoch 28/85
95501/95501 - 14s - loss: 5.8943e-04 - val_loss: 5.5654e-04 - 14s/epoch - 146us/sample
Epoch 29/85
95501/95501 - 14s - loss: 5.8159e-04 - val_loss: 5.7642e-04 - 14s/epoch - 146us/sample
Epoch 30/85
95501/95501 - 14s - loss: 5.9762e-04 - val_loss: 5.4096e-04 - 14s/epoch - 145us/sample
Epoch 31/85
95501/95501 - 14s - loss: 5.6871e-04 - val_loss: 6.2643e-04 - 14s/epoch - 145us/sample
Epoch 32/85
95501/95501 - 14s - loss: 6.2454e-04 - val_loss: 5.6095e-04 - 14s/epoch - 145us/sample
Epoch 33/85
95501/95501 - 14s - loss: 5.7399e-04 - val_loss: 5.3173e-04 - 14s/epoch - 145us/sample
Epoch 34/85
95501/95501 - 14s - loss: 5.5861e-04 - val_loss: 5.5028e-04 - 14s/epoch - 146us/sample
Epoch 35/85
95501/95501 - 14s - loss: 5.7125e-04 - val_loss: 5.2739e-04 - 14s/epoch - 145us/sample
Epoch 36/85
95501/95501 - 14s - loss: 5.5015e-04 - val_loss: 5.2131e-04 - 14s/epoch - 145us/sample
Epoch 37/85
95501/95501 - 14s - loss: 5.4677e-04 - val_loss: 5.2713e-04 - 14s/epoch - 145us/sample
Epoch 38/85
95501/95501 - 14s - loss: 5.4841e-04 - val_loss: 5.2271e-04 - 14s/epoch - 145us/sample
Epoch 39/85
95501/95501 - 14s - loss: 5.4225e-04 - val_loss: 5.0844e-04 - 14s/epoch - 145us/sample
Epoch 40/85
95501/95501 - 14s - loss: 5.3990e-04 - val_loss: 5.1730e-04 - 14s/epoch - 146us/sample
Epoch 41/85
95501/95501 - 14s - loss: 5.4630e-04 - val_loss: 5.0533e-04 - 14s/epoch - 145us/sample
Epoch 42/85
95501/95501 - 14s - loss: 5.3286e-04 - val_loss: 5.0397e-04 - 14s/epoch - 145us/sample
Epoch 43/85
95501/95501 - 14s - loss: 5.2801e-04 - val_loss: 5.0432e-04 - 14s/epoch - 145us/sample
Epoch 44/85
95501/95501 - 14s - loss: 5.2611e-04 - val_loss: 4.9705e-04 - 14s/epoch - 145us/sample
Epoch 45/85
95501/95501 - 14s - loss: 5.2372e-04 - val_loss: 4.9869e-04 - 14s/epoch - 145us/sample
Epoch 46/85
95501/95501 - 14s - loss: 5.2394e-04 - val_loss: 4.9224e-04 - 14s/epoch - 146us/sample
Epoch 47/85
95501/95501 - 14s - loss: 5.2100e-04 - val_loss: 4.9398e-04 - 14s/epoch - 145us/sample
Epoch 48/85
95501/95501 - 14s - loss: 5.1574e-04 - val_loss: 4.9946e-04 - 14s/epoch - 145us/sample
Epoch 49/85
95501/95501 - 14s - loss: 5.1822e-04 - val_loss: 4.9299e-04 - 14s/epoch - 145us/sample
Epoch 50/85
95501/95501 - 14s - loss: 5.2697e-04 - val_loss: 4.9051e-04 - 14s/epoch - 145us/sample
Epoch 51/85
95501/95501 - 14s - loss: 5.1335e-04 - val_loss: 5.7634e-04 - 14s/epoch - 145us/sample
Epoch 52/85
95501/95501 - 14s - loss: 5.8844e-04 - val_loss: 4.9436e-04 - 14s/epoch - 146us/sample
Epoch 53/85
95501/95501 - 14s - loss: 5.1715e-04 - val_loss: 5.3478e-04 - 14s/epoch - 145us/sample
Epoch 54/85
95501/95501 - 14s - loss: 5.4741e-04 - val_loss: 4.9508e-04 - 14s/epoch - 145us/sample
Epoch 55/85
95501/95501 - 14s - loss: 5.1869e-04 - val_loss: 4.8468e-04 - 14s/epoch - 145us/sample
Epoch 56/85
95501/95501 - 14s - loss: 5.1023e-04 - val_loss: 4.9233e-04 - 14s/epoch - 144us/sample
Epoch 57/85
95501/95501 - 14s - loss: 5.0910e-04 - val_loss: 4.8431e-04 - 14s/epoch - 145us/sample
Epoch 58/85
95501/95501 - 14s - loss: 5.0729e-04 - val_loss: 5.0884e-04 - 14s/epoch - 145us/sample
Epoch 59/85
95501/95501 - 14s - loss: 5.2628e-04 - val_loss: 4.8546e-04 - 14s/epoch - 145us/sample
Epoch 60/85
95501/95501 - 14s - loss: 5.1906e-04 - val_loss: 4.7898e-04 - 14s/epoch - 145us/sample
Epoch 61/85
95501/95501 - 14s - loss: 5.0250e-04 - val_loss: 4.8458e-04 - 14s/epoch - 144us/sample
Epoch 62/85
95501/95501 - 14s - loss: 5.0005e-04 - val_loss: 4.7544e-04 - 14s/epoch - 145us/sample
Epoch 63/85
95501/95501 - 14s - loss: 5.0138e-04 - val_loss: 5.7992e-04 - 14s/epoch - 146us/sample
Epoch 64/85
95501/95501 - 14s - loss: 5.8734e-04 - val_loss: 5.0578e-04 - 14s/epoch - 145us/sample
Epoch 65/85
95501/95501 - 14s - loss: 5.2622e-04 - val_loss: 4.8322e-04 - 14s/epoch - 145us/sample
Epoch 66/85
95501/95501 - 14s - loss: 5.0020e-04 - val_loss: 4.9868e-04 - 14s/epoch - 145us/sample
Epoch 67/85
95501/95501 - 14s - loss: 5.1965e-04 - val_loss: 4.7605e-04 - 14s/epoch - 144us/sample
Epoch 68/85
95501/95501 - 14s - loss: 4.9759e-04 - val_loss: 4.7680e-04 - 14s/epoch - 145us/sample
Epoch 69/85
95501/95501 - 14s - loss: 4.9498e-04 - val_loss: 4.7336e-04 - 14s/epoch - 146us/sample
Epoch 70/85
95501/95501 - 14s - loss: 4.9485e-04 - val_loss: 4.6937e-04 - 14s/epoch - 145us/sample
Epoch 71/85
95501/95501 - 14s - loss: 4.9244e-04 - val_loss: 4.8501e-04 - 14s/epoch - 145us/sample
Epoch 72/85
95501/95501 - 14s - loss: 5.0680e-04 - val_loss: 4.8721e-04 - 14s/epoch - 145us/sample
Epoch 73/85
95501/95501 - 14s - loss: 5.0742e-04 - val_loss: 4.7505e-04 - 14s/epoch - 144us/sample
Epoch 74/85
95501/95501 - 14s - loss: 4.9419e-04 - val_loss: 4.6787e-04 - 14s/epoch - 145us/sample
Epoch 75/85
95501/95501 - 14s - loss: 4.9038e-04 - val_loss: 4.7896e-04 - 14s/epoch - 146us/sample
Epoch 76/85
95501/95501 - 14s - loss: 4.9405e-04 - val_loss: 4.6259e-04 - 14s/epoch - 145us/sample
Epoch 77/85
95501/95501 - 14s - loss: 4.8599e-04 - val_loss: 4.7440e-04 - 14s/epoch - 144us/sample
Epoch 78/85
95501/95501 - 14s - loss: 4.8891e-04 - val_loss: 4.6398e-04 - 14s/epoch - 145us/sample
Epoch 79/85
95501/95501 - 14s - loss: 4.8789e-04 - val_loss: 4.6488e-04 - 14s/epoch - 145us/sample
Epoch 80/85
95501/95501 - 14s - loss: 4.8314e-04 - val_loss: 4.6269e-04 - 14s/epoch - 145us/sample
Epoch 81/85
95501/95501 - 14s - loss: 4.8571e-04 - val_loss: 4.6483e-04 - 14s/epoch - 146us/sample
Epoch 82/85
95501/95501 - 14s - loss: 4.8943e-04 - val_loss: 4.6135e-04 - 14s/epoch - 145us/sample
Epoch 83/85
95501/95501 - 14s - loss: 4.8168e-04 - val_loss: 4.9228e-04 - 14s/epoch - 145us/sample
Epoch 84/85
95501/95501 - 14s - loss: 4.9744e-04 - val_loss: 4.5630e-04 - 14s/epoch - 145us/sample
Epoch 85/85
95501/95501 - 14s - loss: 4.8656e-04 - val_loss: 4.6144e-04 - 14s/epoch - 145us/sample
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.00046144127392916016
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 10:34:06.753542: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_31/outputlayer/BiasAdd' id:40385 op device:{requested: '', assigned: ''} def:{{{node decoder_model_31/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_31/outputlayer/MatMul, decoder_model_31/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.0062148770408004895
cosine 0.004912775067441921
MAE: 0.012017927
RMSE: 0.02312976
r2: 0.9652943991976785
RMSE zero-vector: 0.23411466903540806
['1.5custom_VAE', 'logcosh', 64, 85, 0.001, 0.6, 758, 0.00048656042722384796, 0.00046144127392916016, 0.0062148770408004895, 0.004912775067441921, 0.012017927132546902, 0.02312975935637951, 0.9652943991976785, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.7000000000000002 85 0.001 64 2] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2148)         2717220     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_96 (BatchN  (None, 2148)        8592        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_96 (ReLU)                (None, 2148)         0           ['batch_normalization_96[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1628942     ['re_lu_96[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1628942     ['re_lu_96[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4933614     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 10,917,310
Trainable params: 10,907,202
Non-trainable params: 10,108
__________________________________________________________________________________________________
File geneticVAE_MMmp_gap_custom_VAE1.7000000000000002_cr0.6_bs64_ep85_loss_logcosh_lr0.001_AutoEncoder.h5 exists in folder already, skiping this calculation.
2023-02-15 10:34:24.382655: W tensorflow/c/c_api.cc:291] Operation '{name:'batch_normalization_96/moving_mean/Assign' id:41302 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_96/moving_mean/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_96/moving_mean, batch_normalization_96/moving_mean/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-02-15 10:34:31.458444: W tensorflow/c/c_api.cc:291] Operation '{name:'bottleneck_zmean_33/kernel/v/Assign' id:42464 op device:{requested: '', assigned: ''} def:{{{node bottleneck_zmean_33/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](bottleneck_zmean_33/kernel/v, bottleneck_zmean_33/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
2023-02-15 10:34:38.196183: W tensorflow/c/c_api.cc:291] Operation '{name:'decoder_model_33/outputlayer/BiasAdd' id:42132 op device:{requested: '', assigned: ''} def:{{{node decoder_model_33/outputlayer/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format="NHWC"](decoder_model_33/outputlayer/MatMul, decoder_model_33/outputlayer/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
correlation 0.0069161219346132365
cosine 0.005461893355087025
MAE: 0.013978392
RMSE: 0.02454654
r2: 0.9609125777355702
RMSE zero-vector: 0.23411466903540806
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
['1.7000000000000002custom_VAE', 'logcosh', 64, 85, 0.001, 0.6, 758, '--', '--', 0.0069161219346132365, 0.005461893355087025, 0.013978391885757446, 0.024546539410948753, 0.9609125777355702, 0.23411466903540806] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Generation = 3
Fitness    = 98.76685138538917
Last generation's best solutions = [1.6 145 0.0014 64 1] with fitness 98.76685138538917.
Best solutions :  [array([1.5, 145, 0.0012, 64, 1], dtype=object), array([1.6, 145, 0.0014, 64, 1], dtype=object), array([1.6, 145, 0.0014, 64, 1], dtype=object), array([1.6, 145, 0.0014, 64, 1], dtype=object)]
Best solutions fitness :  [96.28315495666305, 98.76685138538917, 98.76685138538917]
[1.5 145 0.0014 64 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization_99 (BatchN  (None, 1896)        7584        ['dense_enc0[0][0]']             
 ormalization)                                                                                    
                                                                                                  
 re_lu_99 (ReLU)                (None, 1896)         0           ['batch_normalization_99[0][0]'] 
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu_99[0][0]']               
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu_99[0][0]']               
                                                                                                  
 bottleneck (Lambda)            (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Train on 95501 samples, validate on 10612 samples
Epoch 1/145
2023-02-15 10:34:55.358878: W tensorflow/c/c_api.cc:291] Operation '{name:'training_64/Adam/bottleneck_zmean_34/bias/m/Assign' id:43635 op device:{requested: '', assigned: ''} def:{{{node training_64/Adam/bottleneck_zmean_34/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_64/Adam/bottleneck_zmean_34/bias/m, training_64/Adam/bottleneck_zmean_34/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
/home/ucl/modl/rgouvea/anaconda3/envs/env_modnetmod/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates = self.state_updates
2023-02-15 10:35:12.834515: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_34/mul' id:43149 op device:{requested: '', assigned: ''} def:{{{node loss_34/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_34/mul/x, loss_34/decoder_model_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
95501/95501 - 24s - loss: 0.0108 - val_loss: 0.0054 - 24s/epoch - 255us/sample
Epoch 2/145
95501/95501 - 14s - loss: 0.0043 - val_loss: 0.0036 - 14s/epoch - 147us/sample
Epoch 3/145
95501/95501 - 14s - loss: 0.0036 - val_loss: 0.0032 - 14s/epoch - 146us/sample
Epoch 4/145
95501/95501 - 14s - loss: 0.0033 - val_loss: 0.0028 - 14s/epoch - 146us/sample
Epoch 5/145
95501/95501 - 14s - loss: 0.0027 - val_loss: 0.0028 - 14s/epoch - 146us/sample
Epoch 6/145
95501/95501 - 14s - loss: 0.0025 - val_loss: 0.0020 - 14s/epoch - 146us/sample
Epoch 7/145
95501/95501 - 14s - loss: 0.0020 - val_loss: 0.0024 - 14s/epoch - 147us/sample
Epoch 8/145
95501/95501 - 14s - loss: 0.0018 - val_loss: 0.0016 - 14s/epoch - 147us/sample
Epoch 9/145
95501/95501 - 14s - loss: 0.0016 - val_loss: 0.0018 - 14s/epoch - 146us/sample
Epoch 10/145
95501/95501 - 14s - loss: 0.0015 - val_loss: 0.0013 - 14s/epoch - 146us/sample
Epoch 11/145
95501/95501 - 14s - loss: 0.0014 - val_loss: 0.0012 - 14s/epoch - 146us/sample
Epoch 12/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0033 - 14s/epoch - 146us/sample
Epoch 13/145
95501/95501 - 14s - loss: 0.0019 - val_loss: 0.0016 - 14s/epoch - 147us/sample
Epoch 14/145
95501/95501 - 14s - loss: 0.0016 - val_loss: 0.0012 - 14s/epoch - 147us/sample
Epoch 15/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0014 - 14s/epoch - 146us/sample
Epoch 16/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 17/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 0.0010 - 14s/epoch - 146us/sample
Epoch 18/145
95501/95501 - 14s - loss: 0.0011 - val_loss: 0.0010 - 14s/epoch - 146us/sample
Epoch 19/145
95501/95501 - 14s - loss: 0.0011 - val_loss: 0.0010 - 14s/epoch - 147us/sample
Epoch 20/145
95501/95501 - 14s - loss: 0.0011 - val_loss: 9.7255e-04 - 14s/epoch - 146us/sample
Epoch 21/145
95501/95501 - 14s - loss: 0.0010 - val_loss: 9.7964e-04 - 14s/epoch - 146us/sample
Epoch 22/145
95501/95501 - 14s - loss: 0.0010 - val_loss: 9.4340e-04 - 14s/epoch - 146us/sample
Epoch 23/145
95501/95501 - 14s - loss: 0.0010 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 24/145
95501/95501 - 14s - loss: 0.0011 - val_loss: 9.5157e-04 - 14s/epoch - 146us/sample
Epoch 25/145
95501/95501 - 14s - loss: 0.0010 - val_loss: 0.0021 - 14s/epoch - 147us/sample
Epoch 26/145
95501/95501 - 14s - loss: 0.0014 - val_loss: 0.0015 - 14s/epoch - 146us/sample
Epoch 27/145
95501/95501 - 14s - loss: 0.0013 - val_loss: 0.0010 - 14s/epoch - 146us/sample
Epoch 28/145
95501/95501 - 14s - loss: 0.0011 - val_loss: 0.0012 - 14s/epoch - 146us/sample
Epoch 29/145
95501/95501 - 14s - loss: 0.0011 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 30/145
95501/95501 - 14s - loss: 9.9628e-04 - val_loss: 0.0012 - 14s/epoch - 146us/sample
Epoch 31/145
95501/95501 - 14s - loss: 0.0012 - val_loss: 9.4439e-04 - 14s/epoch - 147us/sample
Epoch 32/145
95501/95501 - 14s - loss: 0.0010 - val_loss: 9.0723e-04 - 14s/epoch - 146us/sample
Epoch 33/145
95501/95501 - 14s - loss: 9.5794e-04 - val_loss: 8.7015e-04 - 14s/epoch - 146us/sample
Epoch 34/145
95501/95501 - 14s - loss: 9.4623e-04 - val_loss: 9.2432e-04 - 14s/epoch - 146us/sample
Epoch 35/145
95501/95501 - 14s - loss: 9.7318e-04 - val_loss: 9.0983e-04 - 14s/epoch - 146us/sample
Epoch 36/145
95501/95501 - 14s - loss: 9.6203e-04 - val_loss: 8.7141e-04 - 14s/epoch - 147us/sample
Epoch 37/145
95501/95501 - 14s - loss: 9.1711e-04 - val_loss: 8.5503e-04 - 14s/epoch - 146us/sample
Epoch 38/145
95501/95501 - 14s - loss: 9.0724e-04 - val_loss: 8.5135e-04 - 14s/epoch - 146us/sample
Epoch 39/145
95501/95501 - 14s - loss: 8.9766e-04 - val_loss: 9.1650e-04 - 14s/epoch - 146us/sample
Epoch 40/145
95501/95501 - 14s - loss: 9.5229e-04 - val_loss: 8.3980e-04 - 14s/epoch - 146us/sample
Epoch 41/145
95501/95501 - 14s - loss: 9.0695e-04 - val_loss: 0.0011 - 14s/epoch - 146us/sample
Epoch 42/145
95501/95501 - 14s - loss: 8.9635e-04 - val_loss: 0.0010 - 14s/epoch - 147us/sample
Epoch 43/145
95501/95501 - 14s - loss: 0.0010 - val_loss: 8.3271e-04 - 14s/epoch - 146us/sample
Epoch 44/145
95501/95501 - 14s - loss: 8.8444e-04 - val_loss: 8.5726e-04 - 14s/epoch - 146us/sample
Epoch 45/145
95501/95501 - 14s - loss: 9.0709e-04 - val_loss: 8.8143e-04 - 14s/epoch - 146us/sample
Epoch 46/145
95501/95501 - 14s - loss: 9.1798e-04 - val_loss: 8.0574e-04 - 14s/epoch - 146us/sample
Epoch 47/145
95501/95501 - 14s - loss: 8.6567e-04 - val_loss: 8.0689e-04 - 14s/epoch - 147us/sample
Epoch 48/145
95501/95501 - 14s - loss: 8.6290e-04 - val_loss: 8.0108e-04 - 14s/epoch - 147us/sample
Epoch 49/145
95501/95501 - 14s - loss: 8.6274e-04 - val_loss: 7.9127e-04 - 14s/epoch - 146us/sample
Epoch 50/145
95501/95501 - 14s - loss: 8.5150e-04 - val_loss: 8.2151e-04 - 14s/epoch - 146us/sample
Epoch 51/145
95501/95501 - 14s - loss: 8.5526e-04 - val_loss: 8.2734e-04 - 14s/epoch - 146us/sample
Epoch 52/145
95501/95501 - 14s - loss: 8.6678e-04 - val_loss: 7.8358e-04 - 14s/epoch - 146us/sample
Epoch 53/145
95501/95501 - 14s - loss: 8.4419e-04 - val_loss: 9.2281e-04 - 14s/epoch - 147us/sample
Epoch 54/145
95501/95501 - 14s - loss: 9.2150e-04 - val_loss: 7.8338e-04 - 14s/epoch - 147us/sample
Epoch 55/145
95501/95501 - 14s - loss: 8.7572e-04 - val_loss: 7.7972e-04 - 14s/epoch - 146us/sample
Epoch 56/145
95501/95501 - 14s - loss: 8.3557e-04 - val_loss: 7.8889e-04 - 14s/epoch - 146us/sample
Epoch 57/145
95501/95501 - 14s - loss: 8.3384e-04 - val_loss: 7.7247e-04 - 14s/epoch - 146us/sample
Epoch 58/145
95501/95501 - 14s - loss: 8.2387e-04 - val_loss: 7.6829e-04 - 14s/epoch - 146us/sample
Epoch 59/145
95501/95501 - 14s - loss: 8.2732e-04 - val_loss: 9.0871e-04 - 14s/epoch - 148us/sample
Epoch 60/145
95501/95501 - 14s - loss: 9.7551e-04 - val_loss: 7.8744e-04 - 14s/epoch - 146us/sample
Epoch 61/145
95501/95501 - 14s - loss: 8.3357e-04 - val_loss: 7.6459e-04 - 14s/epoch - 146us/sample
Epoch 62/145
95501/95501 - 14s - loss: 8.1982e-04 - val_loss: 8.0626e-04 - 14s/epoch - 146us/sample
Epoch 63/145
95501/95501 - 14s - loss: 8.7003e-04 - val_loss: 7.6592e-04 - 14s/epoch - 146us/sample
Epoch 64/145
95501/95501 - 14s - loss: 8.1959e-04 - val_loss: 7.8342e-04 - 14s/epoch - 146us/sample
Epoch 65/145
95501/95501 - 14s - loss: 8.3873e-04 - val_loss: 8.3618e-04 - 14s/epoch - 147us/sample
Epoch 66/145
95501/95501 - 14s - loss: 8.8976e-04 - val_loss: 7.8581e-04 - 14s/epoch - 146us/sample
Epoch 67/145
95501/95501 - 14s - loss: 8.7320e-04 - val_loss: 7.9193e-04 - 14s/epoch - 146us/sample
Epoch 68/145
95501/95501 - 14s - loss: 8.5304e-04 - val_loss: 7.7774e-04 - 14s/epoch - 146us/sample
Epoch 69/145
95501/95501 - 14s - loss: 8.2344e-04 - val_loss: 7.5726e-04 - 14s/epoch - 146us/sample
Epoch 70/145
95501/95501 - 14s - loss: 8.3605e-04 - val_loss: 7.5050e-04 - 14s/epoch - 148us/sample
Epoch 71/145
95501/95501 - 14s - loss: 8.2831e-04 - val_loss: 8.6303e-04 - 14s/epoch - 146us/sample
Epoch 72/145
95501/95501 - 14s - loss: 9.3889e-04 - val_loss: 8.3431e-04 - 14s/epoch - 146us/sample
Epoch 73/145
95501/95501 - 14s - loss: 8.7046e-04 - val_loss: 8.4663e-04 - 14s/epoch - 146us/sample
Epoch 74/145
95501/95501 - 14s - loss: 9.0739e-04 - val_loss: 7.6575e-04 - 14s/epoch - 146us/sample
Epoch 75/145
95501/95501 - 14s - loss: 8.6277e-04 - val_loss: 7.5668e-04 - 14s/epoch - 147us/sample
Epoch 76/145
95501/95501 - 14s - loss: 8.1437e-04 - val_loss: 7.8194e-04 - 14s/epoch - 147us/sample
Epoch 77/145
95501/95501 - 14s - loss: 8.3842e-04 - val_loss: 7.4630e-04 - 14s/epoch - 146us/sample
Epoch 78/145
95501/95501 - 14s - loss: 8.1038e-04 - val_loss: 7.5561e-04 - 14s/epoch - 146us/sample
Epoch 79/145
95501/95501 - 14s - loss: 8.0061e-04 - val_loss: 7.4493e-04 - 14s/epoch - 146us/sample
Epoch 80/145
95501/95501 - 14s - loss: 8.0413e-04 - val_loss: 7.5081e-04 - 14s/epoch - 146us/sample
Epoch 81/145
95501/95501 - 14s - loss: 7.9463e-04 - val_loss: 7.9866e-04 - 14s/epoch - 147us/sample
Epoch 82/145
95501/95501 - 14s - loss: 8.3717e-04 - val_loss: 7.7954e-04 - 14s/epoch - 147us/sample
Epoch 83/145
95501/95501 - 14s - loss: 8.1597e-04 - val_loss: 7.3450e-04 - 14s/epoch - 146us/sample
Epoch 84/145
95501/95501 - 14s - loss: 7.8604e-04 - val_loss: 7.6848e-04 - 14s/epoch - 146us/sample
Epoch 85/145
95501/95501 - 14s - loss: 8.3062e-04 - val_loss: 7.4315e-04 - 14s/epoch - 146us/sample
Epoch 86/145
95501/95501 - 14s - loss: 7.8588e-04 - val_loss: 7.4216e-04 - 14s/epoch - 146us/sample
Epoch 87/145
95501/95501 - 14s - loss: 7.8693e-04 - val_loss: 7.5530e-04 - 14s/epoch - 147us/sample
Epoch 88/145
95501/95501 - 14s - loss: 8.1261e-04 - val_loss: 7.7858e-04 - 14s/epoch - 146us/sample
Epoch 89/145
95501/95501 - 14s - loss: 8.3321e-04 - val_loss: 7.4524e-04 - 14s/epoch - 146us/sample
Epoch 90/145
95501/95501 - 14s - loss: 7.9114e-04 - val_loss: 8.0036e-04 - 14s/epoch - 146us/sample
Epoch 91/145
95501/95501 - 14s - loss: 8.5668e-04 - val_loss: 7.9432e-04 - 14s/epoch - 146us/sample
Epoch 92/145
95501/95501 - 14s - loss: 8.4588e-04 - val_loss: 8.1779e-04 - 14s/epoch - 146us/sample
Epoch 93/145
95501/95501 - 14s - loss: 8.6012e-04 - val_loss: 7.3927e-04 - 14s/epoch - 147us/sample
Epoch 94/145
95501/95501 - 14s - loss: 7.8604e-04 - val_loss: 7.2896e-04 - 14s/epoch - 146us/sample
Epoch 95/145
95501/95501 - 14s - loss: 7.7932e-04 - val_loss: 7.3596e-04 - 14s/epoch - 146us/sample
Epoch 96/145
95501/95501 - 14s - loss: 7.8014e-04 - val_loss: 7.4514e-04 - 14s/epoch - 146us/sample
Epoch 97/145
95501/95501 - 14s - loss: 8.3025e-04 - val_loss: 7.4366e-04 - 14s/epoch - 146us/sample
Epoch 98/145
95501/95501 - 14s - loss: 7.8899e-04 - val_loss: 7.8023e-04 - 14s/epoch - 147us/sample
Epoch 99/145
95501/95501 - 14s - loss: 8.5405e-04 - val_loss: 7.3307e-04 - 14s/epoch - 147us/sample
Epoch 100/145
95501/95501 - 14s - loss: 7.7749e-04 - val_loss: 7.2323e-04 - 14s/epoch - 146us/sample
Epoch 101/145
95501/95501 - 14s - loss: 7.8065e-04 - val_loss: 7.1337e-04 - 14s/epoch - 146us/sample
Epoch 102/145
95501/95501 - 14s - loss: 7.7505e-04 - val_loss: 7.4569e-04 - 14s/epoch - 146us/sample
Epoch 103/145
95501/95501 - 14s - loss: 7.6880e-04 - val_loss: 7.3902e-04 - 14s/epoch - 146us/sample
Epoch 104/145
95501/95501 - 14s - loss: 7.9232e-04 - val_loss: 7.2052e-04 - 14s/epoch - 147us/sample
Epoch 105/145
95501/95501 - 14s - loss: 7.6554e-04 - val_loss: 7.1287e-04 - 14s/epoch - 146us/sample
Epoch 106/145
95501/95501 - 14s - loss: 7.6134e-04 - val_loss: 7.2151e-04 - 14s/epoch - 146us/sample
Epoch 107/145
95501/95501 - 14s - loss: 7.7993e-04 - val_loss: 8.1782e-04 - 14s/epoch - 146us/sample
Epoch 108/145
95501/95501 - 14s - loss: 8.8239e-04 - val_loss: 7.2544e-04 - 14s/epoch - 146us/sample
Epoch 109/145
95501/95501 - 14s - loss: 7.6808e-04 - val_loss: 7.0800e-04 - 14s/epoch - 147us/sample
Epoch 110/145
95501/95501 - 14s - loss: 7.6411e-04 - val_loss: 8.1350e-04 - 14s/epoch - 146us/sample
Epoch 111/145
95501/95501 - 14s - loss: 8.2664e-04 - val_loss: 7.8769e-04 - 14s/epoch - 146us/sample
Epoch 112/145
95501/95501 - 14s - loss: 8.1814e-04 - val_loss: 7.1166e-04 - 14s/epoch - 146us/sample
Epoch 113/145
95501/95501 - 14s - loss: 7.6298e-04 - val_loss: 7.2988e-04 - 14s/epoch - 146us/sample
Epoch 114/145
95501/95501 - 14s - loss: 7.7396e-04 - val_loss: 7.1896e-04 - 14s/epoch - 146us/sample
Epoch 115/145
95501/95501 - 14s - loss: 7.6405e-04 - val_loss: 7.5630e-04 - 14s/epoch - 148us/sample
Epoch 116/145
95501/95501 - 14s - loss: 8.1001e-04 - val_loss: 7.1743e-04 - 14s/epoch - 146us/sample
Epoch 117/145
95501/95501 - 14s - loss: 7.5738e-04 - val_loss: 7.0761e-04 - 14s/epoch - 146us/sample
Epoch 118/145
95501/95501 - 14s - loss: 7.7418e-04 - val_loss: 7.0626e-04 - 14s/epoch - 146us/sample
Epoch 119/145
95501/95501 - 14s - loss: 7.5527e-04 - val_loss: 7.0630e-04 - 14s/epoch - 146us/sample
Epoch 120/145
95501/95501 - 14s - loss: 7.4998e-04 - val_loss: 7.0569e-04 - 14s/epoch - 146us/sample
Epoch 121/145
95501/95501 - 14s - loss: 7.5042e-04 - val_loss: 7.0282e-04 - 14s/epoch - 147us/sample
Epoch 122/145
95501/95501 - 14s - loss: 7.5204e-04 - val_loss: 7.2719e-04 - 14s/epoch - 146us/sample
Epoch 123/145
95501/95501 - 14s - loss: 7.7776e-04 - val_loss: 7.6808e-04 - 14s/epoch - 146us/sample
Epoch 124/145
95501/95501 - 14s - loss: 7.9691e-04 - val_loss: 7.0183e-04 - 14s/epoch - 146us/sample
Epoch 125/145
95501/95501 - 14s - loss: 7.5146e-04 - val_loss: 6.9353e-04 - 14s/epoch - 146us/sample
Epoch 126/145
95501/95501 - 14s - loss: 7.4884e-04 - val_loss: 7.4402e-04 - 14s/epoch - 147us/sample
Epoch 127/145
95501/95501 - 14s - loss: 8.0361e-04 - val_loss: 7.8184e-04 - 14s/epoch - 147us/sample
Epoch 128/145
95501/95501 - 14s - loss: 8.5309e-04 - val_loss: 7.1426e-04 - 14s/epoch - 146us/sample
Epoch 129/145
95501/95501 - 14s - loss: 7.5954e-04 - val_loss: 7.0153e-04 - 14s/epoch - 146us/sample
Epoch 130/145
95501/95501 - 14s - loss: 7.5060e-04 - val_loss: 7.4362e-04 - 14s/epoch - 146us/sample
Epoch 131/145
95501/95501 - 14s - loss: 7.9227e-04 - val_loss: 6.9474e-04 - 14s/epoch - 144us/sample
Epoch 132/145
95501/95501 - 12s - loss: 7.4766e-04 - val_loss: 7.4055e-04 - 12s/epoch - 123us/sample
Epoch 133/145
95501/95501 - 12s - loss: 7.7395e-04 - val_loss: 6.9550e-04 - 12s/epoch - 121us/sample
Epoch 134/145
95501/95501 - 12s - loss: 7.4792e-04 - val_loss: 6.9673e-04 - 12s/epoch - 122us/sample
Epoch 135/145
95501/95501 - 12s - loss: 7.4824e-04 - val_loss: 6.8879e-04 - 12s/epoch - 121us/sample
Epoch 136/145
slurmstepd: error: *** JOB 36005607 ON mb-rom101 CANCELLED AT 2023-02-15T11:06:21 ***
