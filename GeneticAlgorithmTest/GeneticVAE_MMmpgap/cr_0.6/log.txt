start
Wed Feb 22 18:39:16 CET 2023
2023-02-22 18:39:17.198579: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-22 18:39:17.361342: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-02-22 18:39:56,794 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7f63f7dbe040> object, created with modnet version 0.1.12
NAN values: 12054
NAN values remaining: 0
        AtomicOrbitals|HOMO_character  ...  BondFractions|B - B bond frac.
id                                     ...                                
0                                 3.0  ...                             0.0
1                                 3.0  ...                             0.0
2                                 2.0  ...                             0.0
3                                 2.0  ...                             0.0
4                                 2.0  ...                             0.0
...                               ...  ...                             ...
106108                            3.0  ...                             0.0
106109                            2.0  ...                             0.0
106110                            3.0  ...                             0.0
106111                            3.0  ...                             0.0
106112                            1.0  ...                             0.0

[106113 rows x 1336 columns]
/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py:486: UserWarning: The percentage of genes to mutate (mutation_percent_genes=10) resutled in selecting (0) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).
If you do not want to mutate any gene, please set mutation_type=None.
  if not self.suppress_warnings: warnings.warn("The percentage of genes to mutate (mutation_percent_genes={mutation_percent}) resutled in selecting ({mutation_num}) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).\nIf you do not want to mutate any gene, please set mutation_type=None.".format(mutation_percent=mutation_percent_genes, mutation_num=mutation_num_genes))
/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py:764: UserWarning: Starting from PyGAD 2.6.0, the callback_generation parameter is deprecated and will be removed in a later release of PyGAD. Please use the on_generation parameter instead.
  if not self.suppress_warnings: warnings.warn("Starting from PyGAD 2.6.0, the callback_generation parameter is deprecated and will be removed in a later release of PyGAD. Please use the on_generation parameter instead.")
/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py:820: UserWarning: Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.")
/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py:828: UserWarning: Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.")
[[0.5 210 0.0005 16 1]
 [1.5 90 0.0005 16 1]
 [1.5 210 0.002 256 2]
 [1.0 90 0.0005 16 2]
 [1.0 90 0.002 64 2]
 [2.0 90 0.001 128 1]
 [2.0 120 0.0005 128 1]
 [0.5 90 0.001 128 1]
 [2.0 90 0.001 128 1]
 [2.0 120 0.002 256 1]
 [1.5 210 0.002 256 1]
 [2.0 120 0.001 32 1]
 [2.5 30 0.001 256 1]
 [0.5 30 0.0005 256 1]
 [2.0 150 0.002 256 1]]
[0.5 210 0.0005 16 1] 0
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
2023-02-22 18:40:00.152573: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-22 18:40:00.726486: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2023-02-22 18:40:00.727337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22295 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:23:00.0, compute capability: 8.6
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          479814      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          479814      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1860682     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 3,622,318
Trainable params: 3,618,274
Non-trainable params: 4,044
__________________________________________________________________________________________________
Epoch 1/210
2023-02-22 18:40:05.765342: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
5969/5969 - 26s - loss: 0.0175 - val_loss: 0.0130 - 26s/epoch - 4ms/step
Epoch 2/210
5969/5969 - 23s - loss: 0.0131 - val_loss: 0.0127 - 23s/epoch - 4ms/step
Epoch 3/210
5969/5969 - 23s - loss: 0.0129 - val_loss: 0.0125 - 23s/epoch - 4ms/step
Epoch 4/210
5969/5969 - 23s - loss: 0.0127 - val_loss: 0.0132 - 23s/epoch - 4ms/step
Epoch 5/210
5969/5969 - 23s - loss: 0.0127 - val_loss: 0.0152 - 23s/epoch - 4ms/step
Epoch 6/210
5969/5969 - 23s - loss: 0.0126 - val_loss: 0.0144 - 23s/epoch - 4ms/step
Epoch 7/210
5969/5969 - 23s - loss: 0.0126 - val_loss: 0.0162 - 23s/epoch - 4ms/step
Epoch 8/210
5969/5969 - 23s - loss: 0.0126 - val_loss: 0.0181 - 23s/epoch - 4ms/step
Epoch 9/210
5969/5969 - 23s - loss: 0.0126 - val_loss: 0.0164 - 23s/epoch - 4ms/step
Epoch 10/210
5969/5969 - 23s - loss: 0.0125 - val_loss: 0.0168 - 23s/epoch - 4ms/step
Epoch 11/210
5969/5969 - 23s - loss: 0.0125 - val_loss: 0.0259 - 23s/epoch - 4ms/step
Epoch 12/210
5969/5969 - 23s - loss: 0.0125 - val_loss: 0.0213 - 23s/epoch - 4ms/step
Epoch 13/210
5969/5969 - 23s - loss: 0.0125 - val_loss: 0.0182 - 23s/epoch - 4ms/step
Epoch 14/210
5969/5969 - 23s - loss: 0.0124 - val_loss: 0.0240 - 23s/epoch - 4ms/step
Epoch 15/210
5969/5969 - 23s - loss: 0.0121 - val_loss: 0.0285 - 23s/epoch - 4ms/step
Epoch 16/210
5969/5969 - 23s - loss: 0.0119 - val_loss: 0.0297 - 23s/epoch - 4ms/step
Epoch 17/210
5969/5969 - 23s - loss: 0.0118 - val_loss: 0.0309 - 23s/epoch - 4ms/step
Epoch 18/210
5969/5969 - 23s - loss: 0.0118 - val_loss: 0.0280 - 23s/epoch - 4ms/step
Epoch 19/210
5969/5969 - 23s - loss: 0.0118 - val_loss: 0.0360 - 23s/epoch - 4ms/step
Epoch 20/210
5969/5969 - 23s - loss: 0.0117 - val_loss: 0.0401 - 23s/epoch - 4ms/step
Epoch 21/210
5969/5969 - 23s - loss: 0.0117 - val_loss: 0.0392 - 23s/epoch - 4ms/step
Epoch 22/210
5969/5969 - 23s - loss: 0.0117 - val_loss: 0.0371 - 23s/epoch - 4ms/step
Epoch 23/210
5969/5969 - 23s - loss: 0.0117 - val_loss: 0.0319 - 23s/epoch - 4ms/step
Epoch 24/210
5969/5969 - 23s - loss: 0.0117 - val_loss: 0.0369 - 23s/epoch - 4ms/step
Epoch 25/210
5969/5969 - 23s - loss: 0.0117 - val_loss: 0.0381 - 23s/epoch - 4ms/step
Epoch 26/210
5969/5969 - 23s - loss: 0.0116 - val_loss: 0.0463 - 23s/epoch - 4ms/step
Epoch 27/210
5969/5969 - 23s - loss: 0.0116 - val_loss: 0.0369 - 23s/epoch - 4ms/step
Epoch 28/210
5969/5969 - 24s - loss: 0.0116 - val_loss: 0.0434 - 24s/epoch - 4ms/step
Epoch 29/210
5969/5969 - 23s - loss: 0.0116 - val_loss: 0.0364 - 23s/epoch - 4ms/step
Epoch 30/210
5969/5969 - 23s - loss: 0.0116 - val_loss: 0.0388 - 23s/epoch - 4ms/step
Epoch 31/210
5969/5969 - 23s - loss: 0.0116 - val_loss: 0.0331 - 23s/epoch - 4ms/step
Epoch 32/210
5969/5969 - 23s - loss: 0.0116 - val_loss: 0.0508 - 23s/epoch - 4ms/step
Epoch 33/210
5969/5969 - 23s - loss: 0.0116 - val_loss: 0.0296 - 23s/epoch - 4ms/step
Epoch 34/210
5969/5969 - 23s - loss: 0.0116 - val_loss: 0.0407 - 23s/epoch - 4ms/step
Epoch 35/210
5969/5969 - 23s - loss: 0.0116 - val_loss: 0.0477 - 23s/epoch - 4ms/step
Epoch 36/210
5969/5969 - 23s - loss: 0.0116 - val_loss: 0.0462 - 23s/epoch - 4ms/step
Epoch 37/210
5969/5969 - 23s - loss: 0.0116 - val_loss: 0.0454 - 23s/epoch - 4ms/step
Epoch 38/210
5969/5969 - 24s - loss: 0.0115 - val_loss: 0.0489 - 24s/epoch - 4ms/step
Epoch 39/210
5969/5969 - 24s - loss: 0.0115 - val_loss: 0.0408 - 24s/epoch - 4ms/step
Epoch 40/210
5969/5969 - 24s - loss: 0.0115 - val_loss: 0.0411 - 24s/epoch - 4ms/step
Epoch 41/210
5969/5969 - 23s - loss: 0.0115 - val_loss: 0.0571 - 23s/epoch - 4ms/step
Epoch 42/210
5969/5969 - 24s - loss: 0.0115 - val_loss: 0.0655 - 24s/epoch - 4ms/step
Epoch 43/210
5969/5969 - 23s - loss: 0.0115 - val_loss: 0.0550 - 23s/epoch - 4ms/step
Epoch 44/210
5969/5969 - 23s - loss: 0.0115 - val_loss: 0.0505 - 23s/epoch - 4ms/step
Epoch 45/210
5969/5969 - 24s - loss: 0.0115 - val_loss: 0.0376 - 24s/epoch - 4ms/step
Epoch 46/210
5969/5969 - 24s - loss: 0.0115 - val_loss: 0.0622 - 24s/epoch - 4ms/step
Epoch 47/210
5969/5969 - 24s - loss: 0.0115 - val_loss: 0.1044 - 24s/epoch - 4ms/step
Epoch 48/210
5969/5969 - 26s - loss: 0.0114 - val_loss: 0.0803 - 26s/epoch - 4ms/step
Epoch 49/210
5969/5969 - 26s - loss: 0.0114 - val_loss: 0.0768 - 26s/epoch - 4ms/step
Epoch 50/210
5969/5969 - 25s - loss: 0.0114 - val_loss: 0.0810 - 25s/epoch - 4ms/step
Epoch 51/210
5969/5969 - 24s - loss: 0.0114 - val_loss: 0.0716 - 24s/epoch - 4ms/step
Epoch 52/210
5969/5969 - 25s - loss: 0.0113 - val_loss: 0.0852 - 25s/epoch - 4ms/step
Epoch 53/210
5969/5969 - 24s - loss: 0.0113 - val_loss: 0.0760 - 24s/epoch - 4ms/step
Epoch 54/210
5969/5969 - 25s - loss: 0.0113 - val_loss: 0.1055 - 25s/epoch - 4ms/step
Epoch 55/210
5969/5969 - 24s - loss: 0.0113 - val_loss: 0.0682 - 24s/epoch - 4ms/step
Epoch 56/210
5969/5969 - 25s - loss: 0.0113 - val_loss: 0.0510 - 25s/epoch - 4ms/step
Epoch 57/210
5969/5969 - 24s - loss: 0.0113 - val_loss: 0.1562 - 24s/epoch - 4ms/step
Epoch 58/210
5969/5969 - 25s - loss: 0.0113 - val_loss: 0.0648 - 25s/epoch - 4ms/step
Epoch 59/210
5969/5969 - 25s - loss: 0.0113 - val_loss: 0.0676 - 25s/epoch - 4ms/step
Epoch 60/210
5969/5969 - 24s - loss: 0.0113 - val_loss: 0.0530 - 24s/epoch - 4ms/step
Epoch 61/210
5969/5969 - 25s - loss: 0.0113 - val_loss: 0.0648 - 25s/epoch - 4ms/step
Epoch 62/210
5969/5969 - 24s - loss: 0.0113 - val_loss: 0.0738 - 24s/epoch - 4ms/step
Epoch 63/210
5969/5969 - 24s - loss: 0.0113 - val_loss: 0.0598 - 24s/epoch - 4ms/step
Epoch 64/210
5969/5969 - 25s - loss: 0.0113 - val_loss: 0.1312 - 25s/epoch - 4ms/step
Epoch 65/210
5969/5969 - 24s - loss: 0.0113 - val_loss: 0.1152 - 24s/epoch - 4ms/step
Epoch 66/210
5969/5969 - 25s - loss: 0.0113 - val_loss: 0.0755 - 25s/epoch - 4ms/step
Epoch 67/210
5969/5969 - 25s - loss: 0.0113 - val_loss: 0.0835 - 25s/epoch - 4ms/step
Epoch 68/210
5969/5969 - 24s - loss: 0.0113 - val_loss: 0.1082 - 24s/epoch - 4ms/step
Epoch 69/210
5969/5969 - 25s - loss: 0.0113 - val_loss: 0.0491 - 25s/epoch - 4ms/step
Epoch 70/210
5969/5969 - 25s - loss: 0.0112 - val_loss: 0.0872 - 25s/epoch - 4ms/step
Epoch 71/210
5969/5969 - 25s - loss: 0.0112 - val_loss: 0.0663 - 25s/epoch - 4ms/step
Epoch 72/210
5969/5969 - 24s - loss: 0.0112 - val_loss: 0.0628 - 24s/epoch - 4ms/step
Epoch 73/210
5969/5969 - 25s - loss: 0.0112 - val_loss: 0.1383 - 25s/epoch - 4ms/step
Epoch 74/210
5969/5969 - 24s - loss: 0.0112 - val_loss: 0.0669 - 24s/epoch - 4ms/step
Epoch 75/210
5969/5969 - 25s - loss: 0.0112 - val_loss: 0.0811 - 25s/epoch - 4ms/step
Epoch 76/210
5969/5969 - 24s - loss: 0.0112 - val_loss: 0.1045 - 24s/epoch - 4ms/step
Epoch 77/210
5969/5969 - 24s - loss: 0.0112 - val_loss: 0.0702 - 24s/epoch - 4ms/step
Epoch 78/210
5969/5969 - 24s - loss: 0.0112 - val_loss: 0.1040 - 24s/epoch - 4ms/step
Epoch 79/210
5969/5969 - 24s - loss: 0.0112 - val_loss: 0.1279 - 24s/epoch - 4ms/step
Epoch 80/210
5969/5969 - 24s - loss: 0.0112 - val_loss: 0.1103 - 24s/epoch - 4ms/step
Epoch 81/210
5969/5969 - 25s - loss: 0.0112 - val_loss: 0.0944 - 25s/epoch - 4ms/step
Epoch 82/210
5969/5969 - 24s - loss: 0.0112 - val_loss: 0.1109 - 24s/epoch - 4ms/step
Epoch 83/210
5969/5969 - 24s - loss: 0.0112 - val_loss: 0.0504 - 24s/epoch - 4ms/step
Epoch 84/210
5969/5969 - 24s - loss: 0.0112 - val_loss: 0.0799 - 24s/epoch - 4ms/step
Epoch 85/210
5969/5969 - 25s - loss: 0.0112 - val_loss: 0.0964 - 25s/epoch - 4ms/step
Epoch 86/210
5969/5969 - 24s - loss: 0.0112 - val_loss: 0.0907 - 24s/epoch - 4ms/step
Epoch 87/210
5969/5969 - 24s - loss: 0.0112 - val_loss: 0.0941 - 24s/epoch - 4ms/step
Epoch 88/210
5969/5969 - 25s - loss: 0.0112 - val_loss: 0.0986 - 25s/epoch - 4ms/step
Epoch 89/210
5969/5969 - 24s - loss: 0.0112 - val_loss: 0.0783 - 24s/epoch - 4ms/step
Epoch 90/210
5969/5969 - 25s - loss: 0.0112 - val_loss: 0.0692 - 25s/epoch - 4ms/step
Epoch 91/210
5969/5969 - 24s - loss: 0.0112 - val_loss: 0.0712 - 24s/epoch - 4ms/step
Epoch 92/210
5969/5969 - 24s - loss: 0.0112 - val_loss: 0.1003 - 24s/epoch - 4ms/step
Epoch 93/210
5969/5969 - 25s - loss: 0.0112 - val_loss: 0.0483 - 25s/epoch - 4ms/step
Epoch 94/210
5969/5969 - 24s - loss: 0.0112 - val_loss: 0.0586 - 24s/epoch - 4ms/step
Epoch 95/210
5969/5969 - 25s - loss: 0.0112 - val_loss: 0.1504 - 25s/epoch - 4ms/step
Epoch 96/210
5969/5969 - 24s - loss: 0.0112 - val_loss: 0.0885 - 24s/epoch - 4ms/step
Epoch 97/210
5969/5969 - 24s - loss: 0.0112 - val_loss: 0.0745 - 24s/epoch - 4ms/step
Epoch 98/210
5969/5969 - 24s - loss: 0.0112 - val_loss: 0.0908 - 24s/epoch - 4ms/step
Epoch 99/210
5969/5969 - 24s - loss: 0.0112 - val_loss: 0.0564 - 24s/epoch - 4ms/step
Epoch 100/210
5969/5969 - 24s - loss: 0.0112 - val_loss: 0.0940 - 24s/epoch - 4ms/step
Epoch 101/210
5969/5969 - 25s - loss: 0.0112 - val_loss: 0.0868 - 25s/epoch - 4ms/step
Epoch 102/210
5969/5969 - 24s - loss: 0.0112 - val_loss: 0.0916 - 24s/epoch - 4ms/step
Epoch 103/210
5969/5969 - 24s - loss: 0.0112 - val_loss: 0.0704 - 24s/epoch - 4ms/step
Epoch 104/210
5969/5969 - 25s - loss: 0.0112 - val_loss: 0.1140 - 25s/epoch - 4ms/step
Epoch 105/210
5969/5969 - 25s - loss: 0.0112 - val_loss: 0.0657 - 25s/epoch - 4ms/step
Epoch 106/210
5969/5969 - 25s - loss: 0.0111 - val_loss: 0.0648 - 25s/epoch - 4ms/step
Epoch 107/210
5969/5969 - 25s - loss: 0.0111 - val_loss: 0.0790 - 25s/epoch - 4ms/step
Epoch 108/210
5969/5969 - 24s - loss: 0.0111 - val_loss: 0.1149 - 24s/epoch - 4ms/step
Epoch 109/210
5969/5969 - 24s - loss: 0.0111 - val_loss: 0.0531 - 24s/epoch - 4ms/step
Epoch 110/210
5969/5969 - 24s - loss: 0.0111 - val_loss: 0.1263 - 24s/epoch - 4ms/step
Epoch 111/210
5969/5969 - 24s - loss: 0.0111 - val_loss: 0.0675 - 24s/epoch - 4ms/step
Epoch 112/210
5969/5969 - 24s - loss: 0.0111 - val_loss: 0.0761 - 24s/epoch - 4ms/step
Epoch 113/210
5969/5969 - 24s - loss: 0.0110 - val_loss: 0.1374 - 24s/epoch - 4ms/step
Epoch 114/210
5969/5969 - 25s - loss: 0.0110 - val_loss: 0.1083 - 25s/epoch - 4ms/step
Epoch 115/210
5969/5969 - 25s - loss: 0.0110 - val_loss: 0.1497 - 25s/epoch - 4ms/step
Epoch 116/210
5969/5969 - 25s - loss: 0.0110 - val_loss: 0.0959 - 25s/epoch - 4ms/step
Epoch 117/210
5969/5969 - 24s - loss: 0.0110 - val_loss: 0.0768 - 24s/epoch - 4ms/step
Epoch 118/210
5969/5969 - 24s - loss: 0.0110 - val_loss: 0.0874 - 24s/epoch - 4ms/step
Epoch 119/210
5969/5969 - 24s - loss: 0.0110 - val_loss: 0.1057 - 24s/epoch - 4ms/step
Epoch 120/210
5969/5969 - 24s - loss: 0.0110 - val_loss: 0.1269 - 24s/epoch - 4ms/step
Epoch 121/210
5969/5969 - 24s - loss: 0.0109 - val_loss: 0.0582 - 24s/epoch - 4ms/step
Epoch 122/210
5969/5969 - 25s - loss: 0.0109 - val_loss: 0.1137 - 25s/epoch - 4ms/step
Epoch 123/210
5969/5969 - 25s - loss: 0.0109 - val_loss: 0.0689 - 25s/epoch - 4ms/step
Epoch 124/210
5969/5969 - 24s - loss: 0.0109 - val_loss: 0.0983 - 24s/epoch - 4ms/step
Epoch 125/210
5969/5969 - 25s - loss: 0.0109 - val_loss: 0.0792 - 25s/epoch - 4ms/step
Epoch 126/210
5969/5969 - 24s - loss: 0.0109 - val_loss: 0.0672 - 24s/epoch - 4ms/step
Epoch 127/210
5969/5969 - 24s - loss: 0.0109 - val_loss: 0.1606 - 24s/epoch - 4ms/step
Epoch 128/210
5969/5969 - 24s - loss: 0.0109 - val_loss: 0.0636 - 24s/epoch - 4ms/step
Epoch 129/210
5969/5969 - 25s - loss: 0.0109 - val_loss: 0.0536 - 25s/epoch - 4ms/step
Epoch 130/210
5969/5969 - 24s - loss: 0.0109 - val_loss: 0.0780 - 24s/epoch - 4ms/step
Epoch 131/210
5969/5969 - 24s - loss: 0.0109 - val_loss: 0.1049 - 24s/epoch - 4ms/step
Epoch 132/210
5969/5969 - 24s - loss: 0.0109 - val_loss: 0.1351 - 24s/epoch - 4ms/step
Epoch 133/210
5969/5969 - 25s - loss: 0.0109 - val_loss: 0.0693 - 25s/epoch - 4ms/step
Epoch 134/210
5969/5969 - 25s - loss: 0.0109 - val_loss: 0.0745 - 25s/epoch - 4ms/step
Epoch 135/210
5969/5969 - 25s - loss: 0.0109 - val_loss: 0.0523 - 25s/epoch - 4ms/step
Epoch 136/210
5969/5969 - 25s - loss: 0.0109 - val_loss: 0.0897 - 25s/epoch - 4ms/step
Epoch 137/210
5969/5969 - 25s - loss: 0.0108 - val_loss: 0.0710 - 25s/epoch - 4ms/step
Epoch 138/210
5969/5969 - 24s - loss: 0.0108 - val_loss: 0.0728 - 24s/epoch - 4ms/step
Epoch 139/210
5969/5969 - 24s - loss: 0.0108 - val_loss: 0.0929 - 24s/epoch - 4ms/step
Epoch 140/210
5969/5969 - 24s - loss: 0.0108 - val_loss: 0.0962 - 24s/epoch - 4ms/step
Epoch 141/210
5969/5969 - 25s - loss: 0.0108 - val_loss: 0.0850 - 25s/epoch - 4ms/step
Epoch 142/210
5969/5969 - 25s - loss: 0.0108 - val_loss: 0.0835 - 25s/epoch - 4ms/step
Epoch 143/210
5969/5969 - 25s - loss: 0.0108 - val_loss: 0.0487 - 25s/epoch - 4ms/step
Epoch 144/210
5969/5969 - 26s - loss: 0.0108 - val_loss: 0.0844 - 26s/epoch - 4ms/step
Epoch 145/210
5969/5969 - 25s - loss: 0.0108 - val_loss: 0.0855 - 25s/epoch - 4ms/step
Epoch 146/210
5969/5969 - 25s - loss: 0.0108 - val_loss: 0.0614 - 25s/epoch - 4ms/step
Epoch 147/210
5969/5969 - 25s - loss: 0.0108 - val_loss: 0.0798 - 25s/epoch - 4ms/step
Epoch 148/210
5969/5969 - 25s - loss: 0.0108 - val_loss: 0.1272 - 25s/epoch - 4ms/step
Epoch 149/210
5969/5969 - 25s - loss: 0.0108 - val_loss: 0.0816 - 25s/epoch - 4ms/step
Epoch 150/210
5969/5969 - 25s - loss: 0.0108 - val_loss: 0.0725 - 25s/epoch - 4ms/step
Epoch 151/210
5969/5969 - 25s - loss: 0.0108 - val_loss: 0.1227 - 25s/epoch - 4ms/step
Epoch 152/210
5969/5969 - 25s - loss: 0.0108 - val_loss: 0.0537 - 25s/epoch - 4ms/step
Epoch 153/210
5969/5969 - 25s - loss: 0.0108 - val_loss: 0.0718 - 25s/epoch - 4ms/step
Epoch 154/210
5969/5969 - 25s - loss: 0.0108 - val_loss: 0.0675 - 25s/epoch - 4ms/step
Epoch 155/210
5969/5969 - 25s - loss: 0.0108 - val_loss: 0.0659 - 25s/epoch - 4ms/step
Epoch 156/210
5969/5969 - 25s - loss: 0.0108 - val_loss: 0.0755 - 25s/epoch - 4ms/step
Epoch 157/210
5969/5969 - 26s - loss: 0.0108 - val_loss: 0.1136 - 26s/epoch - 4ms/step
Epoch 158/210
5969/5969 - 25s - loss: 0.0108 - val_loss: 0.1033 - 25s/epoch - 4ms/step
Epoch 159/210
5969/5969 - 25s - loss: 0.0108 - val_loss: 0.0857 - 25s/epoch - 4ms/step
Epoch 160/210
5969/5969 - 25s - loss: 0.0108 - val_loss: 0.0784 - 25s/epoch - 4ms/step
Epoch 161/210
5969/5969 - 25s - loss: 0.0108 - val_loss: 0.0535 - 25s/epoch - 4ms/step
Epoch 162/210
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.0661 - 26s/epoch - 4ms/step
Epoch 163/210
5969/5969 - 25s - loss: 0.0108 - val_loss: 0.1115 - 25s/epoch - 4ms/step
Epoch 164/210
5969/5969 - 25s - loss: 0.0108 - val_loss: 0.0911 - 25s/epoch - 4ms/step
Epoch 165/210
5969/5969 - 25s - loss: 0.0107 - val_loss: 0.0747 - 25s/epoch - 4ms/step
Epoch 166/210
5969/5969 - 25s - loss: 0.0107 - val_loss: 0.0917 - 25s/epoch - 4ms/step
Epoch 167/210
5969/5969 - 25s - loss: 0.0107 - val_loss: 0.0487 - 25s/epoch - 4ms/step
Epoch 168/210
5969/5969 - 25s - loss: 0.0107 - val_loss: 0.0828 - 25s/epoch - 4ms/step
Epoch 169/210
5969/5969 - 25s - loss: 0.0107 - val_loss: 0.0863 - 25s/epoch - 4ms/step
Epoch 170/210
5969/5969 - 25s - loss: 0.0107 - val_loss: 0.1058 - 25s/epoch - 4ms/step
Epoch 171/210
5969/5969 - 25s - loss: 0.0107 - val_loss: 0.0752 - 25s/epoch - 4ms/step
Epoch 172/210
5969/5969 - 24s - loss: 0.0107 - val_loss: 0.0636 - 24s/epoch - 4ms/step
Epoch 173/210
5969/5969 - 25s - loss: 0.0107 - val_loss: 0.0675 - 25s/epoch - 4ms/step
Epoch 174/210
5969/5969 - 24s - loss: 0.0107 - val_loss: 0.0652 - 24s/epoch - 4ms/step
Epoch 175/210
5969/5969 - 25s - loss: 0.0107 - val_loss: 0.0761 - 25s/epoch - 4ms/step
Epoch 176/210
5969/5969 - 24s - loss: 0.0107 - val_loss: 0.0733 - 24s/epoch - 4ms/step
Epoch 177/210
5969/5969 - 25s - loss: 0.0107 - val_loss: 0.0500 - 25s/epoch - 4ms/step
Epoch 178/210
5969/5969 - 24s - loss: 0.0107 - val_loss: 0.1145 - 24s/epoch - 4ms/step
Epoch 179/210
5969/5969 - 25s - loss: 0.0107 - val_loss: 0.1133 - 25s/epoch - 4ms/step
Epoch 180/210
5969/5969 - 24s - loss: 0.0107 - val_loss: 0.0601 - 24s/epoch - 4ms/step
Epoch 181/210
5969/5969 - 24s - loss: 0.0107 - val_loss: 0.0813 - 24s/epoch - 4ms/step
Epoch 182/210
5969/5969 - 25s - loss: 0.0107 - val_loss: 0.0985 - 25s/epoch - 4ms/step
Epoch 183/210
5969/5969 - 25s - loss: 0.0107 - val_loss: 0.0981 - 25s/epoch - 4ms/step
Epoch 184/210
5969/5969 - 25s - loss: 0.0107 - val_loss: 0.0656 - 25s/epoch - 4ms/step
Epoch 185/210
5969/5969 - 24s - loss: 0.0107 - val_loss: 0.0849 - 24s/epoch - 4ms/step
Epoch 186/210
5969/5969 - 25s - loss: 0.0107 - val_loss: 0.0779 - 25s/epoch - 4ms/step
Epoch 187/210
5969/5969 - 24s - loss: 0.0107 - val_loss: 0.0842 - 24s/epoch - 4ms/step
Epoch 188/210
5969/5969 - 24s - loss: 0.0107 - val_loss: 0.0809 - 24s/epoch - 4ms/step
Epoch 189/210
5969/5969 - 24s - loss: 0.0107 - val_loss: 0.0862 - 24s/epoch - 4ms/step
Epoch 190/210
5969/5969 - 25s - loss: 0.0107 - val_loss: 0.0929 - 25s/epoch - 4ms/step
Epoch 191/210
5969/5969 - 25s - loss: 0.0107 - val_loss: 0.0866 - 25s/epoch - 4ms/step
Epoch 192/210
5969/5969 - 25s - loss: 0.0107 - val_loss: 0.0893 - 25s/epoch - 4ms/step
Epoch 193/210
5969/5969 - 25s - loss: 0.0107 - val_loss: 0.1037 - 25s/epoch - 4ms/step
Epoch 194/210
5969/5969 - 25s - loss: 0.0107 - val_loss: 0.0952 - 25s/epoch - 4ms/step
Epoch 195/210
5969/5969 - 25s - loss: 0.0107 - val_loss: 0.1239 - 25s/epoch - 4ms/step
Epoch 196/210
5969/5969 - 25s - loss: 0.0106 - val_loss: 0.2281 - 25s/epoch - 4ms/step
Epoch 197/210
5969/5969 - 25s - loss: 0.0107 - val_loss: 0.0987 - 25s/epoch - 4ms/step
Epoch 198/210
5969/5969 - 25s - loss: 0.0106 - val_loss: 0.0755 - 25s/epoch - 4ms/step
Epoch 199/210
5969/5969 - 25s - loss: 0.0106 - val_loss: 0.0764 - 25s/epoch - 4ms/step
Epoch 200/210
5969/5969 - 25s - loss: 0.0106 - val_loss: 0.0714 - 25s/epoch - 4ms/step
Epoch 201/210
5969/5969 - 25s - loss: 0.0106 - val_loss: 0.1343 - 25s/epoch - 4ms/step
Epoch 202/210
5969/5969 - 25s - loss: 0.0107 - val_loss: 0.1239 - 25s/epoch - 4ms/step
Epoch 203/210
5969/5969 - 25s - loss: 0.0107 - val_loss: 0.0840 - 25s/epoch - 4ms/step
Epoch 204/210
5969/5969 - 25s - loss: 0.0106 - val_loss: 0.1417 - 25s/epoch - 4ms/step
Epoch 205/210
5969/5969 - 25s - loss: 0.0106 - val_loss: 0.0497 - 25s/epoch - 4ms/step
Epoch 206/210
5969/5969 - 26s - loss: 0.0106 - val_loss: 0.0725 - 26s/epoch - 4ms/step
Epoch 207/210
5969/5969 - 25s - loss: 0.0106 - val_loss: 0.0615 - 25s/epoch - 4ms/step
Epoch 208/210
5969/5969 - 25s - loss: 0.0106 - val_loss: 0.0859 - 25s/epoch - 4ms/step
Epoch 209/210
5969/5969 - 26s - loss: 0.0106 - val_loss: 0.0722 - 26s/epoch - 4ms/step
Epoch 210/210
5969/5969 - 26s - loss: 0.0106 - val_loss: 0.0826 - 26s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0826180949807167
  1/332 [..............................] - ETA: 1:09 37/332 [==>...........................] - ETA: 0s   74/332 [=====>........................] - ETA: 0s111/332 [=========>....................] - ETA: 0s147/332 [============>.................] - ETA: 0s184/332 [===============>..............] - ETA: 0s222/332 [===================>..........] - ETA: 0s259/332 [======================>.......] - ETA: 0s295/332 [=========================>....] - ETA: 0s331/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.09146081684582016
cosine 0.07093859105021649
MAE: 0.04671164
RMSE: 0.28023595
r2: -4.09007921858384
RMSE zero-vector: 0.2430644284356365
['0.5custom_VAE', 'mse', 16, 210, 0.0005, 0.6, 758, 0.010636074468493462, 0.0826180949807167, 0.09146081684582016, 0.07093859105021649, 0.04671163856983185, 0.2802359461784363, -4.09007921858384, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 90 0.0005 16 1] 1
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Epoch 1/90
5969/5969 - 27s - loss: 0.0212 - val_loss: 0.0130 - 27s/epoch - 5ms/step
Epoch 2/90
5969/5969 - 26s - loss: 0.0131 - val_loss: 0.0128 - 26s/epoch - 4ms/step
Epoch 3/90
5969/5969 - 25s - loss: 0.0129 - val_loss: 0.0131 - 25s/epoch - 4ms/step
Epoch 4/90
5969/5969 - 26s - loss: 0.0127 - val_loss: 0.0148 - 26s/epoch - 4ms/step
Epoch 5/90
5969/5969 - 25s - loss: 0.0127 - val_loss: 0.0162 - 25s/epoch - 4ms/step
Epoch 6/90
5969/5969 - 25s - loss: 0.0126 - val_loss: 0.0197 - 25s/epoch - 4ms/step
Epoch 7/90
5969/5969 - 26s - loss: 0.0126 - val_loss: 0.0175 - 26s/epoch - 4ms/step
Epoch 8/90
5969/5969 - 25s - loss: 0.0126 - val_loss: 0.0227 - 25s/epoch - 4ms/step
Epoch 9/90
5969/5969 - 26s - loss: 0.0125 - val_loss: 0.0220 - 26s/epoch - 4ms/step
Epoch 10/90
5969/5969 - 26s - loss: 0.0125 - val_loss: 0.0233 - 26s/epoch - 4ms/step
Epoch 11/90
5969/5969 - 26s - loss: 0.0125 - val_loss: 0.0270 - 26s/epoch - 4ms/step
Epoch 12/90
5969/5969 - 25s - loss: 0.0125 - val_loss: 0.0272 - 25s/epoch - 4ms/step
Epoch 13/90
5969/5969 - 26s - loss: 0.0123 - val_loss: 0.0298 - 26s/epoch - 4ms/step
Epoch 14/90
5969/5969 - 25s - loss: 0.0120 - val_loss: 0.0445 - 25s/epoch - 4ms/step
Epoch 15/90
5969/5969 - 25s - loss: 0.0119 - val_loss: 0.0272 - 25s/epoch - 4ms/step
Epoch 16/90
5969/5969 - 25s - loss: 0.0118 - val_loss: 0.0355 - 25s/epoch - 4ms/step
Epoch 17/90
5969/5969 - 25s - loss: 0.0118 - val_loss: 0.0313 - 25s/epoch - 4ms/step
Epoch 18/90
5969/5969 - 26s - loss: 0.0118 - val_loss: 0.0482 - 26s/epoch - 4ms/step
Epoch 19/90
5969/5969 - 26s - loss: 0.0117 - val_loss: 0.0667 - 26s/epoch - 4ms/step
Epoch 20/90
5969/5969 - 25s - loss: 0.0117 - val_loss: 0.0663 - 25s/epoch - 4ms/step
Epoch 21/90
5969/5969 - 25s - loss: 0.0117 - val_loss: 0.0494 - 25s/epoch - 4ms/step
Epoch 22/90
5969/5969 - 25s - loss: 0.0117 - val_loss: 0.0627 - 25s/epoch - 4ms/step
Epoch 23/90
5969/5969 - 24s - loss: 0.0117 - val_loss: 0.0702 - 24s/epoch - 4ms/step
Epoch 24/90
5969/5969 - 24s - loss: 0.0117 - val_loss: 0.0569 - 24s/epoch - 4ms/step
Epoch 25/90
5969/5969 - 25s - loss: 0.0117 - val_loss: 0.0594 - 25s/epoch - 4ms/step
Epoch 26/90
5969/5969 - 25s - loss: 0.0116 - val_loss: 0.0658 - 25s/epoch - 4ms/step
Epoch 27/90
5969/5969 - 24s - loss: 0.0116 - val_loss: 0.0601 - 24s/epoch - 4ms/step
Epoch 28/90
