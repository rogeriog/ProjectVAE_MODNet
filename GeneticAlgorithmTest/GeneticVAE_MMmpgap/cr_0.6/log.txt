start
Wed Feb 22 21:42:14 CET 2023
2023-02-22 21:42:15.359654: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-22 21:42:15.541616: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-02-22 21:42:56,484 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7f0d2b6f21f0> object, created with modnet version 0.1.12
NAN values: 12054
NAN values remaining: 0
        AtomicOrbitals|HOMO_character  ...  BondFractions|B - B bond frac.
id                                     ...                                
0                                 3.0  ...                             0.0
1                                 3.0  ...                             0.0
2                                 2.0  ...                             0.0
3                                 2.0  ...                             0.0
4                                 2.0  ...                             0.0
...                               ...  ...                             ...
106108                            3.0  ...                             0.0
106109                            2.0  ...                             0.0
106110                            3.0  ...                             0.0
106111                            3.0  ...                             0.0
106112                            1.0  ...                             0.0

[106113 rows x 1336 columns]
/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py:486: UserWarning: The percentage of genes to mutate (mutation_percent_genes=10) resutled in selecting (0) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).
If you do not want to mutate any gene, please set mutation_type=None.
  if not self.suppress_warnings: warnings.warn("The percentage of genes to mutate (mutation_percent_genes={mutation_percent}) resutled in selecting ({mutation_num}) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).\nIf you do not want to mutate any gene, please set mutation_type=None.".format(mutation_percent=mutation_percent_genes, mutation_num=mutation_num_genes))
/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py:764: UserWarning: Starting from PyGAD 2.6.0, the callback_generation parameter is deprecated and will be removed in a later release of PyGAD. Please use the on_generation parameter instead.
  if not self.suppress_warnings: warnings.warn("Starting from PyGAD 2.6.0, the callback_generation parameter is deprecated and will be removed in a later release of PyGAD. Please use the on_generation parameter instead.")
/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py:820: UserWarning: Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.")
/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py:828: UserWarning: Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.")
[[1.5 210 0.0005 16 1]
 [2.0 210 0.0005 64 1]
 [1.0 210 0.002 16 2]
 [1.0 120 0.002 256 2]
 [0.5 150 0.002 128 2]
 [2.0 150 0.002 256 1]
 [0.5 210 0.001 32 2]
 [1.0 150 0.0005 16 2]
 [0.5 60 0.002 64 2]
 [0.5 210 0.001 32 1]
 [1.5 30 0.001 256 2]
 [2.0 60 0.001 16 1]
 [1.5 150 0.001 64 2]
 [1.0 90 0.001 64 2]
 [1.5 150 0.0005 64 1]]
[1.5 210 0.0005 16 1] 0
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
2023-02-22 21:43:00.865645: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-22 21:43:01.361716: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2023-02-22 21:43:01.362573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22295 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:23:00.0, compute capability: 8.6
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Epoch 1/210
2023-02-22 21:43:06.765661: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
5969/5969 - 27s - loss: 0.0212 - val_loss: 0.0130 - 27s/epoch - 5ms/step
Epoch 2/210
5969/5969 - 26s - loss: 0.0131 - val_loss: 0.0127 - 26s/epoch - 4ms/step
Epoch 3/210
5969/5969 - 26s - loss: 0.0129 - val_loss: 0.0130 - 26s/epoch - 4ms/step
Epoch 4/210
5969/5969 - 26s - loss: 0.0127 - val_loss: 0.0144 - 26s/epoch - 4ms/step
Epoch 5/210
5969/5969 - 26s - loss: 0.0127 - val_loss: 0.0199 - 26s/epoch - 4ms/step
Epoch 6/210
5969/5969 - 26s - loss: 0.0126 - val_loss: 0.0157 - 26s/epoch - 4ms/step
Epoch 7/210
5969/5969 - 26s - loss: 0.0126 - val_loss: 0.0169 - 26s/epoch - 4ms/step
Epoch 8/210
5969/5969 - 25s - loss: 0.0126 - val_loss: 0.0169 - 25s/epoch - 4ms/step
Epoch 9/210
5969/5969 - 25s - loss: 0.0126 - val_loss: 0.0196 - 25s/epoch - 4ms/step
Epoch 10/210
5969/5969 - 25s - loss: 0.0126 - val_loss: 0.0204 - 25s/epoch - 4ms/step
Epoch 11/210
5969/5969 - 25s - loss: 0.0125 - val_loss: 0.0348 - 25s/epoch - 4ms/step
Epoch 12/210
5969/5969 - 26s - loss: 0.0125 - val_loss: 0.0267 - 26s/epoch - 4ms/step
Epoch 13/210
5969/5969 - 25s - loss: 0.0125 - val_loss: 0.0260 - 25s/epoch - 4ms/step
Epoch 14/210
5969/5969 - 25s - loss: 0.0124 - val_loss: 0.0353 - 25s/epoch - 4ms/step
Epoch 15/210
5969/5969 - 25s - loss: 0.0122 - val_loss: 0.0314 - 25s/epoch - 4ms/step
Epoch 16/210
5969/5969 - 25s - loss: 0.0120 - val_loss: 0.0314 - 25s/epoch - 4ms/step
Epoch 17/210
5969/5969 - 25s - loss: 0.0119 - val_loss: 0.0371 - 25s/epoch - 4ms/step
Epoch 18/210
5969/5969 - 25s - loss: 0.0118 - val_loss: 0.0399 - 25s/epoch - 4ms/step
Epoch 19/210
5969/5969 - 24s - loss: 0.0118 - val_loss: 0.0536 - 24s/epoch - 4ms/step
Epoch 20/210
5969/5969 - 25s - loss: 0.0117 - val_loss: 0.0625 - 25s/epoch - 4ms/step
Epoch 21/210
5969/5969 - 25s - loss: 0.0117 - val_loss: 0.0451 - 25s/epoch - 4ms/step
Epoch 22/210
5969/5969 - 24s - loss: 0.0117 - val_loss: 0.0489 - 24s/epoch - 4ms/step
Epoch 23/210
5969/5969 - 25s - loss: 0.0117 - val_loss: 0.0390 - 25s/epoch - 4ms/step
Epoch 24/210
5969/5969 - 25s - loss: 0.0117 - val_loss: 0.0473 - 25s/epoch - 4ms/step
Epoch 25/210
5969/5969 - 25s - loss: 0.0117 - val_loss: 0.0517 - 25s/epoch - 4ms/step
Epoch 26/210
5969/5969 - 25s - loss: 0.0116 - val_loss: 0.0484 - 25s/epoch - 4ms/step
Epoch 27/210
5969/5969 - 26s - loss: 0.0116 - val_loss: 0.0563 - 26s/epoch - 4ms/step
Epoch 28/210
5969/5969 - 25s - loss: 0.0116 - val_loss: 0.0371 - 25s/epoch - 4ms/step
Epoch 29/210
5969/5969 - 25s - loss: 0.0116 - val_loss: 0.0471 - 25s/epoch - 4ms/step
Epoch 30/210
5969/5969 - 25s - loss: 0.0116 - val_loss: 0.0977 - 25s/epoch - 4ms/step
Epoch 31/210
5969/5969 - 25s - loss: 0.0116 - val_loss: 0.0456 - 25s/epoch - 4ms/step
Epoch 32/210
5969/5969 - 25s - loss: 0.0116 - val_loss: 0.0640 - 25s/epoch - 4ms/step
Epoch 33/210
5969/5969 - 25s - loss: 0.0116 - val_loss: 0.0444 - 25s/epoch - 4ms/step
Epoch 34/210
5969/5969 - 25s - loss: 0.0116 - val_loss: 0.0662 - 25s/epoch - 4ms/step
Epoch 35/210
5969/5969 - 25s - loss: 0.0116 - val_loss: 0.0481 - 25s/epoch - 4ms/step
Epoch 36/210
5969/5969 - 25s - loss: 0.0116 - val_loss: 0.0548 - 25s/epoch - 4ms/step
Epoch 37/210
5969/5969 - 26s - loss: 0.0116 - val_loss: 0.0503 - 26s/epoch - 4ms/step
Epoch 38/210
5969/5969 - 25s - loss: 0.0116 - val_loss: 0.0469 - 25s/epoch - 4ms/step
Epoch 39/210
5969/5969 - 26s - loss: 0.0116 - val_loss: 0.0418 - 26s/epoch - 4ms/step
Epoch 40/210
5969/5969 - 25s - loss: 0.0115 - val_loss: 0.0423 - 25s/epoch - 4ms/step
Epoch 41/210
5969/5969 - 25s - loss: 0.0115 - val_loss: 0.0597 - 25s/epoch - 4ms/step
Epoch 42/210
5969/5969 - 25s - loss: 0.0115 - val_loss: 0.0661 - 25s/epoch - 4ms/step
Epoch 43/210
5969/5969 - 25s - loss: 0.0115 - val_loss: 0.0592 - 25s/epoch - 4ms/step
Epoch 44/210
5969/5969 - 25s - loss: 0.0115 - val_loss: 0.0552 - 25s/epoch - 4ms/step
Epoch 45/210
5969/5969 - 25s - loss: 0.0115 - val_loss: 0.0456 - 25s/epoch - 4ms/step
Epoch 46/210
5969/5969 - 25s - loss: 0.0115 - val_loss: 0.0840 - 25s/epoch - 4ms/step
Epoch 47/210
5969/5969 - 25s - loss: 0.0115 - val_loss: 0.0697 - 25s/epoch - 4ms/step
Epoch 48/210
5969/5969 - 25s - loss: 0.0115 - val_loss: 0.0544 - 25s/epoch - 4ms/step
Epoch 49/210
5969/5969 - 24s - loss: 0.0115 - val_loss: 0.0764 - 24s/epoch - 4ms/step
Epoch 50/210
5969/5969 - 25s - loss: 0.0115 - val_loss: 0.0510 - 25s/epoch - 4ms/step
Epoch 51/210
5969/5969 - 25s - loss: 0.0115 - val_loss: 0.0463 - 25s/epoch - 4ms/step
Epoch 52/210
5969/5969 - 29s - loss: 0.0115 - val_loss: 0.0677 - 29s/epoch - 5ms/step
Epoch 53/210
5969/5969 - 29s - loss: 0.0115 - val_loss: 0.0946 - 29s/epoch - 5ms/step
Epoch 54/210
5969/5969 - 29s - loss: 0.0115 - val_loss: 0.0715 - 29s/epoch - 5ms/step
Epoch 55/210
5969/5969 - 29s - loss: 0.0115 - val_loss: 0.0555 - 29s/epoch - 5ms/step
Epoch 56/210
5969/5969 - 29s - loss: 0.0115 - val_loss: 0.0560 - 29s/epoch - 5ms/step
Epoch 57/210
5969/5969 - 30s - loss: 0.0115 - val_loss: 0.0702 - 30s/epoch - 5ms/step
Epoch 58/210
5969/5969 - 31s - loss: 0.0114 - val_loss: 0.0869 - 31s/epoch - 5ms/step
Epoch 59/210
5969/5969 - 30s - loss: 0.0114 - val_loss: 0.0764 - 30s/epoch - 5ms/step
Epoch 60/210
5969/5969 - 30s - loss: 0.0114 - val_loss: 0.0566 - 30s/epoch - 5ms/step
Epoch 61/210
5969/5969 - 30s - loss: 0.0114 - val_loss: 0.0754 - 30s/epoch - 5ms/step
Epoch 62/210
5969/5969 - 30s - loss: 0.0113 - val_loss: 0.1005 - 30s/epoch - 5ms/step
Epoch 63/210
5969/5969 - 30s - loss: 0.0113 - val_loss: 0.1275 - 30s/epoch - 5ms/step
Epoch 64/210
5969/5969 - 30s - loss: 0.0113 - val_loss: 0.1340 - 30s/epoch - 5ms/step
Epoch 65/210
5969/5969 - 30s - loss: 0.0113 - val_loss: 0.0982 - 30s/epoch - 5ms/step
Epoch 66/210
5969/5969 - 30s - loss: 0.0112 - val_loss: 0.1133 - 30s/epoch - 5ms/step
Epoch 67/210
5969/5969 - 30s - loss: 0.0112 - val_loss: 0.1255 - 30s/epoch - 5ms/step
Epoch 68/210
5969/5969 - 30s - loss: 0.0112 - val_loss: 0.1279 - 30s/epoch - 5ms/step
Epoch 69/210
5969/5969 - 30s - loss: 0.0112 - val_loss: 0.0816 - 30s/epoch - 5ms/step
Epoch 70/210
5969/5969 - 30s - loss: 0.0112 - val_loss: 0.1560 - 30s/epoch - 5ms/step
Epoch 71/210
5969/5969 - 30s - loss: 0.0112 - val_loss: 0.1100 - 30s/epoch - 5ms/step
Epoch 72/210
5969/5969 - 30s - loss: 0.0112 - val_loss: 0.1373 - 30s/epoch - 5ms/step
Epoch 73/210
5969/5969 - 30s - loss: 0.0111 - val_loss: 0.1734 - 30s/epoch - 5ms/step
Epoch 74/210
5969/5969 - 31s - loss: 0.0111 - val_loss: 0.1248 - 31s/epoch - 5ms/step
Epoch 75/210
5969/5969 - 27s - loss: 0.0111 - val_loss: 0.1642 - 27s/epoch - 4ms/step
Epoch 76/210
5969/5969 - 26s - loss: 0.0111 - val_loss: 0.1548 - 26s/epoch - 4ms/step
Epoch 77/210
5969/5969 - 26s - loss: 0.0111 - val_loss: 0.1170 - 26s/epoch - 4ms/step
Epoch 78/210
5969/5969 - 26s - loss: 0.0111 - val_loss: 0.1544 - 26s/epoch - 4ms/step
Epoch 79/210
5969/5969 - 26s - loss: 0.0111 - val_loss: 0.1501 - 26s/epoch - 4ms/step
Epoch 80/210
5969/5969 - 26s - loss: 0.0111 - val_loss: 0.1037 - 26s/epoch - 4ms/step
Epoch 81/210
5969/5969 - 27s - loss: 0.0110 - val_loss: 0.1238 - 27s/epoch - 4ms/step
Epoch 82/210
5969/5969 - 26s - loss: 0.0110 - val_loss: 0.1614 - 26s/epoch - 4ms/step
Epoch 83/210
5969/5969 - 26s - loss: 0.0110 - val_loss: 0.1845 - 26s/epoch - 4ms/step
Epoch 84/210
5969/5969 - 27s - loss: 0.0110 - val_loss: 0.1353 - 27s/epoch - 4ms/step
Epoch 85/210
5969/5969 - 30s - loss: 0.0110 - val_loss: 0.1645 - 30s/epoch - 5ms/step
Epoch 86/210
5969/5969 - 30s - loss: 0.0110 - val_loss: 0.1501 - 30s/epoch - 5ms/step
Epoch 87/210
5969/5969 - 28s - loss: 0.0110 - val_loss: 0.1609 - 28s/epoch - 5ms/step
Epoch 88/210
5969/5969 - 28s - loss: 0.0110 - val_loss: 0.0889 - 28s/epoch - 5ms/step
Epoch 89/210
5969/5969 - 30s - loss: 0.0110 - val_loss: 0.1252 - 30s/epoch - 5ms/step
Epoch 90/210
5969/5969 - 29s - loss: 0.0109 - val_loss: 0.1961 - 29s/epoch - 5ms/step
Epoch 91/210
5969/5969 - 29s - loss: 0.0110 - val_loss: 0.1214 - 29s/epoch - 5ms/step
Epoch 92/210
5969/5969 - 30s - loss: 0.0109 - val_loss: 0.1717 - 30s/epoch - 5ms/step
Epoch 93/210
5969/5969 - 30s - loss: 0.0109 - val_loss: 0.1895 - 30s/epoch - 5ms/step
Epoch 94/210
5969/5969 - 29s - loss: 0.0109 - val_loss: 0.1797 - 29s/epoch - 5ms/step
Epoch 95/210
5969/5969 - 29s - loss: 0.0109 - val_loss: 0.1315 - 29s/epoch - 5ms/step
Epoch 96/210
5969/5969 - 30s - loss: 0.0109 - val_loss: 0.1331 - 30s/epoch - 5ms/step
Epoch 97/210
5969/5969 - 29s - loss: 0.0109 - val_loss: 0.1171 - 29s/epoch - 5ms/step
Epoch 98/210
5969/5969 - 30s - loss: 0.0109 - val_loss: 0.1213 - 30s/epoch - 5ms/step
Epoch 99/210
5969/5969 - 30s - loss: 0.0109 - val_loss: 0.1231 - 30s/epoch - 5ms/step
Epoch 100/210
5969/5969 - 30s - loss: 0.0109 - val_loss: 0.1021 - 30s/epoch - 5ms/step
Epoch 101/210
5969/5969 - 30s - loss: 0.0109 - val_loss: 0.0967 - 30s/epoch - 5ms/step
Epoch 102/210
5969/5969 - 30s - loss: 0.0109 - val_loss: 0.1131 - 30s/epoch - 5ms/step
Epoch 103/210
5969/5969 - 30s - loss: 0.0109 - val_loss: 0.0915 - 30s/epoch - 5ms/step
Epoch 104/210
5969/5969 - 29s - loss: 0.0109 - val_loss: 0.1082 - 29s/epoch - 5ms/step
Epoch 105/210
5969/5969 - 29s - loss: 0.0109 - val_loss: 0.1178 - 29s/epoch - 5ms/step
Epoch 106/210
5969/5969 - 30s - loss: 0.0109 - val_loss: 0.1140 - 30s/epoch - 5ms/step
Epoch 107/210
5969/5969 - 30s - loss: 0.0109 - val_loss: 0.1093 - 30s/epoch - 5ms/step
Epoch 108/210
5969/5969 - 27s - loss: 0.0109 - val_loss: 0.1579 - 27s/epoch - 5ms/step
Epoch 109/210
5969/5969 - 29s - loss: 0.0109 - val_loss: 0.1233 - 29s/epoch - 5ms/step
Epoch 110/210
5969/5969 - 30s - loss: 0.0109 - val_loss: 0.1531 - 30s/epoch - 5ms/step
Epoch 111/210
5969/5969 - 29s - loss: 0.0109 - val_loss: 0.0970 - 29s/epoch - 5ms/step
Epoch 112/210
5969/5969 - 30s - loss: 0.0109 - val_loss: 0.1087 - 30s/epoch - 5ms/step
Epoch 113/210
5969/5969 - 30s - loss: 0.0109 - val_loss: 0.1263 - 30s/epoch - 5ms/step
Epoch 114/210
5969/5969 - 30s - loss: 0.0109 - val_loss: 0.0803 - 30s/epoch - 5ms/step
Epoch 115/210
5969/5969 - 29s - loss: 0.0109 - val_loss: 0.1416 - 29s/epoch - 5ms/step
Epoch 116/210
5969/5969 - 30s - loss: 0.0109 - val_loss: 0.1616 - 30s/epoch - 5ms/step
Epoch 117/210
5969/5969 - 30s - loss: 0.0109 - val_loss: 0.1500 - 30s/epoch - 5ms/step
Epoch 118/210
5969/5969 - 30s - loss: 0.0109 - val_loss: 0.1256 - 30s/epoch - 5ms/step
Epoch 119/210
5969/5969 - 30s - loss: 0.0109 - val_loss: 0.0829 - 30s/epoch - 5ms/step
Epoch 120/210
5969/5969 - 30s - loss: 0.0109 - val_loss: 0.1391 - 30s/epoch - 5ms/step
Epoch 121/210
5969/5969 - 29s - loss: 0.0108 - val_loss: 0.1106 - 29s/epoch - 5ms/step
Epoch 122/210
5969/5969 - 30s - loss: 0.0109 - val_loss: 0.1585 - 30s/epoch - 5ms/step
Epoch 123/210
5969/5969 - 29s - loss: 0.0108 - val_loss: 0.0680 - 29s/epoch - 5ms/step
Epoch 124/210
5969/5969 - 29s - loss: 0.0108 - val_loss: 0.0831 - 29s/epoch - 5ms/step
Epoch 125/210
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.1640 - 30s/epoch - 5ms/step
Epoch 126/210
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.1671 - 30s/epoch - 5ms/step
Epoch 127/210
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.1012 - 30s/epoch - 5ms/step
Epoch 128/210
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.0985 - 30s/epoch - 5ms/step
Epoch 129/210
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.1297 - 30s/epoch - 5ms/step
Epoch 130/210
5969/5969 - 29s - loss: 0.0108 - val_loss: 0.1510 - 29s/epoch - 5ms/step
Epoch 131/210
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.1337 - 30s/epoch - 5ms/step
Epoch 132/210
5969/5969 - 29s - loss: 0.0108 - val_loss: 0.1046 - 29s/epoch - 5ms/step
Epoch 133/210
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.0835 - 27s/epoch - 4ms/step
Epoch 134/210
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.1044 - 30s/epoch - 5ms/step
Epoch 135/210
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.1264 - 30s/epoch - 5ms/step
Epoch 136/210
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.1965 - 30s/epoch - 5ms/step
Epoch 137/210
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.1264 - 30s/epoch - 5ms/step
Epoch 138/210
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.1060 - 30s/epoch - 5ms/step
Epoch 139/210
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.1144 - 30s/epoch - 5ms/step
Epoch 140/210
5969/5969 - 31s - loss: 0.0108 - val_loss: 0.0935 - 31s/epoch - 5ms/step
Epoch 141/210
5969/5969 - 31s - loss: 0.0108 - val_loss: 0.0824 - 31s/epoch - 5ms/step
Epoch 142/210
5969/5969 - 31s - loss: 0.0108 - val_loss: 0.1023 - 31s/epoch - 5ms/step
Epoch 143/210
5969/5969 - 31s - loss: 0.0108 - val_loss: 0.1288 - 31s/epoch - 5ms/step
Epoch 144/210
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.1203 - 30s/epoch - 5ms/step
Epoch 145/210
5969/5969 - 31s - loss: 0.0108 - val_loss: 0.1403 - 31s/epoch - 5ms/step
Epoch 146/210
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.1079 - 30s/epoch - 5ms/step
Epoch 147/210
5969/5969 - 31s - loss: 0.0108 - val_loss: 0.1117 - 31s/epoch - 5ms/step
Epoch 148/210
5969/5969 - 31s - loss: 0.0108 - val_loss: 0.1402 - 31s/epoch - 5ms/step
Epoch 149/210
5969/5969 - 31s - loss: 0.0108 - val_loss: 0.1006 - 31s/epoch - 5ms/step
Epoch 150/210
5969/5969 - 31s - loss: 0.0108 - val_loss: 0.1009 - 31s/epoch - 5ms/step
Epoch 151/210
5969/5969 - 31s - loss: 0.0108 - val_loss: 0.1483 - 31s/epoch - 5ms/step
Epoch 152/210
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.1594 - 30s/epoch - 5ms/step
Epoch 153/210
5969/5969 - 29s - loss: 0.0108 - val_loss: 0.1474 - 29s/epoch - 5ms/step
Epoch 154/210
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.1189 - 30s/epoch - 5ms/step
Epoch 155/210
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.1468 - 30s/epoch - 5ms/step
Epoch 156/210
5969/5969 - 27s - loss: 0.0108 - val_loss: 0.1398 - 27s/epoch - 5ms/step
Epoch 157/210
5969/5969 - 26s - loss: 0.0108 - val_loss: 0.0694 - 26s/epoch - 4ms/step
Epoch 158/210
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.1134 - 30s/epoch - 5ms/step
Epoch 159/210
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.1170 - 30s/epoch - 5ms/step
Epoch 160/210
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.1076 - 30s/epoch - 5ms/step
Epoch 161/210
5969/5969 - 29s - loss: 0.0108 - val_loss: 0.0817 - 29s/epoch - 5ms/step
Epoch 162/210
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.0769 - 30s/epoch - 5ms/step
Epoch 163/210
5969/5969 - 29s - loss: 0.0108 - val_loss: 0.0934 - 29s/epoch - 5ms/step
Epoch 164/210
5969/5969 - 30s - loss: 0.0108 - val_loss: 0.1586 - 30s/epoch - 5ms/step
Epoch 165/210
5969/5969 - 30s - loss: 0.0107 - val_loss: 0.1073 - 30s/epoch - 5ms/step
Epoch 166/210
5969/5969 - 29s - loss: 0.0107 - val_loss: 0.1213 - 29s/epoch - 5ms/step
Epoch 167/210
5969/5969 - 29s - loss: 0.0107 - val_loss: 0.1279 - 29s/epoch - 5ms/step
Epoch 168/210
5969/5969 - 30s - loss: 0.0107 - val_loss: 0.1260 - 30s/epoch - 5ms/step
Epoch 169/210
5969/5969 - 29s - loss: 0.0107 - val_loss: 0.1625 - 29s/epoch - 5ms/step
Epoch 170/210
5969/5969 - 29s - loss: 0.0107 - val_loss: 0.1587 - 29s/epoch - 5ms/step
Epoch 171/210
5969/5969 - 29s - loss: 0.0107 - val_loss: 0.1218 - 29s/epoch - 5ms/step
Epoch 172/210
5969/5969 - 29s - loss: 0.0107 - val_loss: 0.1127 - 29s/epoch - 5ms/step
Epoch 173/210
5969/5969 - 29s - loss: 0.0107 - val_loss: 0.1600 - 29s/epoch - 5ms/step
Epoch 174/210
5969/5969 - 29s - loss: 0.0107 - val_loss: 0.1143 - 29s/epoch - 5ms/step
Epoch 175/210
5969/5969 - 30s - loss: 0.0107 - val_loss: 0.1478 - 30s/epoch - 5ms/step
Epoch 176/210
5969/5969 - 30s - loss: 0.0107 - val_loss: 0.0736 - 30s/epoch - 5ms/step
Epoch 177/210
5969/5969 - 29s - loss: 0.0107 - val_loss: 0.0916 - 29s/epoch - 5ms/step
Epoch 178/210
5969/5969 - 30s - loss: 0.0107 - val_loss: 0.1207 - 30s/epoch - 5ms/step
Epoch 179/210
5969/5969 - 30s - loss: 0.0107 - val_loss: 0.1166 - 30s/epoch - 5ms/step
Epoch 180/210
5969/5969 - 29s - loss: 0.0107 - val_loss: 0.1322 - 29s/epoch - 5ms/step
Epoch 181/210
5969/5969 - 26s - loss: 0.0107 - val_loss: 0.1237 - 26s/epoch - 4ms/step
Epoch 182/210
5969/5969 - 28s - loss: 0.0107 - val_loss: 0.0843 - 28s/epoch - 5ms/step
Epoch 183/210
5969/5969 - 30s - loss: 0.0107 - val_loss: 0.1620 - 30s/epoch - 5ms/step
Epoch 184/210
5969/5969 - 29s - loss: 0.0107 - val_loss: 0.1192 - 29s/epoch - 5ms/step
Epoch 185/210
5969/5969 - 29s - loss: 0.0107 - val_loss: 0.0834 - 29s/epoch - 5ms/step
Epoch 186/210
5969/5969 - 29s - loss: 0.0107 - val_loss: 0.1491 - 29s/epoch - 5ms/step
Epoch 187/210
5969/5969 - 30s - loss: 0.0107 - val_loss: 0.1929 - 30s/epoch - 5ms/step
Epoch 188/210
5969/5969 - 29s - loss: 0.0107 - val_loss: 0.1276 - 29s/epoch - 5ms/step
Epoch 189/210
5969/5969 - 29s - loss: 0.0107 - val_loss: 0.1879 - 29s/epoch - 5ms/step
Epoch 190/210
5969/5969 - 30s - loss: 0.0107 - val_loss: 0.2075 - 30s/epoch - 5ms/step
Epoch 191/210
5969/5969 - 30s - loss: 0.0107 - val_loss: 0.1514 - 30s/epoch - 5ms/step
Epoch 192/210
5969/5969 - 29s - loss: 0.0107 - val_loss: 0.2002 - 29s/epoch - 5ms/step
Epoch 193/210
5969/5969 - 29s - loss: 0.0107 - val_loss: 0.1341 - 29s/epoch - 5ms/step
Epoch 194/210
5969/5969 - 29s - loss: 0.0107 - val_loss: 0.0819 - 29s/epoch - 5ms/step
Epoch 195/210
5969/5969 - 30s - loss: 0.0107 - val_loss: 0.1852 - 30s/epoch - 5ms/step
Epoch 196/210
5969/5969 - 29s - loss: 0.0107 - val_loss: 0.1318 - 29s/epoch - 5ms/step
Epoch 197/210
5969/5969 - 30s - loss: 0.0107 - val_loss: 0.1637 - 30s/epoch - 5ms/step
Epoch 198/210
5969/5969 - 29s - loss: 0.0107 - val_loss: 0.1048 - 29s/epoch - 5ms/step
Epoch 199/210
5969/5969 - 30s - loss: 0.0106 - val_loss: 0.1258 - 30s/epoch - 5ms/step
Epoch 200/210
5969/5969 - 29s - loss: 0.0107 - val_loss: 0.1073 - 29s/epoch - 5ms/step
Epoch 201/210
5969/5969 - 29s - loss: 0.0106 - val_loss: 0.1197 - 29s/epoch - 5ms/step
Epoch 202/210
5969/5969 - 29s - loss: 0.0107 - val_loss: 0.1727 - 29s/epoch - 5ms/step
Epoch 203/210
5969/5969 - 29s - loss: 0.0106 - val_loss: 0.1715 - 29s/epoch - 5ms/step
Epoch 204/210
5969/5969 - 29s - loss: 0.0106 - val_loss: 0.1639 - 29s/epoch - 5ms/step
Epoch 205/210
5969/5969 - 29s - loss: 0.0106 - val_loss: 0.1405 - 29s/epoch - 5ms/step
Epoch 206/210
5969/5969 - 25s - loss: 0.0106 - val_loss: 0.1659 - 25s/epoch - 4ms/step
Epoch 207/210
5969/5969 - 29s - loss: 0.0106 - val_loss: 0.1339 - 29s/epoch - 5ms/step
Epoch 208/210
5969/5969 - 29s - loss: 0.0106 - val_loss: 0.1107 - 29s/epoch - 5ms/step
Epoch 209/210
5969/5969 - 29s - loss: 0.0106 - val_loss: 0.1011 - 29s/epoch - 5ms/step
Epoch 210/210
5969/5969 - 29s - loss: 0.0106 - val_loss: 0.1027 - 29s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.10270411521196365
  1/332 [..............................] - ETA: 1:13 32/332 [=>............................] - ETA: 0s   63/332 [====>.........................] - ETA: 0s 92/332 [=======>......................] - ETA: 0s120/332 [=========>....................] - ETA: 0s150/332 [============>.................] - ETA: 0s182/332 [===============>..............] - ETA: 0s215/332 [==================>...........] - ETA: 0s249/332 [=====================>........] - ETA: 0s284/332 [========================>.....] - ETA: 0s317/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.09074283964704488
cosine 0.07038967118728759
MAE: 0.046964295
RMSE: 0.32822818
r2: -5.982752878332264
RMSE zero-vector: 0.2430644284356365
['1.5custom_VAE', 'mse', 16, 210, 0.0005, 0.6, 758, 0.010627076029777527, 0.10270411521196365, 0.09074283964704488, 0.07038967118728759, 0.04696429520845413, 0.3282281756401062, -5.982752878332264, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 210 0.0005 64 1] 1
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1916982     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1916982     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5703874     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 12,745,870
Trainable params: 12,734,242
Non-trainable params: 11,628
__________________________________________________________________________________________________
Epoch 1/210
1493/1493 - 10s - loss: 0.0290 - val_loss: 0.0221 - 10s/epoch - 7ms/step
Epoch 2/210
1493/1493 - 8s - loss: 0.0155 - val_loss: 0.0153 - 8s/epoch - 6ms/step
Epoch 3/210
1493/1493 - 8s - loss: 0.0136 - val_loss: 0.0131 - 8s/epoch - 6ms/step
Epoch 4/210
1493/1493 - 8s - loss: 0.0129 - val_loss: 0.0125 - 8s/epoch - 5ms/step
Epoch 5/210
1493/1493 - 9s - loss: 0.0126 - val_loss: 0.0123 - 9s/epoch - 6ms/step
Epoch 6/210
1493/1493 - 8s - loss: 0.0124 - val_loss: 0.0120 - 8s/epoch - 5ms/step
Epoch 7/210
1493/1493 - 8s - loss: 0.0120 - val_loss: 0.0116 - 8s/epoch - 5ms/step
Epoch 8/210
1493/1493 - 8s - loss: 0.0117 - val_loss: 0.0114 - 8s/epoch - 5ms/step
Epoch 9/210
1493/1493 - 9s - loss: 0.0115 - val_loss: 0.0113 - 9s/epoch - 6ms/step
Epoch 10/210
1493/1493 - 8s - loss: 0.0114 - val_loss: 0.0112 - 8s/epoch - 5ms/step
Epoch 11/210
1493/1493 - 8s - loss: 0.0114 - val_loss: 0.0111 - 8s/epoch - 5ms/step
Epoch 12/210
1493/1493 - 7s - loss: 0.0113 - val_loss: 0.0110 - 7s/epoch - 5ms/step
Epoch 13/210
1493/1493 - 8s - loss: 0.0113 - val_loss: 0.0111 - 8s/epoch - 6ms/step
Epoch 14/210
1493/1493 - 9s - loss: 0.0113 - val_loss: 0.0110 - 9s/epoch - 6ms/step
Epoch 15/210
1493/1493 - 8s - loss: 0.0112 - val_loss: 0.0110 - 8s/epoch - 5ms/step
Epoch 16/210
1493/1493 - 8s - loss: 0.0112 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 17/210
1493/1493 - 8s - loss: 0.0112 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 18/210
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 19/210
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 20/210
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 21/210
1493/1493 - 8s - loss: 0.0111 - val_loss: 0.0109 - 8s/epoch - 5ms/step
Epoch 22/210
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 23/210
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 24/210
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 25/210
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 26/210
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0108 - 8s/epoch - 5ms/step
Epoch 27/210
1493/1493 - 8s - loss: 0.0110 - val_loss: 0.0107 - 8s/epoch - 6ms/step
Epoch 28/210
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 29/210
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0107 - 8s/epoch - 5ms/step
Epoch 30/210
1493/1493 - 8s - loss: 0.0109 - val_loss: 0.0106 - 8s/epoch - 5ms/step
Epoch 31/210
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0106 - 8s/epoch - 5ms/step
Epoch 32/210
1493/1493 - 8s - loss: 0.0108 - val_loss: 0.0106 - 8s/epoch - 5ms/step
Epoch 33/210
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0105 - 8s/epoch - 5ms/step
Epoch 34/210
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0105 - 8s/epoch - 5ms/step
Epoch 35/210
1493/1493 - 7s - loss: 0.0107 - val_loss: 0.0105 - 7s/epoch - 5ms/step
Epoch 36/210
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 37/210
1493/1493 - 8s - loss: 0.0107 - val_loss: 0.0105 - 8s/epoch - 5ms/step
Epoch 38/210
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0105 - 8s/epoch - 5ms/step
Epoch 39/210
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0105 - 8s/epoch - 5ms/step
Epoch 40/210
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 41/210
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0105 - 8s/epoch - 5ms/step
Epoch 42/210
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0104 - 8s/epoch - 6ms/step
Epoch 43/210
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0104 - 8s/epoch - 6ms/step
Epoch 44/210
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 45/210
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 46/210
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 47/210
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 48/210
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 49/210
1493/1493 - 8s - loss: 0.0106 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 50/210
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0104 - 8s/epoch - 5ms/step
Epoch 51/210
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 52/210
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0104 - 8s/epoch - 6ms/step
Epoch 53/210
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0104 - 8s/epoch - 6ms/step
Epoch 54/210
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 55/210
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 56/210
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 57/210
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 58/210
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 59/210
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 60/210
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 61/210
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 62/210
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 63/210
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 64/210
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 65/210
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 66/210
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 6ms/step
Epoch 67/210
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 68/210
1493/1493 - 8s - loss: 0.0105 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 69/210
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 70/210
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 71/210
1493/1493 - 7s - loss: 0.0104 - val_loss: 0.0102 - 7s/epoch - 5ms/step
Epoch 72/210
1493/1493 - 7s - loss: 0.0104 - val_loss: 0.0103 - 7s/epoch - 4ms/step
Epoch 73/210
1493/1493 - 7s - loss: 0.0104 - val_loss: 0.0103 - 7s/epoch - 5ms/step
Epoch 74/210
1493/1493 - 7s - loss: 0.0104 - val_loss: 0.0103 - 7s/epoch - 5ms/step
Epoch 75/210
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 76/210
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0103 - 8s/epoch - 5ms/step
Epoch 77/210
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0102 - 8s/epoch - 6ms/step
Epoch 78/210
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 79/210
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 80/210
1493/1493 - 8s - loss: 0.0104 - val_loss: 0.0102 - 8s/epoch - 5ms/step
Epoch 81/210
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 82/210
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 83/210
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 84/210
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 85/210
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 86/210
1493/1493 - 8s - loss: 0.0103 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 87/210
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0101 - 8s/epoch - 5ms/step
Epoch 88/210
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 89/210
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 90/210
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 91/210
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 92/210
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 93/210
1493/1493 - 8s - loss: 0.0102 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 94/210
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 95/210
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 96/210
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0100 - 8s/epoch - 5ms/step
Epoch 97/210
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 98/210
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 99/210
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 100/210
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 101/210
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 102/210
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 103/210
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 104/210
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 105/210
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 106/210
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 107/210
1493/1493 - 7s - loss: 0.0101 - val_loss: 0.0099 - 7s/epoch - 5ms/step
Epoch 108/210
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 109/210
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 110/210
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 111/210
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 112/210
1493/1493 - 8s - loss: 0.0101 - val_loss: 0.0099 - 8s/epoch - 5ms/step
Epoch 113/210
1493/1493 - 7s - loss: 0.0100 - val_loss: 0.0098 - 7s/epoch - 5ms/step
Epoch 114/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 115/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 116/210
1493/1493 - 7s - loss: 0.0100 - val_loss: 0.0098 - 7s/epoch - 5ms/step
Epoch 117/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 118/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 119/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 120/210
1493/1493 - 7s - loss: 0.0100 - val_loss: 0.0098 - 7s/epoch - 5ms/step
Epoch 121/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 122/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 123/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 124/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 125/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 126/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 127/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 128/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 129/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 130/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 131/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 132/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 133/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 134/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 135/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 136/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 137/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 138/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 139/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 140/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 141/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 142/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 143/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 144/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 145/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 146/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 147/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 148/210
1493/1493 - 8s - loss: 0.0100 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 149/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0098 - 8s/epoch - 5ms/step
Epoch 150/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 151/210
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 5ms/step
Epoch 152/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 153/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 154/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 155/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 156/210
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 5ms/step
Epoch 157/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 158/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 159/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 160/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 161/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 162/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 163/210
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 5ms/step
Epoch 164/210
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 4ms/step
Epoch 165/210
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 4ms/step
Epoch 166/210
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 4ms/step
Epoch 167/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 168/210
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 4ms/step
Epoch 169/210
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 5ms/step
Epoch 170/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 171/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 172/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 173/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 174/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 175/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 176/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 177/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 178/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 179/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 180/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 181/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 182/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 183/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 184/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 185/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 186/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 187/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 188/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 189/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 190/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 191/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 192/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 193/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 194/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 195/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 196/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 197/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 198/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 199/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 200/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 201/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 202/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 203/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 204/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 205/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 206/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 207/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 208/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0097 - 8s/epoch - 5ms/step
Epoch 209/210
1493/1493 - 8s - loss: 0.0099 - val_loss: 0.0096 - 8s/epoch - 5ms/step
Epoch 210/210
1493/1493 - 8s - loss: 0.0098 - val_loss: 0.0097 - 8s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009650643914937973
  1/332 [..............................] - ETA: 56s 32/332 [=>............................] - ETA: 0s  60/332 [====>.........................] - ETA: 0s 93/332 [=======>......................] - ETA: 0s125/332 [==========>...................] - ETA: 0s158/332 [=============>................] - ETA: 0s192/332 [================>.............] - ETA: 0s226/332 [===================>..........] - ETA: 0s259/332 [======================>.......] - ETA: 0s292/332 [=========================>....] - ETA: 0s325/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.07598075101027793
cosine 0.058246110823978535
MAE: 0.037564598
RMSE: 0.08147823
r2: 0.5697165296237594
RMSE zero-vector: 0.2430644284356365
['2.0custom_VAE', 'mse', 64, 210, 0.0005, 0.6, 758, 0.009849246591329575, 0.009650643914937973, 0.07598075101027793, 0.058246110823978535, 0.03756459802389145, 0.08147823065519333, 0.5697165296237594, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.0 210 0.002 16 2] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          958870      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          958870      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3141746     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 6,663,502
Trainable params: 6,656,930
Non-trainable params: 6,572
__________________________________________________________________________________________________
Epoch 1/210
5969/5969 - 31s - loss: 0.0106 - val_loss: 0.0069 - 31s/epoch - 5ms/step
Epoch 2/210
5969/5969 - 31s - loss: 0.0070 - val_loss: 0.0069 - 31s/epoch - 5ms/step
Epoch 3/210
5969/5969 - 30s - loss: 0.0069 - val_loss: 0.0068 - 30s/epoch - 5ms/step
Epoch 4/210
5969/5969 - 30s - loss: 0.0069 - val_loss: 0.0069 - 30s/epoch - 5ms/step
Epoch 5/210
5969/5969 - 30s - loss: 0.0069 - val_loss: 0.0068 - 30s/epoch - 5ms/step
Epoch 6/210
5969/5969 - 31s - loss: 0.0069 - val_loss: 0.0068 - 31s/epoch - 5ms/step
Epoch 7/210
5969/5969 - 30s - loss: 0.0068 - val_loss: 0.0068 - 30s/epoch - 5ms/step
Epoch 8/210
5969/5969 - 30s - loss: 0.0068 - val_loss: 0.0068 - 30s/epoch - 5ms/step
Epoch 9/210
5969/5969 - 29s - loss: 0.0068 - val_loss: 0.0069 - 29s/epoch - 5ms/step
Epoch 10/210
5969/5969 - 29s - loss: 0.0068 - val_loss: 0.0069 - 29s/epoch - 5ms/step
Epoch 11/210
5969/5969 - 30s - loss: 0.0068 - val_loss: 0.0068 - 30s/epoch - 5ms/step
Epoch 12/210
5969/5969 - 29s - loss: 0.0068 - val_loss: 0.0069 - 29s/epoch - 5ms/step
Epoch 13/210
5969/5969 - 26s - loss: 0.0068 - val_loss: 0.0070 - 26s/epoch - 4ms/step
Epoch 14/210
5969/5969 - 29s - loss: 0.0067 - val_loss: 0.0069 - 29s/epoch - 5ms/step
Epoch 15/210
5969/5969 - 29s - loss: 0.0065 - val_loss: 0.0074 - 29s/epoch - 5ms/step
Epoch 16/210
5969/5969 - 29s - loss: 0.0065 - val_loss: 0.0069 - 29s/epoch - 5ms/step
Epoch 17/210
5969/5969 - 29s - loss: 0.0065 - val_loss: 0.0070 - 29s/epoch - 5ms/step
Epoch 18/210
5969/5969 - 29s - loss: 0.0065 - val_loss: 0.0077 - 29s/epoch - 5ms/step
Epoch 19/210
5969/5969 - 30s - loss: 0.0065 - val_loss: 0.0075 - 30s/epoch - 5ms/step
Epoch 20/210
5969/5969 - 29s - loss: 0.0064 - val_loss: 0.0070 - 29s/epoch - 5ms/step
Epoch 21/210
5969/5969 - 29s - loss: 0.0064 - val_loss: 0.0076 - 29s/epoch - 5ms/step
Epoch 22/210
5969/5969 - 29s - loss: 0.0064 - val_loss: 0.0086 - 29s/epoch - 5ms/step
Epoch 23/210
5969/5969 - 29s - loss: 0.0064 - val_loss: 0.0079 - 29s/epoch - 5ms/step
Epoch 24/210
5969/5969 - 29s - loss: 0.0064 - val_loss: 0.0073 - 29s/epoch - 5ms/step
Epoch 25/210
5969/5969 - 29s - loss: 0.0064 - val_loss: 0.0075 - 29s/epoch - 5ms/step
Epoch 26/210
5969/5969 - 29s - loss: 0.0064 - val_loss: 0.0069 - 29s/epoch - 5ms/step
Epoch 27/210
5969/5969 - 29s - loss: 0.0064 - val_loss: 0.0083 - 29s/epoch - 5ms/step
Epoch 28/210
5969/5969 - 29s - loss: 0.0064 - val_loss: 0.0077 - 29s/epoch - 5ms/step
Epoch 29/210
5969/5969 - 29s - loss: 0.0064 - val_loss: 0.0078 - 29s/epoch - 5ms/step
Epoch 30/210
5969/5969 - 30s - loss: 0.0064 - val_loss: 0.0084 - 30s/epoch - 5ms/step
Epoch 31/210
5969/5969 - 30s - loss: 0.0064 - val_loss: 0.0080 - 30s/epoch - 5ms/step
Epoch 32/210
5969/5969 - 29s - loss: 0.0064 - val_loss: 0.0075 - 29s/epoch - 5ms/step
Epoch 33/210
5969/5969 - 29s - loss: 0.0064 - val_loss: 0.0071 - 29s/epoch - 5ms/step
Epoch 34/210
5969/5969 - 29s - loss: 0.0064 - val_loss: 0.0071 - 29s/epoch - 5ms/step
Epoch 35/210
5969/5969 - 29s - loss: 0.0064 - val_loss: 0.0073 - 29s/epoch - 5ms/step
Epoch 36/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0081 - 29s/epoch - 5ms/step
Epoch 37/210
5969/5969 - 28s - loss: 0.0063 - val_loss: 0.0075 - 28s/epoch - 5ms/step
Epoch 38/210
5969/5969 - 27s - loss: 0.0063 - val_loss: 0.0081 - 27s/epoch - 4ms/step
Epoch 39/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0082 - 30s/epoch - 5ms/step
Epoch 40/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0074 - 30s/epoch - 5ms/step
Epoch 41/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0079 - 29s/epoch - 5ms/step
Epoch 42/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0080 - 29s/epoch - 5ms/step
Epoch 43/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0073 - 30s/epoch - 5ms/step
Epoch 44/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0076 - 30s/epoch - 5ms/step
Epoch 45/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0073 - 30s/epoch - 5ms/step
Epoch 46/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0075 - 30s/epoch - 5ms/step
Epoch 47/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0075 - 31s/epoch - 5ms/step
Epoch 48/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0076 - 31s/epoch - 5ms/step
Epoch 49/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0078 - 30s/epoch - 5ms/step
Epoch 50/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0074 - 30s/epoch - 5ms/step
Epoch 51/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0075 - 30s/epoch - 5ms/step
Epoch 52/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0074 - 30s/epoch - 5ms/step
Epoch 53/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0076 - 30s/epoch - 5ms/step
Epoch 54/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0073 - 30s/epoch - 5ms/step
Epoch 55/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0075 - 31s/epoch - 5ms/step
Epoch 56/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0069 - 30s/epoch - 5ms/step
Epoch 57/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0070 - 31s/epoch - 5ms/step
Epoch 58/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0076 - 30s/epoch - 5ms/step
Epoch 59/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0076 - 30s/epoch - 5ms/step
Epoch 60/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0072 - 30s/epoch - 5ms/step
Epoch 61/210
5969/5969 - 28s - loss: 0.0063 - val_loss: 0.0072 - 28s/epoch - 5ms/step
Epoch 62/210
5969/5969 - 27s - loss: 0.0063 - val_loss: 0.0079 - 27s/epoch - 5ms/step
Epoch 63/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0075 - 30s/epoch - 5ms/step
Epoch 64/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0071 - 31s/epoch - 5ms/step
Epoch 65/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0078 - 31s/epoch - 5ms/step
Epoch 66/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0074 - 30s/epoch - 5ms/step
Epoch 67/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0072 - 30s/epoch - 5ms/step
Epoch 68/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0076 - 30s/epoch - 5ms/step
Epoch 69/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0070 - 30s/epoch - 5ms/step
Epoch 70/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0074 - 30s/epoch - 5ms/step
Epoch 71/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0072 - 31s/epoch - 5ms/step
Epoch 72/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0070 - 31s/epoch - 5ms/step
Epoch 73/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0072 - 31s/epoch - 5ms/step
Epoch 74/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0072 - 30s/epoch - 5ms/step
Epoch 75/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0074 - 31s/epoch - 5ms/step
Epoch 76/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0070 - 31s/epoch - 5ms/step
Epoch 77/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0072 - 30s/epoch - 5ms/step
Epoch 78/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0077 - 29s/epoch - 5ms/step
Epoch 79/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0070 - 29s/epoch - 5ms/step
Epoch 80/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0073 - 29s/epoch - 5ms/step
Epoch 81/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0073 - 30s/epoch - 5ms/step
Epoch 82/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0073 - 29s/epoch - 5ms/step
Epoch 83/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0072 - 29s/epoch - 5ms/step
Epoch 84/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0070 - 29s/epoch - 5ms/step
Epoch 85/210
5969/5969 - 28s - loss: 0.0063 - val_loss: 0.0072 - 28s/epoch - 5ms/step
Epoch 86/210
5969/5969 - 27s - loss: 0.0063 - val_loss: 0.0071 - 27s/epoch - 4ms/step
Epoch 87/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0071 - 29s/epoch - 5ms/step
Epoch 88/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0071 - 30s/epoch - 5ms/step
Epoch 89/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0070 - 29s/epoch - 5ms/step
Epoch 90/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0078 - 29s/epoch - 5ms/step
Epoch 91/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0070 - 29s/epoch - 5ms/step
Epoch 92/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0075 - 29s/epoch - 5ms/step
Epoch 93/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0080 - 29s/epoch - 5ms/step
Epoch 94/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0071 - 29s/epoch - 5ms/step
Epoch 95/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0073 - 29s/epoch - 5ms/step
Epoch 96/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0074 - 29s/epoch - 5ms/step
Epoch 97/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0069 - 29s/epoch - 5ms/step
Epoch 98/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0078 - 29s/epoch - 5ms/step
Epoch 99/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0068 - 29s/epoch - 5ms/step
Epoch 100/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0072 - 29s/epoch - 5ms/step
Epoch 101/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0072 - 29s/epoch - 5ms/step
Epoch 102/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0073 - 29s/epoch - 5ms/step
Epoch 103/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0077 - 29s/epoch - 5ms/step
Epoch 104/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0069 - 30s/epoch - 5ms/step
Epoch 105/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0074 - 30s/epoch - 5ms/step
Epoch 106/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0073 - 30s/epoch - 5ms/step
Epoch 107/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0071 - 29s/epoch - 5ms/step
Epoch 108/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0069 - 29s/epoch - 5ms/step
Epoch 109/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0076 - 29s/epoch - 5ms/step
Epoch 110/210
5969/5969 - 27s - loss: 0.0063 - val_loss: 0.0074 - 27s/epoch - 4ms/step
Epoch 111/210
5969/5969 - 27s - loss: 0.0063 - val_loss: 0.0075 - 27s/epoch - 5ms/step
Epoch 112/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0070 - 29s/epoch - 5ms/step
Epoch 113/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0073 - 29s/epoch - 5ms/step
Epoch 114/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0074 - 29s/epoch - 5ms/step
Epoch 115/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0081 - 29s/epoch - 5ms/step
Epoch 116/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0072 - 29s/epoch - 5ms/step
Epoch 117/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0069 - 29s/epoch - 5ms/step
Epoch 118/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0070 - 29s/epoch - 5ms/step
Epoch 119/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0074 - 29s/epoch - 5ms/step
Epoch 120/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0074 - 29s/epoch - 5ms/step
Epoch 121/210
5969/5969 - 28s - loss: 0.0063 - val_loss: 0.0071 - 28s/epoch - 5ms/step
Epoch 122/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0071 - 29s/epoch - 5ms/step
Epoch 123/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0075 - 29s/epoch - 5ms/step
Epoch 124/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0076 - 29s/epoch - 5ms/step
Epoch 125/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0074 - 29s/epoch - 5ms/step
Epoch 126/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0072 - 29s/epoch - 5ms/step
Epoch 127/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0072 - 29s/epoch - 5ms/step
Epoch 128/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0074 - 30s/epoch - 5ms/step
Epoch 129/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0077 - 30s/epoch - 5ms/step
Epoch 130/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0075 - 30s/epoch - 5ms/step
Epoch 131/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0073 - 30s/epoch - 5ms/step
Epoch 132/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0076 - 30s/epoch - 5ms/step
Epoch 133/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0079 - 29s/epoch - 5ms/step
Epoch 134/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0071 - 29s/epoch - 5ms/step
Epoch 135/210
5969/5969 - 27s - loss: 0.0063 - val_loss: 0.0076 - 27s/epoch - 5ms/step
Epoch 136/210
5969/5969 - 27s - loss: 0.0063 - val_loss: 0.0075 - 27s/epoch - 4ms/step
Epoch 137/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0077 - 31s/epoch - 5ms/step
Epoch 138/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0074 - 31s/epoch - 5ms/step
Epoch 139/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0074 - 31s/epoch - 5ms/step
Epoch 140/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0078 - 31s/epoch - 5ms/step
Epoch 141/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0076 - 31s/epoch - 5ms/step
Epoch 142/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0079 - 31s/epoch - 5ms/step
Epoch 143/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0073 - 31s/epoch - 5ms/step
Epoch 144/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0073 - 30s/epoch - 5ms/step
Epoch 145/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0077 - 30s/epoch - 5ms/step
Epoch 146/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0076 - 31s/epoch - 5ms/step
Epoch 147/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0076 - 31s/epoch - 5ms/step
Epoch 148/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0088 - 30s/epoch - 5ms/step
Epoch 149/210
5969/5969 - 32s - loss: 0.0063 - val_loss: 0.0076 - 32s/epoch - 5ms/step
Epoch 150/210
5969/5969 - 33s - loss: 0.0063 - val_loss: 0.0076 - 33s/epoch - 6ms/step
Epoch 151/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0081 - 31s/epoch - 5ms/step
Epoch 152/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0082 - 31s/epoch - 5ms/step
Epoch 153/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0076 - 31s/epoch - 5ms/step
Epoch 154/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0076 - 31s/epoch - 5ms/step
Epoch 155/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0079 - 31s/epoch - 5ms/step
Epoch 156/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0078 - 30s/epoch - 5ms/step
Epoch 157/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0083 - 29s/epoch - 5ms/step
Epoch 158/210
5969/5969 - 25s - loss: 0.0062 - val_loss: 0.0079 - 25s/epoch - 4ms/step
Epoch 159/210
5969/5969 - 29s - loss: 0.0063 - val_loss: 0.0083 - 29s/epoch - 5ms/step
Epoch 160/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0078 - 31s/epoch - 5ms/step
Epoch 161/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0077 - 31s/epoch - 5ms/step
Epoch 162/210
5969/5969 - 32s - loss: 0.0063 - val_loss: 0.0086 - 32s/epoch - 5ms/step
Epoch 163/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0079 - 31s/epoch - 5ms/step
Epoch 164/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0077 - 30s/epoch - 5ms/step
Epoch 165/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0074 - 31s/epoch - 5ms/step
Epoch 166/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0080 - 31s/epoch - 5ms/step
Epoch 167/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0078 - 31s/epoch - 5ms/step
Epoch 168/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0083 - 31s/epoch - 5ms/step
Epoch 169/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0083 - 31s/epoch - 5ms/step
Epoch 170/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0080 - 31s/epoch - 5ms/step
Epoch 171/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0077 - 31s/epoch - 5ms/step
Epoch 172/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0078 - 31s/epoch - 5ms/step
Epoch 173/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0080 - 31s/epoch - 5ms/step
Epoch 174/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0080 - 31s/epoch - 5ms/step
Epoch 175/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0076 - 31s/epoch - 5ms/step
Epoch 176/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0083 - 31s/epoch - 5ms/step
Epoch 177/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0079 - 31s/epoch - 5ms/step
Epoch 178/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0074 - 31s/epoch - 5ms/step
Epoch 179/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0078 - 31s/epoch - 5ms/step
Epoch 180/210
5969/5969 - 26s - loss: 0.0063 - val_loss: 0.0087 - 26s/epoch - 4ms/step
Epoch 181/210
5969/5969 - 26s - loss: 0.0063 - val_loss: 0.0080 - 26s/epoch - 4ms/step
Epoch 182/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0087 - 31s/epoch - 5ms/step
Epoch 183/210
5969/5969 - 32s - loss: 0.0063 - val_loss: 0.0079 - 32s/epoch - 5ms/step
Epoch 184/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0080 - 31s/epoch - 5ms/step
Epoch 185/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0083 - 31s/epoch - 5ms/step
Epoch 186/210
5969/5969 - 32s - loss: 0.0063 - val_loss: 0.0084 - 32s/epoch - 5ms/step
Epoch 187/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0077 - 31s/epoch - 5ms/step
Epoch 188/210
5969/5969 - 32s - loss: 0.0063 - val_loss: 0.0082 - 32s/epoch - 5ms/step
Epoch 189/210
5969/5969 - 31s - loss: 0.0062 - val_loss: 0.0087 - 31s/epoch - 5ms/step
Epoch 190/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0092 - 31s/epoch - 5ms/step
Epoch 191/210
5969/5969 - 32s - loss: 0.0063 - val_loss: 0.0080 - 32s/epoch - 5ms/step
Epoch 192/210
5969/5969 - 30s - loss: 0.0063 - val_loss: 0.0089 - 30s/epoch - 5ms/step
Epoch 193/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0075 - 31s/epoch - 5ms/step
Epoch 194/210
5969/5969 - 32s - loss: 0.0062 - val_loss: 0.0088 - 32s/epoch - 5ms/step
Epoch 195/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0081 - 31s/epoch - 5ms/step
Epoch 196/210
5969/5969 - 31s - loss: 0.0062 - val_loss: 0.0080 - 31s/epoch - 5ms/step
Epoch 197/210
5969/5969 - 32s - loss: 0.0062 - val_loss: 0.0082 - 32s/epoch - 5ms/step
Epoch 198/210
5969/5969 - 31s - loss: 0.0062 - val_loss: 0.0079 - 31s/epoch - 5ms/step
Epoch 199/210
5969/5969 - 32s - loss: 0.0063 - val_loss: 0.0086 - 32s/epoch - 5ms/step
Epoch 200/210
5969/5969 - 31s - loss: 0.0062 - val_loss: 0.0077 - 31s/epoch - 5ms/step
Epoch 201/210
5969/5969 - 30s - loss: 0.0062 - val_loss: 0.0085 - 30s/epoch - 5ms/step
Epoch 202/210
5969/5969 - 27s - loss: 0.0063 - val_loss: 0.0086 - 27s/epoch - 5ms/step
Epoch 203/210
5969/5969 - 25s - loss: 0.0062 - val_loss: 0.0085 - 25s/epoch - 4ms/step
Epoch 204/210
5969/5969 - 28s - loss: 0.0062 - val_loss: 0.0080 - 28s/epoch - 5ms/step
Epoch 205/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0086 - 31s/epoch - 5ms/step
Epoch 206/210
5969/5969 - 30s - loss: 0.0062 - val_loss: 0.0080 - 30s/epoch - 5ms/step
Epoch 207/210
5969/5969 - 31s - loss: 0.0062 - val_loss: 0.0079 - 31s/epoch - 5ms/step
Epoch 208/210
5969/5969 - 31s - loss: 0.0062 - val_loss: 0.0093 - 31s/epoch - 5ms/step
Epoch 209/210
5969/5969 - 31s - loss: 0.0062 - val_loss: 0.0082 - 31s/epoch - 5ms/step
Epoch 210/210
5969/5969 - 31s - loss: 0.0063 - val_loss: 0.0081 - 31s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.008072858676314354
  1/332 [..............................] - ETA: 1:12 28/332 [=>............................] - ETA: 0s   61/332 [====>.........................] - ETA: 0s 93/332 [=======>......................] - ETA: 0s125/332 [==========>...................] - ETA: 0s157/332 [=============>................] - ETA: 0s189/332 [================>.............] - ETA: 0s221/332 [==================>...........] - ETA: 0s252/332 [=====================>........] - ETA: 0s283/332 [========================>.....] - ETA: 0s315/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.12542303660979282
cosine 0.09586685305602408
MAE: 0.051450912
RMSE: 0.1479648
r2: -0.41903289625909684
RMSE zero-vector: 0.2430644284356365
['1.0custom_VAE', 'logcosh', 16, 210, 0.002, 0.6, 758, 0.006253231316804886, 0.008072858676314354, 0.12542303660979282, 0.09586685305602408, 0.05145091190934181, 0.14796480536460876, -0.41903289625909684, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.0 120 0.002 256 2] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          958870      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          958870      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3141746     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 6,663,502
Trainable params: 6,656,930
Non-trainable params: 6,572
__________________________________________________________________________________________________
Epoch 1/120
374/374 - 4s - loss: 0.0221 - val_loss: 0.0102 - 4s/epoch - 11ms/step
Epoch 2/120
374/374 - 2s - loss: 0.0081 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 3/120
374/374 - 2s - loss: 0.0080 - val_loss: 0.0096 - 2s/epoch - 6ms/step
Epoch 4/120
374/374 - 2s - loss: 0.0077 - val_loss: 0.0086 - 2s/epoch - 6ms/step
Epoch 5/120
374/374 - 2s - loss: 0.0075 - val_loss: 0.0079 - 2s/epoch - 6ms/step
Epoch 6/120
374/374 - 2s - loss: 0.0074 - val_loss: 0.0084 - 2s/epoch - 6ms/step
Epoch 7/120
374/374 - 2s - loss: 0.0073 - val_loss: 0.0079 - 2s/epoch - 6ms/step
Epoch 8/120
374/374 - 2s - loss: 0.0073 - val_loss: 0.0081 - 2s/epoch - 6ms/step
Epoch 9/120
374/374 - 2s - loss: 0.0072 - val_loss: 0.0081 - 2s/epoch - 6ms/step
Epoch 10/120
374/374 - 2s - loss: 0.0071 - val_loss: 0.0074 - 2s/epoch - 6ms/step
Epoch 11/120
374/374 - 2s - loss: 0.0071 - val_loss: 0.0073 - 2s/epoch - 6ms/step
Epoch 12/120
374/374 - 2s - loss: 0.0070 - val_loss: 0.0073 - 2s/epoch - 6ms/step
Epoch 13/120
374/374 - 2s - loss: 0.0069 - val_loss: 0.0073 - 2s/epoch - 6ms/step
Epoch 14/120
374/374 - 2s - loss: 0.0069 - val_loss: 0.0070 - 2s/epoch - 6ms/step
Epoch 15/120
374/374 - 2s - loss: 0.0069 - val_loss: 0.0076 - 2s/epoch - 6ms/step
Epoch 16/120
374/374 - 2s - loss: 0.0068 - val_loss: 0.0068 - 2s/epoch - 6ms/step
Epoch 17/120
374/374 - 2s - loss: 0.0068 - val_loss: 0.0069 - 2s/epoch - 6ms/step
Epoch 18/120
374/374 - 2s - loss: 0.0068 - val_loss: 0.0068 - 2s/epoch - 6ms/step
Epoch 19/120
374/374 - 2s - loss: 0.0068 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 20/120
374/374 - 2s - loss: 0.0067 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 21/120
374/374 - 2s - loss: 0.0067 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 22/120
374/374 - 2s - loss: 0.0067 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 23/120
374/374 - 2s - loss: 0.0067 - val_loss: 0.0068 - 2s/epoch - 6ms/step
Epoch 24/120
374/374 - 2s - loss: 0.0067 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 25/120
374/374 - 2s - loss: 0.0067 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 26/120
374/374 - 2s - loss: 0.0067 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 27/120
374/374 - 2s - loss: 0.0067 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 28/120
374/374 - 2s - loss: 0.0067 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 29/120
374/374 - 2s - loss: 0.0067 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 30/120
374/374 - 2s - loss: 0.0067 - val_loss: 0.0067 - 2s/epoch - 6ms/step
Epoch 31/120
374/374 - 2s - loss: 0.0067 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 32/120
374/374 - 2s - loss: 0.0067 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 33/120
374/374 - 2s - loss: 0.0067 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 34/120
374/374 - 2s - loss: 0.0067 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 35/120
374/374 - 2s - loss: 0.0067 - val_loss: 0.0066 - 2s/epoch - 6ms/step
Epoch 36/120
374/374 - 2s - loss: 0.0066 - val_loss: 0.0065 - 2s/epoch - 6ms/step
Epoch 37/120
374/374 - 2s - loss: 0.0065 - val_loss: 0.0064 - 2s/epoch - 6ms/step
Epoch 38/120
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 39/120
374/374 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 6ms/step
Epoch 40/120
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 41/120
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 42/120
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 43/120
374/374 - 3s - loss: 0.0062 - val_loss: 0.0062 - 3s/epoch - 7ms/step
Epoch 44/120
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 7ms/step
Epoch 45/120
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 46/120
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 47/120
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 48/120
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 49/120
374/374 - 2s - loss: 0.0064 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 50/120
374/374 - 2s - loss: 0.0063 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 51/120
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 52/120
374/374 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 7ms/step
Epoch 53/120
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 54/120
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 55/120
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 6ms/step
Epoch 56/120
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 57/120
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 58/120
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 59/120
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 60/120
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 7ms/step
Epoch 61/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 62/120
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 63/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 64/120
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 65/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 66/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 67/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 68/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 69/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 70/120
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 71/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 72/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 73/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 74/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 75/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 76/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 77/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 78/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 79/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 80/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 81/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 82/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 83/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 84/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 85/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 86/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 87/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 88/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 89/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 90/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 91/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 92/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 93/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 94/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 95/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 96/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 97/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 98/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 99/120
374/374 - 3s - loss: 0.0061 - val_loss: 0.0060 - 3s/epoch - 7ms/step
Epoch 100/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 101/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 7ms/step
Epoch 102/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 103/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 104/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 105/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 106/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 107/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 108/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 109/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 110/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 111/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 112/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 113/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 114/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 115/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
Epoch 116/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 117/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 118/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 119/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 6ms/step
Epoch 120/120
374/374 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.006055834237486124
  1/332 [..............................] - ETA: 1:04 32/332 [=>............................] - ETA: 0s   63/332 [====>.........................] - ETA: 0s 94/332 [=======>......................] - ETA: 0s126/332 [==========>...................] - ETA: 0s158/332 [=============>................] - ETA: 0s190/332 [================>.............] - ETA: 0s221/332 [==================>...........] - ETA: 0s253/332 [=====================>........] - ETA: 0s286/332 [========================>.....] - ETA: 0s319/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.11933098703094847
cosine 0.09116477206141554
MAE: 0.0474965
RMSE: 0.10107053
r2: 0.3379046500053411
RMSE zero-vector: 0.2430644284356365
['1.0custom_VAE', 'logcosh', 256, 120, 0.002, 0.6, 758, 0.006086130626499653, 0.006055834237486124, 0.11933098703094847, 0.09116477206141554, 0.047496501356363297, 0.10107053071260452, 0.3379046500053411, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 150 0.002 128 2] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          479814      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          479814      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1860682     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 3,622,318
Trainable params: 3,618,274
Non-trainable params: 4,044
__________________________________________________________________________________________________
Epoch 1/150
747/747 - 7s - loss: 0.0142 - val_loss: 0.0093 - 7s/epoch - 9ms/step
Epoch 2/150
747/747 - 4s - loss: 0.0076 - val_loss: 0.0078 - 4s/epoch - 6ms/step
Epoch 3/150
747/747 - 4s - loss: 0.0073 - val_loss: 0.0073 - 4s/epoch - 5ms/step
Epoch 4/150
747/747 - 4s - loss: 0.0072 - val_loss: 0.0074 - 4s/epoch - 6ms/step
Epoch 5/150
747/747 - 4s - loss: 0.0071 - val_loss: 0.0070 - 4s/epoch - 6ms/step
Epoch 6/150
747/747 - 4s - loss: 0.0070 - val_loss: 0.0071 - 4s/epoch - 6ms/step
Epoch 7/150
747/747 - 4s - loss: 0.0069 - val_loss: 0.0069 - 4s/epoch - 6ms/step
Epoch 8/150
747/747 - 4s - loss: 0.0068 - val_loss: 0.0068 - 4s/epoch - 6ms/step
Epoch 9/150
747/747 - 4s - loss: 0.0068 - val_loss: 0.0068 - 4s/epoch - 6ms/step
Epoch 10/150
747/747 - 4s - loss: 0.0068 - val_loss: 0.0067 - 4s/epoch - 6ms/step
Epoch 11/150
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 6ms/step
Epoch 12/150
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 6ms/step
Epoch 13/150
747/747 - 5s - loss: 0.0067 - val_loss: 0.0067 - 5s/epoch - 6ms/step
Epoch 14/150
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 6ms/step
Epoch 15/150
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 16/150
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 17/150
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 18/150
747/747 - 4s - loss: 0.0067 - val_loss: 0.0067 - 4s/epoch - 5ms/step
Epoch 19/150
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 20/150
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 21/150
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 6ms/step
Epoch 22/150
747/747 - 4s - loss: 0.0067 - val_loss: 0.0066 - 4s/epoch - 6ms/step
Epoch 23/150
747/747 - 4s - loss: 0.0066 - val_loss: 0.0066 - 4s/epoch - 5ms/step
Epoch 24/150
747/747 - 4s - loss: 0.0065 - val_loss: 0.0064 - 4s/epoch - 5ms/step
Epoch 25/150
747/747 - 4s - loss: 0.0064 - val_loss: 0.0063 - 4s/epoch - 5ms/step
Epoch 26/150
747/747 - 3s - loss: 0.0063 - val_loss: 0.0062 - 3s/epoch - 5ms/step
Epoch 27/150
747/747 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 28/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 29/150
747/747 - 3s - loss: 0.0062 - val_loss: 0.0062 - 3s/epoch - 5ms/step
Epoch 30/150
747/747 - 3s - loss: 0.0062 - val_loss: 0.0062 - 3s/epoch - 5ms/step
Epoch 31/150
747/747 - 3s - loss: 0.0062 - val_loss: 0.0062 - 3s/epoch - 5ms/step
Epoch 32/150
747/747 - 3s - loss: 0.0062 - val_loss: 0.0061 - 3s/epoch - 5ms/step
Epoch 33/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 34/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 5ms/step
Epoch 35/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 36/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 37/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 38/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 39/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 40/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 41/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 42/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 43/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 44/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 45/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 46/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 47/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 48/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 49/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 50/150
747/747 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 51/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 52/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 53/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 54/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 55/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 56/150
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 7ms/step
Epoch 57/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 58/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 59/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 60/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 61/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 62/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 63/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 64/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 65/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 66/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 67/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 68/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 69/150
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 70/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 71/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 72/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 73/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 74/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 75/150
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 76/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 77/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 78/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 79/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 80/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 81/150
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 82/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 83/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 84/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 85/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 86/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 87/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 88/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 89/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 90/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 91/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 92/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 93/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 94/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 95/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 96/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 97/150
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 6ms/step
Epoch 98/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 99/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 100/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 101/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 102/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 103/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 5ms/step
Epoch 104/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 105/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 106/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 107/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 108/150
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 7ms/step
Epoch 109/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 110/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 111/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 112/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 113/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 114/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 115/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 116/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 117/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 118/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 119/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 120/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 121/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 122/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 123/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 5ms/step
Epoch 124/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 125/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 126/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 127/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 128/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 129/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 130/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 131/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 132/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 133/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 134/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 135/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
Epoch 136/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 137/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 138/150
747/747 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 7ms/step
Epoch 139/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 140/150
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 141/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 142/150
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 7ms/step
Epoch 143/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 144/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 145/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 146/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 147/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 148/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 6ms/step
Epoch 149/150
747/747 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 6ms/step
Epoch 150/150
747/747 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 6ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.006060638464987278
  1/332 [..............................] - ETA: 1:12 30/332 [=>............................] - ETA: 0s   61/332 [====>.........................] - ETA: 0s 93/332 [=======>......................] - ETA: 0s125/332 [==========>...................] - ETA: 0s157/332 [=============>................] - ETA: 0s188/332 [===============>..............] - ETA: 0s219/332 [==================>...........] - ETA: 0s250/332 [=====================>........] - ETA: 0s283/332 [========================>.....] - ETA: 0s315/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.11918832941062936
cosine 0.09104426948250441
MAE: 0.047423672
RMSE: 0.10106659
r2: 0.3379560714563972
RMSE zero-vector: 0.2430644284356365
['0.5custom_VAE', 'logcosh', 128, 150, 0.002, 0.6, 758, 0.006097538396716118, 0.006060638464987278, 0.11918832941062936, 0.09104426948250441, 0.047423671931028366, 0.10106658935546875, 0.3379560714563972, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 150 0.002 256 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1916982     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1916982     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5703874     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 12,745,870
Trainable params: 12,734,242
Non-trainable params: 11,628
__________________________________________________________________________________________________
Epoch 1/150
374/374 - 4s - loss: 0.0461 - val_loss: 0.0241 - 4s/epoch - 11ms/step
Epoch 2/150
374/374 - 2s - loss: 0.0177 - val_loss: 0.0270 - 2s/epoch - 6ms/step
Epoch 3/150
374/374 - 2s - loss: 0.0160 - val_loss: 0.0179 - 2s/epoch - 6ms/step
Epoch 4/150
374/374 - 2s - loss: 0.0152 - val_loss: 0.0221 - 2s/epoch - 6ms/step
Epoch 5/150
374/374 - 2s - loss: 0.0146 - val_loss: 0.0216 - 2s/epoch - 6ms/step
Epoch 6/150
374/374 - 2s - loss: 0.0143 - val_loss: 0.0169 - 2s/epoch - 7ms/step
Epoch 7/150
374/374 - 2s - loss: 0.0138 - val_loss: 0.0150 - 2s/epoch - 6ms/step
Epoch 8/150
374/374 - 2s - loss: 0.0135 - val_loss: 0.0983 - 2s/epoch - 6ms/step
Epoch 9/150
374/374 - 2s - loss: 0.0148 - val_loss: 0.0144 - 2s/epoch - 7ms/step
Epoch 10/150
374/374 - 3s - loss: 0.0129 - val_loss: 0.0141 - 3s/epoch - 7ms/step
Epoch 11/150
374/374 - 2s - loss: 0.0128 - val_loss: 0.0130 - 2s/epoch - 7ms/step
Epoch 12/150
374/374 - 2s - loss: 0.0126 - val_loss: 0.0247 - 2s/epoch - 6ms/step
Epoch 13/150
374/374 - 2s - loss: 0.0141 - val_loss: 0.0144 - 2s/epoch - 6ms/step
Epoch 14/150
374/374 - 2s - loss: 0.0129 - val_loss: 0.0123 - 2s/epoch - 6ms/step
Epoch 15/150
374/374 - 2s - loss: 0.0123 - val_loss: 0.0124 - 2s/epoch - 6ms/step
Epoch 16/150
374/374 - 2s - loss: 0.0123 - val_loss: 0.0121 - 2s/epoch - 6ms/step
Epoch 17/150
374/374 - 2s - loss: 0.0120 - val_loss: 0.0130 - 2s/epoch - 6ms/step
Epoch 18/150
374/374 - 2s - loss: 0.0124 - val_loss: 0.0122 - 2s/epoch - 6ms/step
Epoch 19/150
374/374 - 2s - loss: 0.0123 - val_loss: 0.0137 - 2s/epoch - 6ms/step
Epoch 20/150
374/374 - 2s - loss: 0.0139 - val_loss: 0.0132 - 2s/epoch - 6ms/step
Epoch 21/150
374/374 - 2s - loss: 0.0180 - val_loss: 0.0152 - 2s/epoch - 6ms/step
Epoch 22/150
374/374 - 2s - loss: 0.0272 - val_loss: 0.0149 - 2s/epoch - 6ms/step
Epoch 23/150
374/374 - 2s - loss: 0.0222 - val_loss: 0.0163 - 2s/epoch - 6ms/step
Epoch 24/150
374/374 - 2s - loss: 0.0138 - val_loss: 0.0129 - 2s/epoch - 7ms/step
Epoch 25/150
374/374 - 2s - loss: 0.0129 - val_loss: 0.0127 - 2s/epoch - 7ms/step
Epoch 26/150
374/374 - 2s - loss: 0.0126 - val_loss: 0.0122 - 2s/epoch - 6ms/step
Epoch 27/150
374/374 - 2s - loss: 0.0124 - val_loss: 0.0119 - 2s/epoch - 6ms/step
Epoch 28/150
374/374 - 3s - loss: 0.0120 - val_loss: 0.0118 - 3s/epoch - 7ms/step
Epoch 29/150
374/374 - 2s - loss: 0.0126 - val_loss: 0.0145 - 2s/epoch - 6ms/step
Epoch 30/150
374/374 - 2s - loss: 0.0173 - val_loss: 0.0125 - 2s/epoch - 6ms/step
Epoch 31/150
374/374 - 2s - loss: 0.0125 - val_loss: 0.0122 - 2s/epoch - 6ms/step
Epoch 32/150
374/374 - 2s - loss: 0.0123 - val_loss: 0.0131 - 2s/epoch - 6ms/step
Epoch 33/150
374/374 - 2s - loss: 0.0158 - val_loss: 0.0125 - 2s/epoch - 6ms/step
Epoch 34/150
374/374 - 2s - loss: 0.0121 - val_loss: 0.0117 - 2s/epoch - 6ms/step
Epoch 35/150
374/374 - 2s - loss: 0.0119 - val_loss: 0.0119 - 2s/epoch - 6ms/step
Epoch 36/150
374/374 - 2s - loss: 0.0118 - val_loss: 0.0117 - 2s/epoch - 6ms/step
Epoch 37/150
374/374 - 2s - loss: 0.0124 - val_loss: 0.0144 - 2s/epoch - 6ms/step
Epoch 38/150
374/374 - 2s - loss: 0.0117 - val_loss: 0.0114 - 2s/epoch - 7ms/step
Epoch 39/150
374/374 - 2s - loss: 0.0115 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 40/150
374/374 - 2s - loss: 0.0115 - val_loss: 0.0122 - 2s/epoch - 7ms/step
Epoch 41/150
374/374 - 2s - loss: 0.0121 - val_loss: 0.0113 - 2s/epoch - 7ms/step
Epoch 42/150
374/374 - 2s - loss: 0.0115 - val_loss: 0.0133 - 2s/epoch - 6ms/step
Epoch 43/150
374/374 - 2s - loss: 0.0144 - val_loss: 0.0120 - 2s/epoch - 6ms/step
Epoch 44/150
374/374 - 2s - loss: 0.0119 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 45/150
374/374 - 2s - loss: 0.0114 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 46/150
374/374 - 2s - loss: 0.0113 - val_loss: 0.0143 - 2s/epoch - 6ms/step
Epoch 47/150
374/374 - 2s - loss: 0.0135 - val_loss: 0.0113 - 2s/epoch - 6ms/step
Epoch 48/150
374/374 - 2s - loss: 0.0113 - val_loss: 0.0119 - 2s/epoch - 6ms/step
Epoch 49/150
374/374 - 2s - loss: 0.0122 - val_loss: 0.0114 - 2s/epoch - 6ms/step
Epoch 50/150
374/374 - 2s - loss: 0.0116 - val_loss: 0.0118 - 2s/epoch - 6ms/step
Epoch 51/150
374/374 - 2s - loss: 0.0122 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 52/150
374/374 - 2s - loss: 0.0112 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 53/150
374/374 - 2s - loss: 0.0112 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 54/150
374/374 - 2s - loss: 0.0111 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 55/150
374/374 - 2s - loss: 0.0111 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 56/150
374/374 - 2s - loss: 0.0111 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 57/150
374/374 - 2s - loss: 0.0111 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 58/150
374/374 - 2s - loss: 0.0110 - val_loss: 0.0146 - 2s/epoch - 6ms/step
Epoch 59/150
374/374 - 2s - loss: 0.0122 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 60/150
374/374 - 2s - loss: 0.0110 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 61/150
374/374 - 2s - loss: 0.0110 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 62/150
374/374 - 2s - loss: 0.0111 - val_loss: 0.0108 - 2s/epoch - 5ms/step
Epoch 63/150
374/374 - 2s - loss: 0.0109 - val_loss: 0.0108 - 2s/epoch - 5ms/step
Epoch 64/150
374/374 - 2s - loss: 0.0109 - val_loss: 0.0108 - 2s/epoch - 5ms/step
Epoch 65/150
374/374 - 2s - loss: 0.0109 - val_loss: 0.0108 - 2s/epoch - 5ms/step
Epoch 66/150
374/374 - 2s - loss: 0.0109 - val_loss: 0.0109 - 2s/epoch - 5ms/step
Epoch 67/150
374/374 - 2s - loss: 0.0109 - val_loss: 0.0111 - 2s/epoch - 5ms/step
Epoch 68/150
374/374 - 2s - loss: 0.0112 - val_loss: 0.0107 - 2s/epoch - 5ms/step
Epoch 69/150
374/374 - 2s - loss: 0.0108 - val_loss: 0.0107 - 2s/epoch - 5ms/step
Epoch 70/150
374/374 - 2s - loss: 0.0108 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 71/150
374/374 - 2s - loss: 0.0114 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 72/150
374/374 - 2s - loss: 0.0108 - val_loss: 0.0123 - 2s/epoch - 6ms/step
Epoch 73/150
374/374 - 2s - loss: 0.0135 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 74/150
374/374 - 2s - loss: 0.0109 - val_loss: 0.0111 - 2s/epoch - 6ms/step
Epoch 75/150
374/374 - 3s - loss: 0.0122 - val_loss: 0.0108 - 3s/epoch - 7ms/step
Epoch 76/150
374/374 - 2s - loss: 0.0108 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 77/150
374/374 - 2s - loss: 0.0108 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 78/150
374/374 - 2s - loss: 0.0107 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 79/150
374/374 - 2s - loss: 0.0111 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 80/150
374/374 - 2s - loss: 0.0111 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 81/150
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 82/150
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 83/150
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 84/150
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 85/150
374/374 - 2s - loss: 0.0107 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 86/150
374/374 - 2s - loss: 0.0106 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 87/150
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 88/150
374/374 - 2s - loss: 0.0106 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 89/150
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 90/150
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 91/150
374/374 - 2s - loss: 0.0106 - val_loss: 0.0116 - 2s/epoch - 6ms/step
Epoch 92/150
374/374 - 2s - loss: 0.0122 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 93/150
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 94/150
374/374 - 2s - loss: 0.0106 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 95/150
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 96/150
374/374 - 2s - loss: 0.0105 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 97/150
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 98/150
374/374 - 2s - loss: 0.0105 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 99/150
374/374 - 2s - loss: 0.0105 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 100/150
374/374 - 2s - loss: 0.0114 - val_loss: 0.0112 - 2s/epoch - 6ms/step
Epoch 101/150
374/374 - 2s - loss: 0.0127 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 102/150
374/374 - 2s - loss: 0.0106 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 103/150
374/374 - 2s - loss: 0.0106 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 104/150
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 105/150
374/374 - 2s - loss: 0.0105 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 106/150
374/374 - 2s - loss: 0.0105 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 107/150
374/374 - 2s - loss: 0.0105 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 108/150
374/374 - 2s - loss: 0.0106 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 109/150
374/374 - 2s - loss: 0.0105 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 110/150
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 111/150
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 7ms/step
Epoch 112/150
374/374 - 2s - loss: 0.0104 - val_loss: 0.0104 - 2s/epoch - 6ms/step
Epoch 113/150
374/374 - 2s - loss: 0.0105 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 114/150
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 115/150
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 116/150
374/374 - 2s - loss: 0.0104 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 117/150
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 118/150
374/374 - 2s - loss: 0.0103 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 119/150
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 120/150
374/374 - 2s - loss: 0.0103 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 121/150
374/374 - 2s - loss: 0.0103 - val_loss: 0.0107 - 2s/epoch - 6ms/step
Epoch 122/150
374/374 - 2s - loss: 0.0104 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 123/150
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 124/150
374/374 - 2s - loss: 0.0102 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 125/150
374/374 - 2s - loss: 0.0104 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 126/150
374/374 - 2s - loss: 0.0102 - val_loss: 0.0103 - 2s/epoch - 6ms/step
Epoch 127/150
374/374 - 2s - loss: 0.0105 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 128/150
374/374 - 2s - loss: 0.0102 - val_loss: 0.0101 - 2s/epoch - 7ms/step
Epoch 129/150
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 130/150
374/374 - 2s - loss: 0.0101 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 131/150
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 132/150
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 7ms/step
Epoch 133/150
374/374 - 2s - loss: 0.0101 - val_loss: 0.0106 - 2s/epoch - 6ms/step
Epoch 134/150
374/374 - 2s - loss: 0.0113 - val_loss: 0.0105 - 2s/epoch - 6ms/step
Epoch 135/150
374/374 - 2s - loss: 0.0114 - val_loss: 0.0102 - 2s/epoch - 6ms/step
Epoch 136/150
374/374 - 2s - loss: 0.0102 - val_loss: 0.0110 - 2s/epoch - 6ms/step
Epoch 137/150
374/374 - 2s - loss: 0.0115 - val_loss: 0.0101 - 2s/epoch - 6ms/step
Epoch 138/150
374/374 - 2s - loss: 0.0102 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 139/150
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 140/150
374/374 - 2s - loss: 0.0101 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 141/150
374/374 - 2s - loss: 0.0101 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 142/150
374/374 - 2s - loss: 0.0101 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 143/150
374/374 - 2s - loss: 0.0101 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 144/150
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 145/150
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 146/150
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 147/150
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 6ms/step
Epoch 148/150
374/374 - 2s - loss: 0.0100 - val_loss: 0.0104 - 2s/epoch - 7ms/step
Epoch 149/150
374/374 - 2s - loss: 0.0103 - val_loss: 0.0100 - 2s/epoch - 6ms/step
Epoch 150/150
374/374 - 2s - loss: 0.0100 - val_loss: 0.0099 - 2s/epoch - 7ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.00989102479070425
  1/332 [..............................] - ETA: 1:07 31/332 [=>............................] - ETA: 0s   62/332 [====>.........................] - ETA: 0s 92/332 [=======>......................] - ETA: 0s125/332 [==========>...................] - ETA: 0s155/332 [=============>................] - ETA: 0s187/332 [===============>..............] - ETA: 0s219/332 [==================>...........] - ETA: 0s249/332 [=====================>........] - ETA: 0s278/332 [========================>.....] - ETA: 0s304/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 2ms/step
correlation 0.08112096916806955
cosine 0.06211130253063522
MAE: 0.038747527
RMSE: 0.083938874
r2: 0.5433349254112421
RMSE zero-vector: 0.2430644284356365
['2.0custom_VAE', 'mse', 256, 150, 0.002, 0.6, 758, 0.0100394980981946, 0.00989102479070425, 0.08112096916806955, 0.06211130253063522, 0.038747526705265045, 0.08393887430429459, 0.5433349254112421, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 210 0.001 32 2] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          479814      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          479814      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1860682     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 3,622,318
Trainable params: 3,618,274
Non-trainable params: 4,044
__________________________________________________________________________________________________
Epoch 1/210
2985/2985 - 17s - loss: 0.0104 - val_loss: 0.0074 - 17s/epoch - 6ms/step
Epoch 2/210
2985/2985 - 15s - loss: 0.0071 - val_loss: 0.0068 - 15s/epoch - 5ms/step
Epoch 3/210
2985/2985 - 16s - loss: 0.0069 - val_loss: 0.0068 - 16s/epoch - 5ms/step
Epoch 4/210
2985/2985 - 16s - loss: 0.0068 - val_loss: 0.0067 - 16s/epoch - 5ms/step
Epoch 5/210
2985/2985 - 16s - loss: 0.0068 - val_loss: 0.0067 - 16s/epoch - 5ms/step
Epoch 6/210
2985/2985 - 16s - loss: 0.0068 - val_loss: 0.0067 - 16s/epoch - 5ms/step
Epoch 7/210
2985/2985 - 15s - loss: 0.0068 - val_loss: 0.0067 - 15s/epoch - 5ms/step
Epoch 8/210
2985/2985 - 16s - loss: 0.0067 - val_loss: 0.0067 - 16s/epoch - 5ms/step
Epoch 9/210
2985/2985 - 16s - loss: 0.0067 - val_loss: 0.0067 - 16s/epoch - 5ms/step
Epoch 10/210
2985/2985 - 15s - loss: 0.0067 - val_loss: 0.0067 - 15s/epoch - 5ms/step
Epoch 11/210
2985/2985 - 16s - loss: 0.0067 - val_loss: 0.0068 - 16s/epoch - 5ms/step
Epoch 12/210
2985/2985 - 15s - loss: 0.0067 - val_loss: 0.0068 - 15s/epoch - 5ms/step
Epoch 13/210
2985/2985 - 15s - loss: 0.0067 - val_loss: 0.0068 - 15s/epoch - 5ms/step
Epoch 14/210
2985/2985 - 16s - loss: 0.0067 - val_loss: 0.0067 - 16s/epoch - 5ms/step
Epoch 15/210
2985/2985 - 15s - loss: 0.0067 - val_loss: 0.0067 - 15s/epoch - 5ms/step
Epoch 16/210
2985/2985 - 16s - loss: 0.0067 - val_loss: 0.0068 - 16s/epoch - 6ms/step
Epoch 17/210
2985/2985 - 16s - loss: 0.0067 - val_loss: 0.0068 - 16s/epoch - 6ms/step
Epoch 18/210
2985/2985 - 16s - loss: 0.0067 - val_loss: 0.0068 - 16s/epoch - 5ms/step
Epoch 19/210
2985/2985 - 16s - loss: 0.0067 - val_loss: 0.0068 - 16s/epoch - 5ms/step
Epoch 20/210
2985/2985 - 16s - loss: 0.0067 - val_loss: 0.0067 - 16s/epoch - 5ms/step
Epoch 21/210
2985/2985 - 16s - loss: 0.0067 - val_loss: 0.0068 - 16s/epoch - 5ms/step
Epoch 22/210
2985/2985 - 16s - loss: 0.0067 - val_loss: 0.0069 - 16s/epoch - 5ms/step
Epoch 23/210
2985/2985 - 15s - loss: 0.0067 - val_loss: 0.0068 - 15s/epoch - 5ms/step
Epoch 24/210
2985/2985 - 16s - loss: 0.0067 - val_loss: 0.0069 - 16s/epoch - 5ms/step
Epoch 25/210
2985/2985 - 16s - loss: 0.0067 - val_loss: 0.0068 - 16s/epoch - 5ms/step
Epoch 26/210
2985/2985 - 16s - loss: 0.0067 - val_loss: 0.0069 - 16s/epoch - 5ms/step
Epoch 27/210
2985/2985 - 15s - loss: 0.0067 - val_loss: 0.0068 - 15s/epoch - 5ms/step
Epoch 28/210
2985/2985 - 15s - loss: 0.0067 - val_loss: 0.0071 - 15s/epoch - 5ms/step
Epoch 29/210
2985/2985 - 15s - loss: 0.0067 - val_loss: 0.0069 - 15s/epoch - 5ms/step
Epoch 30/210
2985/2985 - 14s - loss: 0.0067 - val_loss: 0.0068 - 14s/epoch - 5ms/step
Epoch 31/210
2985/2985 - 13s - loss: 0.0067 - val_loss: 0.0067 - 13s/epoch - 4ms/step
Epoch 32/210
2985/2985 - 16s - loss: 0.0065 - val_loss: 0.0066 - 16s/epoch - 5ms/step
Epoch 33/210
2985/2985 - 16s - loss: 0.0064 - val_loss: 0.0065 - 16s/epoch - 5ms/step
Epoch 34/210
2985/2985 - 15s - loss: 0.0064 - val_loss: 0.0067 - 15s/epoch - 5ms/step
Epoch 35/210
2985/2985 - 16s - loss: 0.0064 - val_loss: 0.0066 - 16s/epoch - 5ms/step
Epoch 36/210
2985/2985 - 16s - loss: 0.0064 - val_loss: 0.0067 - 16s/epoch - 5ms/step
Epoch 37/210
2985/2985 - 16s - loss: 0.0064 - val_loss: 0.0067 - 16s/epoch - 5ms/step
Epoch 38/210
2985/2985 - 16s - loss: 0.0063 - val_loss: 0.0065 - 16s/epoch - 5ms/step
Epoch 39/210
2985/2985 - 16s - loss: 0.0063 - val_loss: 0.0064 - 16s/epoch - 5ms/step
Epoch 40/210
2985/2985 - 16s - loss: 0.0063 - val_loss: 0.0065 - 16s/epoch - 5ms/step
Epoch 41/210
2985/2985 - 16s - loss: 0.0063 - val_loss: 0.0067 - 16s/epoch - 5ms/step
Epoch 42/210
2985/2985 - 16s - loss: 0.0063 - val_loss: 0.0070 - 16s/epoch - 5ms/step
Epoch 43/210
2985/2985 - 17s - loss: 0.0063 - val_loss: 0.0065 - 17s/epoch - 6ms/step
Epoch 44/210
2985/2985 - 16s - loss: 0.0063 - val_loss: 0.0067 - 16s/epoch - 5ms/step
Epoch 45/210
2985/2985 - 16s - loss: 0.0063 - val_loss: 0.0065 - 16s/epoch - 5ms/step
Epoch 46/210
2985/2985 - 16s - loss: 0.0063 - val_loss: 0.0067 - 16s/epoch - 5ms/step
Epoch 47/210
2985/2985 - 16s - loss: 0.0063 - val_loss: 0.0065 - 16s/epoch - 5ms/step
Epoch 48/210
2985/2985 - 16s - loss: 0.0063 - val_loss: 0.0065 - 16s/epoch - 5ms/step
Epoch 49/210
2985/2985 - 16s - loss: 0.0063 - val_loss: 0.0067 - 16s/epoch - 5ms/step
Epoch 50/210
2985/2985 - 15s - loss: 0.0063 - val_loss: 0.0068 - 15s/epoch - 5ms/step
Epoch 51/210
2985/2985 - 16s - loss: 0.0063 - val_loss: 0.0067 - 16s/epoch - 5ms/step
Epoch 52/210
2985/2985 - 15s - loss: 0.0063 - val_loss: 0.0066 - 15s/epoch - 5ms/step
Epoch 53/210
2985/2985 - 16s - loss: 0.0063 - val_loss: 0.0066 - 16s/epoch - 5ms/step
Epoch 54/210
2985/2985 - 16s - loss: 0.0063 - val_loss: 0.0068 - 16s/epoch - 5ms/step
Epoch 55/210
2985/2985 - 16s - loss: 0.0063 - val_loss: 0.0064 - 16s/epoch - 5ms/step
Epoch 56/210
2985/2985 - 16s - loss: 0.0063 - val_loss: 0.0066 - 16s/epoch - 5ms/step
Epoch 57/210
2985/2985 - 17s - loss: 0.0063 - val_loss: 0.0068 - 17s/epoch - 6ms/step
Epoch 58/210
2985/2985 - 17s - loss: 0.0063 - val_loss: 0.0065 - 17s/epoch - 6ms/step
Epoch 59/210
2985/2985 - 17s - loss: 0.0063 - val_loss: 0.0068 - 17s/epoch - 6ms/step
Epoch 60/210
2985/2985 - 17s - loss: 0.0063 - val_loss: 0.0065 - 17s/epoch - 6ms/step
Epoch 61/210
2985/2985 - 18s - loss: 0.0063 - val_loss: 0.0065 - 18s/epoch - 6ms/step
Epoch 62/210
2985/2985 - 18s - loss: 0.0063 - val_loss: 0.0066 - 18s/epoch - 6ms/step
Epoch 63/210
2985/2985 - 17s - loss: 0.0063 - val_loss: 0.0067 - 17s/epoch - 6ms/step
Epoch 64/210
2985/2985 - 17s - loss: 0.0063 - val_loss: 0.0064 - 17s/epoch - 6ms/step
Epoch 65/210
2985/2985 - 17s - loss: 0.0063 - val_loss: 0.0065 - 17s/epoch - 6ms/step
Epoch 66/210
2985/2985 - 18s - loss: 0.0063 - val_loss: 0.0066 - 18s/epoch - 6ms/step
Epoch 67/210
2985/2985 - 17s - loss: 0.0063 - val_loss: 0.0068 - 17s/epoch - 6ms/step
Epoch 68/210
2985/2985 - 17s - loss: 0.0063 - val_loss: 0.0067 - 17s/epoch - 6ms/step
Epoch 69/210
2985/2985 - 17s - loss: 0.0063 - val_loss: 0.0067 - 17s/epoch - 6ms/step
Epoch 70/210
2985/2985 - 14s - loss: 0.0063 - val_loss: 0.0067 - 14s/epoch - 5ms/step
Epoch 71/210
2985/2985 - 14s - loss: 0.0063 - val_loss: 0.0068 - 14s/epoch - 5ms/step
Epoch 72/210
2985/2985 - 15s - loss: 0.0063 - val_loss: 0.0065 - 15s/epoch - 5ms/step
Epoch 73/210
2985/2985 - 13s - loss: 0.0063 - val_loss: 0.0070 - 13s/epoch - 4ms/step
Epoch 74/210
2985/2985 - 14s - loss: 0.0063 - val_loss: 0.0068 - 14s/epoch - 5ms/step
Epoch 75/210
2985/2985 - 17s - loss: 0.0063 - val_loss: 0.0067 - 17s/epoch - 6ms/step
Epoch 76/210
2985/2985 - 17s - loss: 0.0063 - val_loss: 0.0067 - 17s/epoch - 6ms/step
Epoch 77/210
2985/2985 - 16s - loss: 0.0062 - val_loss: 0.0066 - 16s/epoch - 5ms/step
Epoch 78/210
2985/2985 - 16s - loss: 0.0063 - val_loss: 0.0064 - 16s/epoch - 5ms/step
Epoch 79/210
2985/2985 - 16s - loss: 0.0063 - val_loss: 0.0065 - 16s/epoch - 5ms/step
Epoch 80/210
2985/2985 - 17s - loss: 0.0063 - val_loss: 0.0064 - 17s/epoch - 6ms/step
Epoch 81/210
2985/2985 - 16s - loss: 0.0062 - val_loss: 0.0069 - 16s/epoch - 5ms/step
Epoch 82/210
2985/2985 - 16s - loss: 0.0062 - val_loss: 0.0066 - 16s/epoch - 5ms/step
Epoch 83/210
2985/2985 - 16s - loss: 0.0062 - val_loss: 0.0068 - 16s/epoch - 5ms/step
Epoch 84/210
2985/2985 - 16s - loss: 0.0063 - val_loss: 0.0066 - 16s/epoch - 5ms/step
Epoch 85/210
2985/2985 - 16s - loss: 0.0062 - val_loss: 0.0066 - 16s/epoch - 6ms/step
Epoch 86/210
2985/2985 - 17s - loss: 0.0063 - val_loss: 0.0064 - 17s/epoch - 6ms/step
Epoch 87/210
2985/2985 - 16s - loss: 0.0062 - val_loss: 0.0064 - 16s/epoch - 5ms/step
Epoch 88/210
2985/2985 - 17s - loss: 0.0062 - val_loss: 0.0065 - 17s/epoch - 6ms/step
Epoch 89/210
2985/2985 - 16s - loss: 0.0062 - val_loss: 0.0066 - 16s/epoch - 5ms/step
Epoch 90/210
2985/2985 - 17s - loss: 0.0062 - val_loss: 0.0068 - 17s/epoch - 6ms/step
Epoch 91/210
2985/2985 - 16s - loss: 0.0062 - val_loss: 0.0065 - 16s/epoch - 6ms/step
Epoch 92/210
2985/2985 - 16s - loss: 0.0062 - val_loss: 0.0064 - 16s/epoch - 5ms/step
Epoch 93/210
2985/2985 - 16s - loss: 0.0062 - val_loss: 0.0070 - 16s/epoch - 5ms/step
Epoch 94/210
2985/2985 - 16s - loss: 0.0062 - val_loss: 0.0066 - 16s/epoch - 5ms/step
Epoch 95/210
2985/2985 - 16s - loss: 0.0062 - val_loss: 0.0064 - 16s/epoch - 5ms/step
Epoch 96/210
2985/2985 - 16s - loss: 0.0062 - val_loss: 0.0065 - 16s/epoch - 5ms/step
Epoch 97/210
2985/2985 - 16s - loss: 0.0062 - val_loss: 0.0067 - 16s/epoch - 5ms/step
Epoch 98/210
2985/2985 - 17s - loss: 0.0062 - val_loss: 0.0066 - 17s/epoch - 6ms/step
Epoch 99/210
2985/2985 - 16s - loss: 0.0062 - val_loss: 0.0065 - 16s/epoch - 5ms/step
Epoch 100/210
2985/2985 - 16s - loss: 0.0062 - val_loss: 0.0066 - 16s/epoch - 5ms/step
Epoch 101/210
2985/2985 - 16s - loss: 0.0062 - val_loss: 0.0064 - 16s/epoch - 5ms/step
Epoch 102/210
2985/2985 - 16s - loss: 0.0062 - val_loss: 0.0066 - 16s/epoch - 5ms/step
Epoch 103/210
2985/2985 - 17s - loss: 0.0062 - val_loss: 0.0066 - 17s/epoch - 6ms/step
Epoch 104/210
2985/2985 - 16s - loss: 0.0062 - val_loss: 0.0067 - 16s/epoch - 6ms/step
Epoch 105/210
2985/2985 - 17s - loss: 0.0062 - val_loss: 0.0066 - 17s/epoch - 6ms/step
Epoch 106/210
2985/2985 - 17s - loss: 0.0062 - val_loss: 0.0065 - 17s/epoch - 6ms/step
Epoch 107/210
2985/2985 - 16s - loss: 0.0062 - val_loss: 0.0066 - 16s/epoch - 5ms/step
Epoch 108/210
2985/2985 - 17s - loss: 0.0062 - val_loss: 0.0068 - 17s/epoch - 6ms/step
Epoch 109/210
2985/2985 - 17s - loss: 0.0062 - val_loss: 0.0065 - 17s/epoch - 6ms/step
Epoch 110/210
2985/2985 - 16s - loss: 0.0062 - val_loss: 0.0067 - 16s/epoch - 5ms/step
Epoch 111/210
2985/2985 - 16s - loss: 0.0062 - val_loss: 0.0065 - 16s/epoch - 6ms/step
Epoch 112/210
2985/2985 - 16s - loss: 0.0062 - val_loss: 0.0066 - 16s/epoch - 5ms/step
Epoch 113/210
2985/2985 - 15s - loss: 0.0062 - val_loss: 0.0067 - 15s/epoch - 5ms/step
Epoch 114/210
2985/2985 - 16s - loss: 0.0062 - val_loss: 0.0067 - 16s/epoch - 5ms/step
Epoch 115/210
2985/2985 - 15s - loss: 0.0062 - val_loss: 0.0065 - 15s/epoch - 5ms/step
Epoch 116/210
2985/2985 - 14s - loss: 0.0062 - val_loss: 0.0065 - 14s/epoch - 5ms/step
Epoch 117/210
2985/2985 - 14s - loss: 0.0062 - val_loss: 0.0067 - 14s/epoch - 5ms/step
Epoch 118/210
2985/2985 - 14s - loss: 0.0062 - val_loss: 0.0067 - 14s/epoch - 5ms/step
Epoch 119/210
2985/2985 - 14s - loss: 0.0062 - val_loss: 0.0066 - 14s/epoch - 5ms/step
Epoch 120/210
2985/2985 - 14s - loss: 0.0062 - val_loss: 0.0068 - 14s/epoch - 5ms/step
Epoch 121/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0066 - 13s/epoch - 4ms/step
Epoch 122/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0068 - 13s/epoch - 5ms/step
Epoch 123/210
2985/2985 - 14s - loss: 0.0062 - val_loss: 0.0066 - 14s/epoch - 5ms/step
Epoch 124/210
2985/2985 - 14s - loss: 0.0062 - val_loss: 0.0066 - 14s/epoch - 5ms/step
Epoch 125/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0067 - 13s/epoch - 4ms/step
Epoch 126/210
2985/2985 - 14s - loss: 0.0062 - val_loss: 0.0067 - 14s/epoch - 5ms/step
Epoch 127/210
2985/2985 - 14s - loss: 0.0062 - val_loss: 0.0065 - 14s/epoch - 5ms/step
Epoch 128/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0064 - 13s/epoch - 4ms/step
Epoch 129/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0064 - 13s/epoch - 4ms/step
Epoch 130/210
2985/2985 - 14s - loss: 0.0062 - val_loss: 0.0065 - 14s/epoch - 5ms/step
Epoch 131/210
2985/2985 - 14s - loss: 0.0062 - val_loss: 0.0064 - 14s/epoch - 5ms/step
Epoch 132/210
2985/2985 - 14s - loss: 0.0062 - val_loss: 0.0066 - 14s/epoch - 5ms/step
Epoch 133/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 134/210
2985/2985 - 12s - loss: 0.0062 - val_loss: 0.0067 - 12s/epoch - 4ms/step
Epoch 135/210
2985/2985 - 12s - loss: 0.0062 - val_loss: 0.0065 - 12s/epoch - 4ms/step
Epoch 136/210
2985/2985 - 12s - loss: 0.0062 - val_loss: 0.0066 - 12s/epoch - 4ms/step
Epoch 137/210
2985/2985 - 12s - loss: 0.0062 - val_loss: 0.0067 - 12s/epoch - 4ms/step
Epoch 138/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0069 - 13s/epoch - 4ms/step
Epoch 139/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 140/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 141/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 142/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 143/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 144/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 145/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 146/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 147/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0064 - 13s/epoch - 4ms/step
Epoch 148/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0067 - 13s/epoch - 4ms/step
Epoch 149/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0064 - 13s/epoch - 4ms/step
Epoch 150/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 151/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 152/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0064 - 13s/epoch - 4ms/step
Epoch 153/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 154/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 155/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0066 - 13s/epoch - 4ms/step
Epoch 156/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0063 - 13s/epoch - 4ms/step
Epoch 157/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 158/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 159/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 160/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 161/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 162/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 163/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 164/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 165/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0066 - 13s/epoch - 4ms/step
Epoch 166/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0064 - 13s/epoch - 4ms/step
Epoch 167/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 168/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 169/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0066 - 13s/epoch - 4ms/step
Epoch 170/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0067 - 13s/epoch - 4ms/step
Epoch 171/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0063 - 13s/epoch - 4ms/step
Epoch 172/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 173/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0066 - 13s/epoch - 4ms/step
Epoch 174/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0063 - 13s/epoch - 4ms/step
Epoch 175/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0064 - 13s/epoch - 4ms/step
Epoch 176/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 177/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 178/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 179/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0064 - 13s/epoch - 4ms/step
Epoch 180/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0064 - 13s/epoch - 4ms/step
Epoch 181/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0064 - 13s/epoch - 4ms/step
Epoch 182/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 183/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0064 - 13s/epoch - 4ms/step
Epoch 184/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0063 - 13s/epoch - 4ms/step
Epoch 185/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0064 - 13s/epoch - 4ms/step
Epoch 186/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0064 - 13s/epoch - 4ms/step
Epoch 187/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 188/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0064 - 13s/epoch - 4ms/step
Epoch 189/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 190/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0063 - 13s/epoch - 4ms/step
Epoch 191/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 192/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0064 - 13s/epoch - 4ms/step
Epoch 193/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0064 - 13s/epoch - 4ms/step
Epoch 194/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 195/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0064 - 13s/epoch - 4ms/step
Epoch 196/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0064 - 13s/epoch - 4ms/step
Epoch 197/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0064 - 13s/epoch - 4ms/step
Epoch 198/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0063 - 13s/epoch - 4ms/step
Epoch 199/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0066 - 13s/epoch - 4ms/step
Epoch 200/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0063 - 13s/epoch - 4ms/step
Epoch 201/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0063 - 13s/epoch - 4ms/step
Epoch 202/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0063 - 13s/epoch - 4ms/step
Epoch 203/210
2985/2985 - 14s - loss: 0.0062 - val_loss: 0.0064 - 14s/epoch - 5ms/step
Epoch 204/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0066 - 13s/epoch - 4ms/step
Epoch 205/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0066 - 13s/epoch - 4ms/step
Epoch 206/210
2985/2985 - 14s - loss: 0.0062 - val_loss: 0.0064 - 14s/epoch - 5ms/step
Epoch 207/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0064 - 13s/epoch - 5ms/step
Epoch 208/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 4ms/step
Epoch 209/210
2985/2985 - 13s - loss: 0.0062 - val_loss: 0.0065 - 13s/epoch - 5ms/step
Epoch 210/210
2985/2985 - 14s - loss: 0.0062 - val_loss: 0.0063 - 14s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.006272595375776291
  1/332 [..............................] - ETA: 55s 36/332 [==>...........................] - ETA: 0s  72/332 [=====>........................] - ETA: 0s108/332 [========>.....................] - ETA: 0s143/332 [===========>..................] - ETA: 0s179/332 [===============>..............] - ETA: 0s215/332 [==================>...........] - ETA: 0s251/332 [=====================>........] - ETA: 0s287/332 [========================>.....] - ETA: 0s322/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.12446557384727139
cosine 0.09514335780785392
MAE: 0.049138203
RMSE: 0.1055826
r2: 0.27746891660564155
RMSE zero-vector: 0.2430644284356365
['0.5custom_VAE', 'logcosh', 32, 210, 0.001, 0.6, 758, 0.006216639187186956, 0.006272595375776291, 0.12446557384727139, 0.09514335780785392, 0.049138203263282776, 0.10558260232210159, 0.27746891660564155, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.0 150 0.0005 16 2] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          958870      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          958870      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3141746     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 6,663,502
Trainable params: 6,656,930
Non-trainable params: 6,572
__________________________________________________________________________________________________
Epoch 1/150
5969/5969 - 28s - loss: 0.0107 - val_loss: 0.0069 - 28s/epoch - 5ms/step
Epoch 2/150
5969/5969 - 27s - loss: 0.0069 - val_loss: 0.0068 - 27s/epoch - 4ms/step
Epoch 3/150
5969/5969 - 27s - loss: 0.0069 - val_loss: 0.0068 - 27s/epoch - 5ms/step
Epoch 4/150
5969/5969 - 24s - loss: 0.0069 - val_loss: 0.0068 - 24s/epoch - 4ms/step
Epoch 5/150
5969/5969 - 25s - loss: 0.0069 - val_loss: 0.0069 - 25s/epoch - 4ms/step
Epoch 6/150
5969/5969 - 27s - loss: 0.0069 - val_loss: 0.0068 - 27s/epoch - 5ms/step
Epoch 7/150
5969/5969 - 27s - loss: 0.0069 - val_loss: 0.0068 - 27s/epoch - 5ms/step
Epoch 8/150
5969/5969 - 27s - loss: 0.0068 - val_loss: 0.0068 - 27s/epoch - 4ms/step
Epoch 9/150
5969/5969 - 27s - loss: 0.0068 - val_loss: 0.0069 - 27s/epoch - 5ms/step
Epoch 10/150
5969/5969 - 27s - loss: 0.0068 - val_loss: 0.0068 - 27s/epoch - 4ms/step
Epoch 11/150
5969/5969 - 27s - loss: 0.0068 - val_loss: 0.0068 - 27s/epoch - 5ms/step
Epoch 12/150
5969/5969 - 27s - loss: 0.0068 - val_loss: 0.0069 - 27s/epoch - 5ms/step
Epoch 13/150
5969/5969 - 27s - loss: 0.0068 - val_loss: 0.0069 - 27s/epoch - 4ms/step
Epoch 14/150
5969/5969 - 27s - loss: 0.0068 - val_loss: 0.0070 - 27s/epoch - 5ms/step
Epoch 15/150
5969/5969 - 27s - loss: 0.0067 - val_loss: 0.0069 - 27s/epoch - 5ms/step
Epoch 16/150
5969/5969 - 27s - loss: 0.0065 - val_loss: 0.0071 - 27s/epoch - 5ms/step
Epoch 17/150
5969/5969 - 27s - loss: 0.0065 - val_loss: 0.0073 - 27s/epoch - 4ms/step
Epoch 18/150
5969/5969 - 27s - loss: 0.0065 - val_loss: 0.0076 - 27s/epoch - 5ms/step
Epoch 19/150
5969/5969 - 27s - loss: 0.0065 - val_loss: 0.0077 - 27s/epoch - 5ms/step
Epoch 20/150
5969/5969 - 27s - loss: 0.0065 - val_loss: 0.0078 - 27s/epoch - 5ms/step
Epoch 21/150
5969/5969 - 27s - loss: 0.0065 - val_loss: 0.0075 - 27s/epoch - 5ms/step
Epoch 22/150
5969/5969 - 27s - loss: 0.0064 - val_loss: 0.0079 - 27s/epoch - 5ms/step
Epoch 23/150
5969/5969 - 27s - loss: 0.0064 - val_loss: 0.0082 - 27s/epoch - 5ms/step
Epoch 24/150
5969/5969 - 27s - loss: 0.0064 - val_loss: 0.0089 - 27s/epoch - 5ms/step
Epoch 25/150
5969/5969 - 27s - loss: 0.0064 - val_loss: 0.0084 - 27s/epoch - 4ms/step
Epoch 26/150
5969/5969 - 27s - loss: 0.0064 - val_loss: 0.0074 - 27s/epoch - 4ms/step
Epoch 27/150
5969/5969 - 27s - loss: 0.0064 - val_loss: 0.0087 - 27s/epoch - 4ms/step
Epoch 28/150
5969/5969 - 27s - loss: 0.0064 - val_loss: 0.0080 - 27s/epoch - 5ms/step
Epoch 29/150
5969/5969 - 28s - loss: 0.0064 - val_loss: 0.0083 - 28s/epoch - 5ms/step
Epoch 30/150
5969/5969 - 29s - loss: 0.0064 - val_loss: 0.0087 - 29s/epoch - 5ms/step
Epoch 31/150
5969/5969 - 28s - loss: 0.0064 - val_loss: 0.0086 - 28s/epoch - 5ms/step
Epoch 32/150
5969/5969 - 29s - loss: 0.0064 - val_loss: 0.0082 - 29s/epoch - 5ms/step
Epoch 33/150
5969/5969 - 29s - loss: 0.0064 - val_loss: 0.0077 - 29s/epoch - 5ms/step
Epoch 34/150
5969/5969 - 28s - loss: 0.0064 - val_loss: 0.0079 - 28s/epoch - 5ms/step
Epoch 35/150
5969/5969 - 28s - loss: 0.0064 - val_loss: 0.0083 - 28s/epoch - 5ms/step
Epoch 36/150
5969/5969 - 28s - loss: 0.0064 - val_loss: 0.0091 - 28s/epoch - 5ms/step
Epoch 37/150
5969/5969 - 27s - loss: 0.0064 - val_loss: 0.0089 - 27s/epoch - 5ms/step
Epoch 38/150
5969/5969 - 27s - loss: 0.0064 - val_loss: 0.0087 - 27s/epoch - 5ms/step
Epoch 39/150
5969/5969 - 28s - loss: 0.0064 - val_loss: 0.0083 - 28s/epoch - 5ms/step
Epoch 40/150
5969/5969 - 26s - loss: 0.0064 - val_loss: 0.0079 - 26s/epoch - 4ms/step
Epoch 41/150
5969/5969 - 26s - loss: 0.0064 - val_loss: 0.0093 - 26s/epoch - 4ms/step
Epoch 42/150
5969/5969 - 26s - loss: 0.0064 - val_loss: 0.0090 - 26s/epoch - 4ms/step
Epoch 43/150
5969/5969 - 26s - loss: 0.0064 - val_loss: 0.0089 - 26s/epoch - 4ms/step
Epoch 44/150
5969/5969 - 26s - loss: 0.0064 - val_loss: 0.0082 - 26s/epoch - 4ms/step
Epoch 45/150
5969/5969 - 25s - loss: 0.0064 - val_loss: 0.0090 - 25s/epoch - 4ms/step
Epoch 46/150
5969/5969 - 25s - loss: 0.0064 - val_loss: 0.0079 - 25s/epoch - 4ms/step
Epoch 47/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0087 - 24s/epoch - 4ms/step
Epoch 48/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0088 - 24s/epoch - 4ms/step
Epoch 49/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0089 - 24s/epoch - 4ms/step
Epoch 50/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0099 - 24s/epoch - 4ms/step
Epoch 51/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0088 - 24s/epoch - 4ms/step
Epoch 52/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0092 - 24s/epoch - 4ms/step
Epoch 53/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0091 - 24s/epoch - 4ms/step
Epoch 54/150
5969/5969 - 25s - loss: 0.0064 - val_loss: 0.0085 - 25s/epoch - 4ms/step
Epoch 55/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0096 - 24s/epoch - 4ms/step
Epoch 56/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0084 - 24s/epoch - 4ms/step
Epoch 57/150
5969/5969 - 25s - loss: 0.0064 - val_loss: 0.0091 - 25s/epoch - 4ms/step
Epoch 58/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0100 - 24s/epoch - 4ms/step
Epoch 59/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0100 - 24s/epoch - 4ms/step
Epoch 60/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0099 - 24s/epoch - 4ms/step
Epoch 61/150
5969/5969 - 25s - loss: 0.0064 - val_loss: 0.0091 - 25s/epoch - 4ms/step
Epoch 62/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0099 - 24s/epoch - 4ms/step
Epoch 63/150
5969/5969 - 25s - loss: 0.0064 - val_loss: 0.0086 - 25s/epoch - 4ms/step
Epoch 64/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0102 - 24s/epoch - 4ms/step
Epoch 65/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0100 - 24s/epoch - 4ms/step
Epoch 66/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0103 - 24s/epoch - 4ms/step
Epoch 67/150
5969/5969 - 25s - loss: 0.0064 - val_loss: 0.0095 - 25s/epoch - 4ms/step
Epoch 68/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0085 - 24s/epoch - 4ms/step
Epoch 69/150
5969/5969 - 25s - loss: 0.0064 - val_loss: 0.0086 - 25s/epoch - 4ms/step
Epoch 70/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0083 - 24s/epoch - 4ms/step
Epoch 71/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0106 - 24s/epoch - 4ms/step
Epoch 72/150
5969/5969 - 25s - loss: 0.0064 - val_loss: 0.0096 - 25s/epoch - 4ms/step
Epoch 73/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0117 - 24s/epoch - 4ms/step
Epoch 74/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0096 - 24s/epoch - 4ms/step
Epoch 75/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0086 - 24s/epoch - 4ms/step
Epoch 76/150
5969/5969 - 25s - loss: 0.0064 - val_loss: 0.0086 - 25s/epoch - 4ms/step
Epoch 77/150
5969/5969 - 25s - loss: 0.0064 - val_loss: 0.0087 - 25s/epoch - 4ms/step
Epoch 78/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0115 - 24s/epoch - 4ms/step
Epoch 79/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0087 - 24s/epoch - 4ms/step
Epoch 80/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0119 - 24s/epoch - 4ms/step
Epoch 81/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0099 - 24s/epoch - 4ms/step
Epoch 82/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0106 - 24s/epoch - 4ms/step
Epoch 83/150
5969/5969 - 25s - loss: 0.0064 - val_loss: 0.0093 - 25s/epoch - 4ms/step
Epoch 84/150
5969/5969 - 26s - loss: 0.0064 - val_loss: 0.0097 - 26s/epoch - 4ms/step
Epoch 85/150
5969/5969 - 25s - loss: 0.0064 - val_loss: 0.0094 - 25s/epoch - 4ms/step
Epoch 86/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0092 - 24s/epoch - 4ms/step
Epoch 87/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0097 - 24s/epoch - 4ms/step
Epoch 88/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0097 - 24s/epoch - 4ms/step
Epoch 89/150
5969/5969 - 25s - loss: 0.0064 - val_loss: 0.0094 - 25s/epoch - 4ms/step
Epoch 90/150
5969/5969 - 25s - loss: 0.0063 - val_loss: 0.0091 - 25s/epoch - 4ms/step
Epoch 91/150
5969/5969 - 25s - loss: 0.0063 - val_loss: 0.0098 - 25s/epoch - 4ms/step
Epoch 92/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0107 - 24s/epoch - 4ms/step
Epoch 93/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0097 - 24s/epoch - 4ms/step
Epoch 94/150
5969/5969 - 25s - loss: 0.0064 - val_loss: 0.0117 - 25s/epoch - 4ms/step
Epoch 95/150
5969/5969 - 25s - loss: 0.0064 - val_loss: 0.0105 - 25s/epoch - 4ms/step
Epoch 96/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0091 - 24s/epoch - 4ms/step
Epoch 97/150
5969/5969 - 25s - loss: 0.0064 - val_loss: 0.0105 - 25s/epoch - 4ms/step
Epoch 98/150
5969/5969 - 24s - loss: 0.0064 - val_loss: 0.0095 - 24s/epoch - 4ms/step
Epoch 99/150
5969/5969 - 25s - loss: 0.0063 - val_loss: 0.0113 - 25s/epoch - 4ms/step
Epoch 100/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0096 - 24s/epoch - 4ms/step
Epoch 101/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0112 - 24s/epoch - 4ms/step
Epoch 102/150
5969/5969 - 25s - loss: 0.0064 - val_loss: 0.0105 - 25s/epoch - 4ms/step
Epoch 103/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0111 - 24s/epoch - 4ms/step
Epoch 104/150
5969/5969 - 25s - loss: 0.0063 - val_loss: 0.0091 - 25s/epoch - 4ms/step
Epoch 105/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0111 - 24s/epoch - 4ms/step
Epoch 106/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0083 - 24s/epoch - 4ms/step
Epoch 107/150
5969/5969 - 25s - loss: 0.0063 - val_loss: 0.0102 - 25s/epoch - 4ms/step
Epoch 108/150
5969/5969 - 25s - loss: 0.0063 - val_loss: 0.0103 - 25s/epoch - 4ms/step
Epoch 109/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0097 - 24s/epoch - 4ms/step
Epoch 110/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0113 - 24s/epoch - 4ms/step
Epoch 111/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0097 - 24s/epoch - 4ms/step
Epoch 112/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0101 - 24s/epoch - 4ms/step
Epoch 113/150
5969/5969 - 25s - loss: 0.0063 - val_loss: 0.0107 - 25s/epoch - 4ms/step
Epoch 114/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0090 - 24s/epoch - 4ms/step
Epoch 115/150
5969/5969 - 25s - loss: 0.0063 - val_loss: 0.0097 - 25s/epoch - 4ms/step
Epoch 116/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0102 - 24s/epoch - 4ms/step
Epoch 117/150
5969/5969 - 25s - loss: 0.0063 - val_loss: 0.0098 - 25s/epoch - 4ms/step
Epoch 118/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0109 - 24s/epoch - 4ms/step
Epoch 119/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0091 - 24s/epoch - 4ms/step
Epoch 120/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0087 - 24s/epoch - 4ms/step
Epoch 121/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0090 - 24s/epoch - 4ms/step
Epoch 122/150
5969/5969 - 25s - loss: 0.0063 - val_loss: 0.0103 - 25s/epoch - 4ms/step
Epoch 123/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0101 - 24s/epoch - 4ms/step
Epoch 124/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0086 - 24s/epoch - 4ms/step
Epoch 125/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0099 - 24s/epoch - 4ms/step
Epoch 126/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0095 - 24s/epoch - 4ms/step
Epoch 127/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0094 - 24s/epoch - 4ms/step
Epoch 128/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0097 - 24s/epoch - 4ms/step
Epoch 129/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0089 - 24s/epoch - 4ms/step
Epoch 130/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0108 - 24s/epoch - 4ms/step
Epoch 131/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0093 - 24s/epoch - 4ms/step
Epoch 132/150
5969/5969 - 25s - loss: 0.0063 - val_loss: 0.0104 - 25s/epoch - 4ms/step
Epoch 133/150
5969/5969 - 25s - loss: 0.0063 - val_loss: 0.0094 - 25s/epoch - 4ms/step
Epoch 134/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0102 - 24s/epoch - 4ms/step
Epoch 135/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0108 - 24s/epoch - 4ms/step
Epoch 136/150
5969/5969 - 25s - loss: 0.0063 - val_loss: 0.0098 - 25s/epoch - 4ms/step
Epoch 137/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0107 - 24s/epoch - 4ms/step
Epoch 138/150
5969/5969 - 25s - loss: 0.0063 - val_loss: 0.0104 - 25s/epoch - 4ms/step
Epoch 139/150
5969/5969 - 25s - loss: 0.0063 - val_loss: 0.0105 - 25s/epoch - 4ms/step
Epoch 140/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0115 - 24s/epoch - 4ms/step
Epoch 141/150
5969/5969 - 25s - loss: 0.0063 - val_loss: 0.0104 - 25s/epoch - 4ms/step
Epoch 142/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0103 - 24s/epoch - 4ms/step
Epoch 143/150
5969/5969 - 25s - loss: 0.0063 - val_loss: 0.0086 - 25s/epoch - 4ms/step
Epoch 144/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0093 - 24s/epoch - 4ms/step
Epoch 145/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0118 - 24s/epoch - 4ms/step
Epoch 146/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0101 - 24s/epoch - 4ms/step
Epoch 147/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0122 - 24s/epoch - 4ms/step
Epoch 148/150
5969/5969 - 24s - loss: 0.0063 - val_loss: 0.0101 - 24s/epoch - 4ms/step
Epoch 149/150
5969/5969 - 25s - loss: 0.0063 - val_loss: 0.0126 - 25s/epoch - 4ms/step
Epoch 150/150
5969/5969 - 25s - loss: 0.0063 - val_loss: 0.0113 - 25s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.01133425347507
  1/332 [..............................] - ETA: 55s 37/332 [==>...........................] - ETA: 0s  74/332 [=====>........................] - ETA: 0s111/332 [=========>....................] - ETA: 0s148/332 [============>.................] - ETA: 0s185/332 [===============>..............] - ETA: 0s222/332 [===================>..........] - ETA: 0s259/332 [======================>.......] - ETA: 0s296/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.1298357744501462
cosine 0.09927695308581326
MAE: 0.055748392
RMSE: 0.30214623
r2: -4.917096248920408
RMSE zero-vector: 0.2430644284356365
['1.0custom_VAE', 'logcosh', 16, 150, 0.0005, 0.6, 758, 0.006337813567370176, 0.01133425347507, 0.1298357744501462, 0.09927695308581326, 0.05574839189648628, 0.30214622616767883, -4.917096248920408, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 60 0.002 64 2] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          479814      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          479814      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1860682     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 3,622,318
Trainable params: 3,618,274
Non-trainable params: 4,044
__________________________________________________________________________________________________
Epoch 1/60
1493/1493 - 8s - loss: 0.0116 - val_loss: 0.0077 - 8s/epoch - 5ms/step
Epoch 2/60
1493/1493 - 6s - loss: 0.0074 - val_loss: 0.0073 - 6s/epoch - 4ms/step
Epoch 3/60
1493/1493 - 6s - loss: 0.0071 - val_loss: 0.0071 - 6s/epoch - 4ms/step
Epoch 4/60
1493/1493 - 6s - loss: 0.0069 - val_loss: 0.0069 - 6s/epoch - 4ms/step
Epoch 5/60
1493/1493 - 6s - loss: 0.0068 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 6/60
1493/1493 - 6s - loss: 0.0068 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 7/60
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 8/60
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 9/60
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0066 - 6s/epoch - 4ms/step
Epoch 10/60
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 11/60
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 12/60
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0066 - 6s/epoch - 4ms/step
Epoch 13/60
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 14/60
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0066 - 6s/epoch - 4ms/step
Epoch 15/60
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0066 - 6s/epoch - 4ms/step
Epoch 16/60
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 17/60
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0066 - 6s/epoch - 4ms/step
Epoch 18/60
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 19/60
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0066 - 6s/epoch - 4ms/step
Epoch 20/60
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0066 - 6s/epoch - 4ms/step
Epoch 21/60
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0066 - 6s/epoch - 4ms/step
Epoch 22/60
1493/1493 - 6s - loss: 0.0066 - val_loss: 0.0065 - 6s/epoch - 4ms/step
Epoch 23/60
1493/1493 - 6s - loss: 0.0065 - val_loss: 0.0064 - 6s/epoch - 4ms/step
Epoch 24/60
1493/1493 - 6s - loss: 0.0064 - val_loss: 0.0063 - 6s/epoch - 4ms/step
Epoch 25/60
1493/1493 - 6s - loss: 0.0063 - val_loss: 0.0063 - 6s/epoch - 4ms/step
Epoch 26/60
1493/1493 - 6s - loss: 0.0063 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 27/60
1493/1493 - 6s - loss: 0.0063 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 28/60
1493/1493 - 6s - loss: 0.0063 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 29/60
1493/1493 - 6s - loss: 0.0063 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 30/60
1493/1493 - 6s - loss: 0.0063 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 31/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 32/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 33/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 34/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 35/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 36/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 37/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 38/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 39/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 40/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 41/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 42/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 43/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 44/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 45/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 46/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 47/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 48/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 49/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 50/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 51/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 52/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 53/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 54/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 55/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 56/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 57/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 58/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 59/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 60/60
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.0061617628671228886
  1/332 [..............................] - ETA: 54s 38/332 [==>...........................] - ETA: 0s  76/332 [=====>........................] - ETA: 0s114/332 [=========>....................] - ETA: 0s152/332 [============>.................] - ETA: 0s191/332 [================>.............] - ETA: 0s228/332 [===================>..........] - ETA: 0s266/332 [=======================>......] - ETA: 0s305/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.12236596228568185
cosine 0.09349426599075512
MAE: 0.0484596
RMSE: 0.10345985
r2: 0.30623020788243527
RMSE zero-vector: 0.2430644284356365
['0.5custom_VAE', 'logcosh', 64, 60, 0.002, 0.6, 758, 0.00618774862959981, 0.0061617628671228886, 0.12236596228568185, 0.09349426599075512, 0.048459600657224655, 0.10345984995365143, 0.30623020788243527, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 210 0.001 32 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          479814      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          479814      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1860682     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 3,622,318
Trainable params: 3,618,274
Non-trainable params: 4,044
__________________________________________________________________________________________________
Epoch 1/210
2985/2985 - 13s - loss: 0.0192 - val_loss: 0.0141 - 13s/epoch - 5ms/step
Epoch 2/210
2985/2985 - 12s - loss: 0.0134 - val_loss: 0.0127 - 12s/epoch - 4ms/step
Epoch 3/210
2985/2985 - 12s - loss: 0.0128 - val_loss: 0.0124 - 12s/epoch - 4ms/step
Epoch 4/210
2985/2985 - 12s - loss: 0.0126 - val_loss: 0.0123 - 12s/epoch - 4ms/step
Epoch 5/210
2985/2985 - 12s - loss: 0.0125 - val_loss: 0.0123 - 12s/epoch - 4ms/step
Epoch 6/210
2985/2985 - 12s - loss: 0.0124 - val_loss: 0.0123 - 12s/epoch - 4ms/step
Epoch 7/210
2985/2985 - 12s - loss: 0.0123 - val_loss: 0.0123 - 12s/epoch - 4ms/step
Epoch 8/210
2985/2985 - 12s - loss: 0.0123 - val_loss: 0.0126 - 12s/epoch - 4ms/step
Epoch 9/210
2985/2985 - 12s - loss: 0.0123 - val_loss: 0.0128 - 12s/epoch - 4ms/step
Epoch 10/210
2985/2985 - 13s - loss: 0.0123 - val_loss: 0.0122 - 13s/epoch - 4ms/step
Epoch 11/210
2985/2985 - 12s - loss: 0.0122 - val_loss: 0.0126 - 12s/epoch - 4ms/step
Epoch 12/210
2985/2985 - 12s - loss: 0.0122 - val_loss: 0.0126 - 12s/epoch - 4ms/step
Epoch 13/210
2985/2985 - 12s - loss: 0.0122 - val_loss: 0.0124 - 12s/epoch - 4ms/step
Epoch 14/210
2985/2985 - 12s - loss: 0.0122 - val_loss: 0.0130 - 12s/epoch - 4ms/step
Epoch 15/210
2985/2985 - 12s - loss: 0.0121 - val_loss: 0.0122 - 12s/epoch - 4ms/step
Epoch 16/210
2985/2985 - 12s - loss: 0.0119 - val_loss: 0.0121 - 12s/epoch - 4ms/step
Epoch 17/210
2985/2985 - 12s - loss: 0.0117 - val_loss: 0.0118 - 12s/epoch - 4ms/step
Epoch 18/210
2985/2985 - 12s - loss: 0.0115 - val_loss: 0.0120 - 12s/epoch - 4ms/step
Epoch 19/210
2985/2985 - 12s - loss: 0.0115 - val_loss: 0.0123 - 12s/epoch - 4ms/step
Epoch 20/210
2985/2985 - 13s - loss: 0.0114 - val_loss: 0.0125 - 13s/epoch - 4ms/step
Epoch 21/210
2985/2985 - 12s - loss: 0.0114 - val_loss: 0.0122 - 12s/epoch - 4ms/step
Epoch 22/210
2985/2985 - 12s - loss: 0.0114 - val_loss: 0.0130 - 12s/epoch - 4ms/step
Epoch 23/210
2985/2985 - 13s - loss: 0.0113 - val_loss: 0.0121 - 13s/epoch - 4ms/step
Epoch 24/210
2985/2985 - 12s - loss: 0.0113 - val_loss: 0.0119 - 12s/epoch - 4ms/step
Epoch 25/210
2985/2985 - 12s - loss: 0.0113 - val_loss: 0.0122 - 12s/epoch - 4ms/step
Epoch 26/210
2985/2985 - 12s - loss: 0.0113 - val_loss: 0.0128 - 12s/epoch - 4ms/step
Epoch 27/210
2985/2985 - 12s - loss: 0.0113 - val_loss: 0.0128 - 12s/epoch - 4ms/step
Epoch 28/210
2985/2985 - 12s - loss: 0.0113 - val_loss: 0.0124 - 12s/epoch - 4ms/step
Epoch 29/210
2985/2985 - 12s - loss: 0.0113 - val_loss: 0.0129 - 12s/epoch - 4ms/step
Epoch 30/210
2985/2985 - 12s - loss: 0.0112 - val_loss: 0.0158 - 12s/epoch - 4ms/step
Epoch 31/210
2985/2985 - 12s - loss: 0.0112 - val_loss: 0.0123 - 12s/epoch - 4ms/step
Epoch 32/210
2985/2985 - 12s - loss: 0.0112 - val_loss: 0.0120 - 12s/epoch - 4ms/step
Epoch 33/210
2985/2985 - 12s - loss: 0.0112 - val_loss: 0.0121 - 12s/epoch - 4ms/step
Epoch 34/210
2985/2985 - 13s - loss: 0.0112 - val_loss: 0.0135 - 13s/epoch - 4ms/step
Epoch 35/210
2985/2985 - 12s - loss: 0.0111 - val_loss: 0.0128 - 12s/epoch - 4ms/step
Epoch 36/210
2985/2985 - 12s - loss: 0.0111 - val_loss: 0.0126 - 12s/epoch - 4ms/step
Epoch 37/210
2985/2985 - 13s - loss: 0.0110 - val_loss: 0.0125 - 13s/epoch - 4ms/step
Epoch 38/210
2985/2985 - 12s - loss: 0.0110 - val_loss: 0.0129 - 12s/epoch - 4ms/step
Epoch 39/210
2985/2985 - 12s - loss: 0.0110 - val_loss: 0.0133 - 12s/epoch - 4ms/step
Epoch 40/210
2985/2985 - 13s - loss: 0.0109 - val_loss: 0.0116 - 13s/epoch - 4ms/step
Epoch 41/210
2985/2985 - 12s - loss: 0.0109 - val_loss: 0.0141 - 12s/epoch - 4ms/step
Epoch 42/210
2985/2985 - 12s - loss: 0.0109 - val_loss: 0.0143 - 12s/epoch - 4ms/step
Epoch 43/210
2985/2985 - 13s - loss: 0.0109 - val_loss: 0.0134 - 13s/epoch - 4ms/step
Epoch 44/210
2985/2985 - 12s - loss: 0.0108 - val_loss: 0.0118 - 12s/epoch - 4ms/step
Epoch 45/210
2985/2985 - 12s - loss: 0.0108 - val_loss: 0.0142 - 12s/epoch - 4ms/step
Epoch 46/210
2985/2985 - 12s - loss: 0.0108 - val_loss: 0.0145 - 12s/epoch - 4ms/step
Epoch 47/210
2985/2985 - 13s - loss: 0.0108 - val_loss: 0.0124 - 13s/epoch - 4ms/step
Epoch 48/210
2985/2985 - 12s - loss: 0.0108 - val_loss: 0.0146 - 12s/epoch - 4ms/step
Epoch 49/210
2985/2985 - 12s - loss: 0.0107 - val_loss: 0.0135 - 12s/epoch - 4ms/step
Epoch 50/210
2985/2985 - 12s - loss: 0.0107 - val_loss: 0.0129 - 12s/epoch - 4ms/step
Epoch 51/210
2985/2985 - 13s - loss: 0.0107 - val_loss: 0.0138 - 13s/epoch - 4ms/step
Epoch 52/210
2985/2985 - 12s - loss: 0.0107 - val_loss: 0.0129 - 12s/epoch - 4ms/step
Epoch 53/210
2985/2985 - 12s - loss: 0.0107 - val_loss: 0.0130 - 12s/epoch - 4ms/step
Epoch 54/210
2985/2985 - 12s - loss: 0.0107 - val_loss: 0.0126 - 12s/epoch - 4ms/step
Epoch 55/210
2985/2985 - 12s - loss: 0.0107 - val_loss: 0.0134 - 12s/epoch - 4ms/step
Epoch 56/210
2985/2985 - 12s - loss: 0.0107 - val_loss: 0.0141 - 12s/epoch - 4ms/step
Epoch 57/210
2985/2985 - 13s - loss: 0.0107 - val_loss: 0.0131 - 13s/epoch - 4ms/step
Epoch 58/210
2985/2985 - 12s - loss: 0.0107 - val_loss: 0.0124 - 12s/epoch - 4ms/step
Epoch 59/210
2985/2985 - 12s - loss: 0.0106 - val_loss: 0.0134 - 12s/epoch - 4ms/step
Epoch 60/210
2985/2985 - 12s - loss: 0.0106 - val_loss: 0.0130 - 12s/epoch - 4ms/step
Epoch 61/210
2985/2985 - 13s - loss: 0.0106 - val_loss: 0.0131 - 13s/epoch - 4ms/step
Epoch 62/210
2985/2985 - 12s - loss: 0.0106 - val_loss: 0.0141 - 12s/epoch - 4ms/step
Epoch 63/210
2985/2985 - 12s - loss: 0.0106 - val_loss: 0.0130 - 12s/epoch - 4ms/step
Epoch 64/210
2985/2985 - 12s - loss: 0.0106 - val_loss: 0.0128 - 12s/epoch - 4ms/step
Epoch 65/210
2985/2985 - 12s - loss: 0.0106 - val_loss: 0.0126 - 12s/epoch - 4ms/step
Epoch 66/210
2985/2985 - 12s - loss: 0.0106 - val_loss: 0.0125 - 12s/epoch - 4ms/step
Epoch 67/210
2985/2985 - 12s - loss: 0.0106 - val_loss: 0.0131 - 12s/epoch - 4ms/step
Epoch 68/210
2985/2985 - 13s - loss: 0.0106 - val_loss: 0.0135 - 13s/epoch - 4ms/step
Epoch 69/210
2985/2985 - 12s - loss: 0.0106 - val_loss: 0.0127 - 12s/epoch - 4ms/step
Epoch 70/210
2985/2985 - 12s - loss: 0.0106 - val_loss: 0.0143 - 12s/epoch - 4ms/step
Epoch 71/210
2985/2985 - 12s - loss: 0.0106 - val_loss: 0.0130 - 12s/epoch - 4ms/step
Epoch 72/210
2985/2985 - 13s - loss: 0.0106 - val_loss: 0.0150 - 13s/epoch - 4ms/step
Epoch 73/210
2985/2985 - 12s - loss: 0.0106 - val_loss: 0.0136 - 12s/epoch - 4ms/step
Epoch 74/210
2985/2985 - 12s - loss: 0.0106 - val_loss: 0.0122 - 12s/epoch - 4ms/step
Epoch 75/210
2985/2985 - 12s - loss: 0.0106 - val_loss: 0.0131 - 12s/epoch - 4ms/step
Epoch 76/210
2985/2985 - 12s - loss: 0.0106 - val_loss: 0.0135 - 12s/epoch - 4ms/step
Epoch 77/210
2985/2985 - 12s - loss: 0.0105 - val_loss: 0.0141 - 12s/epoch - 4ms/step
Epoch 78/210
2985/2985 - 12s - loss: 0.0106 - val_loss: 0.0132 - 12s/epoch - 4ms/step
Epoch 79/210
2985/2985 - 13s - loss: 0.0106 - val_loss: 0.0121 - 13s/epoch - 4ms/step
Epoch 80/210
2985/2985 - 12s - loss: 0.0106 - val_loss: 0.0133 - 12s/epoch - 4ms/step
Epoch 81/210
2985/2985 - 12s - loss: 0.0105 - val_loss: 0.0125 - 12s/epoch - 4ms/step
Epoch 82/210
2985/2985 - 12s - loss: 0.0105 - val_loss: 0.0118 - 12s/epoch - 4ms/step
Epoch 83/210
2985/2985 - 13s - loss: 0.0105 - val_loss: 0.0128 - 13s/epoch - 4ms/step
Epoch 84/210
2985/2985 - 12s - loss: 0.0106 - val_loss: 0.0139 - 12s/epoch - 4ms/step
Epoch 85/210
2985/2985 - 12s - loss: 0.0105 - val_loss: 0.0131 - 12s/epoch - 4ms/step
Epoch 86/210
2985/2985 - 13s - loss: 0.0105 - val_loss: 0.0117 - 13s/epoch - 4ms/step
Epoch 87/210
2985/2985 - 12s - loss: 0.0105 - val_loss: 0.0141 - 12s/epoch - 4ms/step
Epoch 88/210
2985/2985 - 12s - loss: 0.0105 - val_loss: 0.0123 - 12s/epoch - 4ms/step
Epoch 89/210
2985/2985 - 13s - loss: 0.0105 - val_loss: 0.0130 - 13s/epoch - 4ms/step
Epoch 90/210
2985/2985 - 12s - loss: 0.0105 - val_loss: 0.0125 - 12s/epoch - 4ms/step
Epoch 91/210
2985/2985 - 12s - loss: 0.0105 - val_loss: 0.0130 - 12s/epoch - 4ms/step
Epoch 92/210
2985/2985 - 13s - loss: 0.0105 - val_loss: 0.0124 - 13s/epoch - 4ms/step
Epoch 93/210
2985/2985 - 13s - loss: 0.0105 - val_loss: 0.0120 - 13s/epoch - 4ms/step
Epoch 94/210
2985/2985 - 12s - loss: 0.0105 - val_loss: 0.0148 - 12s/epoch - 4ms/step
Epoch 95/210
2985/2985 - 12s - loss: 0.0105 - val_loss: 0.0125 - 12s/epoch - 4ms/step
Epoch 96/210
2985/2985 - 12s - loss: 0.0105 - val_loss: 0.0132 - 12s/epoch - 4ms/step
Epoch 97/210
2985/2985 - 12s - loss: 0.0105 - val_loss: 0.0117 - 12s/epoch - 4ms/step
Epoch 98/210
2985/2985 - 12s - loss: 0.0105 - val_loss: 0.0143 - 12s/epoch - 4ms/step
Epoch 99/210
2985/2985 - 13s - loss: 0.0105 - val_loss: 0.0129 - 13s/epoch - 4ms/step
Epoch 100/210
2985/2985 - 13s - loss: 0.0105 - val_loss: 0.0137 - 13s/epoch - 4ms/step
Epoch 101/210
2985/2985 - 12s - loss: 0.0105 - val_loss: 0.0117 - 12s/epoch - 4ms/step
Epoch 102/210
2985/2985 - 12s - loss: 0.0105 - val_loss: 0.0140 - 12s/epoch - 4ms/step
Epoch 103/210
2985/2985 - 12s - loss: 0.0105 - val_loss: 0.0122 - 12s/epoch - 4ms/step
Epoch 104/210
2985/2985 - 12s - loss: 0.0105 - val_loss: 0.0127 - 12s/epoch - 4ms/step
Epoch 105/210
2985/2985 - 12s - loss: 0.0105 - val_loss: 0.0141 - 12s/epoch - 4ms/step
Epoch 106/210
2985/2985 - 12s - loss: 0.0105 - val_loss: 0.0139 - 12s/epoch - 4ms/step
Epoch 107/210
2985/2985 - 12s - loss: 0.0105 - val_loss: 0.0142 - 12s/epoch - 4ms/step
Epoch 108/210
2985/2985 - 12s - loss: 0.0105 - val_loss: 0.0136 - 12s/epoch - 4ms/step
Epoch 109/210
2985/2985 - 12s - loss: 0.0105 - val_loss: 0.0120 - 12s/epoch - 4ms/step
Epoch 110/210
2985/2985 - 12s - loss: 0.0105 - val_loss: 0.0132 - 12s/epoch - 4ms/step
Epoch 111/210
2985/2985 - 13s - loss: 0.0105 - val_loss: 0.0136 - 13s/epoch - 4ms/step
Epoch 112/210
2985/2985 - 12s - loss: 0.0104 - val_loss: 0.0136 - 12s/epoch - 4ms/step
Epoch 113/210
2985/2985 - 12s - loss: 0.0104 - val_loss: 0.0117 - 12s/epoch - 4ms/step
Epoch 114/210
2985/2985 - 12s - loss: 0.0104 - val_loss: 0.0123 - 12s/epoch - 4ms/step
Epoch 115/210
2985/2985 - 12s - loss: 0.0104 - val_loss: 0.0127 - 12s/epoch - 4ms/step
Epoch 116/210
2985/2985 - 12s - loss: 0.0104 - val_loss: 0.0133 - 12s/epoch - 4ms/step
Epoch 117/210
2985/2985 - 12s - loss: 0.0104 - val_loss: 0.0134 - 12s/epoch - 4ms/step
Epoch 118/210
2985/2985 - 12s - loss: 0.0104 - val_loss: 0.0130 - 12s/epoch - 4ms/step
Epoch 119/210
2985/2985 - 12s - loss: 0.0104 - val_loss: 0.0121 - 12s/epoch - 4ms/step
Epoch 120/210
2985/2985 - 12s - loss: 0.0104 - val_loss: 0.0126 - 12s/epoch - 4ms/step
Epoch 121/210
2985/2985 - 12s - loss: 0.0104 - val_loss: 0.0131 - 12s/epoch - 4ms/step
Epoch 122/210
2985/2985 - 13s - loss: 0.0104 - val_loss: 0.0144 - 13s/epoch - 4ms/step
Epoch 123/210
2985/2985 - 12s - loss: 0.0104 - val_loss: 0.0133 - 12s/epoch - 4ms/step
Epoch 124/210
2985/2985 - 12s - loss: 0.0104 - val_loss: 0.0133 - 12s/epoch - 4ms/step
Epoch 125/210
2985/2985 - 12s - loss: 0.0104 - val_loss: 0.0125 - 12s/epoch - 4ms/step
Epoch 126/210
2985/2985 - 12s - loss: 0.0104 - val_loss: 0.0125 - 12s/epoch - 4ms/step
Epoch 127/210
2985/2985 - 12s - loss: 0.0104 - val_loss: 0.0124 - 12s/epoch - 4ms/step
Epoch 128/210
2985/2985 - 12s - loss: 0.0103 - val_loss: 0.0142 - 12s/epoch - 4ms/step
Epoch 129/210
2985/2985 - 12s - loss: 0.0103 - val_loss: 0.0130 - 12s/epoch - 4ms/step
Epoch 130/210
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0144 - 13s/epoch - 4ms/step
Epoch 131/210
2985/2985 - 12s - loss: 0.0103 - val_loss: 0.0138 - 12s/epoch - 4ms/step
Epoch 132/210
2985/2985 - 12s - loss: 0.0103 - val_loss: 0.0124 - 12s/epoch - 4ms/step
Epoch 133/210
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0141 - 13s/epoch - 4ms/step
Epoch 134/210
2985/2985 - 12s - loss: 0.0103 - val_loss: 0.0120 - 12s/epoch - 4ms/step
Epoch 135/210
2985/2985 - 12s - loss: 0.0103 - val_loss: 0.0129 - 12s/epoch - 4ms/step
Epoch 136/210
2985/2985 - 12s - loss: 0.0103 - val_loss: 0.0132 - 12s/epoch - 4ms/step
Epoch 137/210
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0143 - 13s/epoch - 4ms/step
Epoch 138/210
2985/2985 - 12s - loss: 0.0103 - val_loss: 0.0141 - 12s/epoch - 4ms/step
Epoch 139/210
2985/2985 - 12s - loss: 0.0103 - val_loss: 0.0132 - 12s/epoch - 4ms/step
Epoch 140/210
2985/2985 - 12s - loss: 0.0103 - val_loss: 0.0154 - 12s/epoch - 4ms/step
Epoch 141/210
2985/2985 - 12s - loss: 0.0103 - val_loss: 0.0144 - 12s/epoch - 4ms/step
Epoch 142/210
2985/2985 - 12s - loss: 0.0103 - val_loss: 0.0153 - 12s/epoch - 4ms/step
Epoch 143/210
2985/2985 - 12s - loss: 0.0103 - val_loss: 0.0136 - 12s/epoch - 4ms/step
Epoch 144/210
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0121 - 13s/epoch - 4ms/step
Epoch 145/210
2985/2985 - 12s - loss: 0.0103 - val_loss: 0.0132 - 12s/epoch - 4ms/step
Epoch 146/210
2985/2985 - 12s - loss: 0.0103 - val_loss: 0.0122 - 12s/epoch - 4ms/step
Epoch 147/210
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0134 - 13s/epoch - 4ms/step
Epoch 148/210
2985/2985 - 12s - loss: 0.0103 - val_loss: 0.0123 - 12s/epoch - 4ms/step
Epoch 149/210
2985/2985 - 12s - loss: 0.0103 - val_loss: 0.0112 - 12s/epoch - 4ms/step
Epoch 150/210
2985/2985 - 12s - loss: 0.0103 - val_loss: 0.0121 - 12s/epoch - 4ms/step
Epoch 151/210
2985/2985 - 12s - loss: 0.0103 - val_loss: 0.0134 - 12s/epoch - 4ms/step
Epoch 152/210
2985/2985 - 12s - loss: 0.0103 - val_loss: 0.0132 - 12s/epoch - 4ms/step
Epoch 153/210
2985/2985 - 12s - loss: 0.0103 - val_loss: 0.0127 - 12s/epoch - 4ms/step
Epoch 154/210
2985/2985 - 12s - loss: 0.0103 - val_loss: 0.0123 - 12s/epoch - 4ms/step
Epoch 155/210
2985/2985 - 12s - loss: 0.0103 - val_loss: 0.0114 - 12s/epoch - 4ms/step
Epoch 156/210
2985/2985 - 12s - loss: 0.0103 - val_loss: 0.0120 - 12s/epoch - 4ms/step
Epoch 157/210
2985/2985 - 12s - loss: 0.0103 - val_loss: 0.0117 - 12s/epoch - 4ms/step
Epoch 158/210
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0122 - 13s/epoch - 4ms/step
Epoch 159/210
2985/2985 - 12s - loss: 0.0103 - val_loss: 0.0127 - 12s/epoch - 4ms/step
Epoch 160/210
2985/2985 - 12s - loss: 0.0103 - val_loss: 0.0119 - 12s/epoch - 4ms/step
Epoch 161/210
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0111 - 13s/epoch - 4ms/step
Epoch 162/210
2985/2985 - 13s - loss: 0.0103 - val_loss: 0.0118 - 13s/epoch - 4ms/step
Epoch 163/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0112 - 12s/epoch - 4ms/step
Epoch 164/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0120 - 12s/epoch - 4ms/step
Epoch 165/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0111 - 12s/epoch - 4ms/step
Epoch 166/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0137 - 12s/epoch - 4ms/step
Epoch 167/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0116 - 12s/epoch - 4ms/step
Epoch 168/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0110 - 12s/epoch - 4ms/step
Epoch 169/210
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0111 - 13s/epoch - 4ms/step
Epoch 170/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0116 - 12s/epoch - 4ms/step
Epoch 171/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0120 - 12s/epoch - 4ms/step
Epoch 172/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0114 - 12s/epoch - 4ms/step
Epoch 173/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0117 - 12s/epoch - 4ms/step
Epoch 174/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0111 - 12s/epoch - 4ms/step
Epoch 175/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0112 - 12s/epoch - 4ms/step
Epoch 176/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0129 - 12s/epoch - 4ms/step
Epoch 177/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0113 - 12s/epoch - 4ms/step
Epoch 178/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0114 - 12s/epoch - 4ms/step
Epoch 179/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0115 - 12s/epoch - 4ms/step
Epoch 180/210
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0111 - 13s/epoch - 4ms/step
Epoch 181/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0118 - 12s/epoch - 4ms/step
Epoch 182/210
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0117 - 13s/epoch - 4ms/step
Epoch 183/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0123 - 12s/epoch - 4ms/step
Epoch 184/210
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0111 - 13s/epoch - 4ms/step
Epoch 185/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0116 - 12s/epoch - 4ms/step
Epoch 186/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0113 - 12s/epoch - 4ms/step
Epoch 187/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0108 - 12s/epoch - 4ms/step
Epoch 188/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0105 - 12s/epoch - 4ms/step
Epoch 189/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0117 - 12s/epoch - 4ms/step
Epoch 190/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0114 - 12s/epoch - 4ms/step
Epoch 191/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0110 - 12s/epoch - 4ms/step
Epoch 192/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0124 - 12s/epoch - 4ms/step
Epoch 193/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0114 - 12s/epoch - 4ms/step
Epoch 194/210
2985/2985 - 13s - loss: 0.0102 - val_loss: 0.0117 - 13s/epoch - 4ms/step
Epoch 195/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0116 - 12s/epoch - 4ms/step
Epoch 196/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0107 - 12s/epoch - 4ms/step
Epoch 197/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0106 - 12s/epoch - 4ms/step
Epoch 198/210
2985/2985 - 12s - loss: 0.0102 - val_loss: 0.0121 - 12s/epoch - 4ms/step
Epoch 199/210
2985/2985 - 12s - loss: 0.0101 - val_loss: 0.0112 - 12s/epoch - 4ms/step
Epoch 200/210
2985/2985 - 12s - loss: 0.0101 - val_loss: 0.0110 - 12s/epoch - 4ms/step
Epoch 201/210
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0121 - 13s/epoch - 4ms/step
Epoch 202/210
2985/2985 - 12s - loss: 0.0101 - val_loss: 0.0111 - 12s/epoch - 4ms/step
Epoch 203/210
2985/2985 - 12s - loss: 0.0101 - val_loss: 0.0114 - 12s/epoch - 4ms/step
Epoch 204/210
2985/2985 - 12s - loss: 0.0101 - val_loss: 0.0109 - 12s/epoch - 4ms/step
Epoch 205/210
2985/2985 - 13s - loss: 0.0101 - val_loss: 0.0106 - 13s/epoch - 4ms/step
Epoch 206/210
2985/2985 - 12s - loss: 0.0101 - val_loss: 0.0110 - 12s/epoch - 4ms/step
Epoch 207/210
2985/2985 - 12s - loss: 0.0101 - val_loss: 0.0104 - 12s/epoch - 4ms/step
Epoch 208/210
2985/2985 - 12s - loss: 0.0101 - val_loss: 0.0103 - 12s/epoch - 4ms/step
Epoch 209/210
2985/2985 - 12s - loss: 0.0101 - val_loss: 0.0109 - 12s/epoch - 4ms/step
Epoch 210/210
2985/2985 - 12s - loss: 0.0101 - val_loss: 0.0112 - 12s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.011213410645723343
  1/332 [..............................] - ETA: 49s 37/332 [==>...........................] - ETA: 0s  75/332 [=====>........................] - ETA: 0s112/332 [=========>....................] - ETA: 0s149/332 [============>.................] - ETA: 0s186/332 [===============>..............] - ETA: 0s223/332 [===================>..........] - ETA: 0s260/332 [======================>.......] - ETA: 0s298/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.07957276442434669
cosine 0.061061607484901984
MAE: 0.039198194
RMSE: 0.09297162
r2: 0.4397612778219351
RMSE zero-vector: 0.2430644284356365
['0.5custom_VAE', 'mse', 32, 210, 0.001, 0.6, 758, 0.010139224119484425, 0.011213410645723343, 0.07957276442434669, 0.061061607484901984, 0.039198193699121475, 0.09297162294387817, 0.4397612778219351, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 30 0.001 256 2] 10
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Epoch 1/30
374/374 - 3s - loss: 0.0264 - val_loss: 0.0099 - 3s/epoch - 9ms/step
Epoch 2/30
374/374 - 2s - loss: 0.0085 - val_loss: 0.0097 - 2s/epoch - 5ms/step
Epoch 3/30
374/374 - 2s - loss: 0.0081 - val_loss: 0.0110 - 2s/epoch - 5ms/step
Epoch 4/30
374/374 - 2s - loss: 0.0079 - val_loss: 0.0118 - 2s/epoch - 5ms/step
Epoch 5/30
374/374 - 2s - loss: 0.0078 - val_loss: 0.0087 - 2s/epoch - 5ms/step
Epoch 6/30
374/374 - 2s - loss: 0.0076 - val_loss: 0.0498 - 2s/epoch - 5ms/step
Epoch 7/30
374/374 - 2s - loss: 0.0076 - val_loss: 0.0082 - 2s/epoch - 5ms/step
Epoch 8/30
374/374 - 2s - loss: 0.0073 - val_loss: 0.0112 - 2s/epoch - 5ms/step
Epoch 9/30
374/374 - 2s - loss: 0.0073 - val_loss: 0.0088 - 2s/epoch - 5ms/step
Epoch 10/30
374/374 - 2s - loss: 0.0071 - val_loss: 0.0074 - 2s/epoch - 5ms/step
Epoch 11/30
374/374 - 2s - loss: 0.0071 - val_loss: 0.0075 - 2s/epoch - 5ms/step
Epoch 12/30
374/374 - 2s - loss: 0.0070 - val_loss: 0.0072 - 2s/epoch - 5ms/step
Epoch 13/30
374/374 - 2s - loss: 0.0069 - val_loss: 0.0072 - 2s/epoch - 5ms/step
Epoch 14/30
374/374 - 2s - loss: 0.0069 - val_loss: 0.0149 - 2s/epoch - 5ms/step
Epoch 15/30
374/374 - 2s - loss: 0.0078 - val_loss: 0.0149 - 2s/epoch - 5ms/step
Epoch 16/30
374/374 - 2s - loss: 0.0078 - val_loss: 0.0066 - 2s/epoch - 5ms/step
Epoch 17/30
374/374 - 2s - loss: 0.0066 - val_loss: 0.0103 - 2s/epoch - 5ms/step
Epoch 18/30
374/374 - 2s - loss: 0.0083 - val_loss: 0.0071 - 2s/epoch - 5ms/step
Epoch 19/30
374/374 - 2s - loss: 0.0066 - val_loss: 0.0067 - 2s/epoch - 5ms/step
Epoch 20/30
374/374 - 2s - loss: 0.0067 - val_loss: 0.0064 - 2s/epoch - 5ms/step
Epoch 21/30
374/374 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 22/30
374/374 - 2s - loss: 0.0063 - val_loss: 0.0066 - 2s/epoch - 5ms/step
Epoch 23/30
374/374 - 2s - loss: 0.0066 - val_loss: 0.0067 - 2s/epoch - 5ms/step
Epoch 24/30
374/374 - 2s - loss: 0.0065 - val_loss: 0.0066 - 2s/epoch - 5ms/step
Epoch 25/30
374/374 - 2s - loss: 0.0065 - val_loss: 0.0063 - 2s/epoch - 5ms/step
Epoch 26/30
374/374 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 27/30
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 28/30
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 29/30
374/374 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 5ms/step
Epoch 30/30
374/374 - 2s - loss: 0.0062 - val_loss: 0.0063 - 2s/epoch - 5ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.006265397649258375
  1/332 [..............................] - ETA: 55s 37/332 [==>...........................] - ETA: 0s  74/332 [=====>........................] - ETA: 0s111/332 [=========>....................] - ETA: 0s149/332 [============>.................] - ETA: 0s187/332 [===============>..............] - ETA: 0s225/332 [===================>..........] - ETA: 0s262/332 [======================>.......] - ETA: 0s299/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.12754144419309016
cosine 0.09746507437576045
MAE: 0.050994843
RMSE: 0.1042996
r2: 0.2949303217919256
RMSE zero-vector: 0.2430644284356365
['1.5custom_VAE', 'logcosh', 256, 30, 0.001, 0.6, 758, 0.006219886243343353, 0.006265397649258375, 0.12754144419309016, 0.09746507437576045, 0.05099484324455261, 0.10429959744215012, 0.2949303217919256, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 60 0.001 16 1] 11
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1916982     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1916982     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5703874     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 12,745,870
Trainable params: 12,734,242
Non-trainable params: 11,628
__________________________________________________________________________________________________
Epoch 1/60
5969/5969 - 26s - loss: 0.0227 - val_loss: 0.0130 - 26s/epoch - 4ms/step
Epoch 2/60
5969/5969 - 24s - loss: 0.0131 - val_loss: 0.0128 - 24s/epoch - 4ms/step
Epoch 3/60
5969/5969 - 24s - loss: 0.0129 - val_loss: 0.0127 - 24s/epoch - 4ms/step
Epoch 4/60
5969/5969 - 25s - loss: 0.0128 - val_loss: 0.0147 - 25s/epoch - 4ms/step
Epoch 5/60
5969/5969 - 24s - loss: 0.0127 - val_loss: 0.0148 - 24s/epoch - 4ms/step
Epoch 6/60
5969/5969 - 25s - loss: 0.0126 - val_loss: 0.0153 - 25s/epoch - 4ms/step
Epoch 7/60
5969/5969 - 24s - loss: 0.0126 - val_loss: 0.0160 - 24s/epoch - 4ms/step
Epoch 8/60
5969/5969 - 24s - loss: 0.0125 - val_loss: 0.0209 - 24s/epoch - 4ms/step
Epoch 9/60
5969/5969 - 24s - loss: 0.0123 - val_loss: 0.0176 - 24s/epoch - 4ms/step
Epoch 10/60
5969/5969 - 24s - loss: 0.0120 - val_loss: 0.0232 - 24s/epoch - 4ms/step
Epoch 11/60
5969/5969 - 25s - loss: 0.0119 - val_loss: 0.0250 - 25s/epoch - 4ms/step
Epoch 12/60
5969/5969 - 24s - loss: 0.0118 - val_loss: 0.0307 - 24s/epoch - 4ms/step
Epoch 13/60
5969/5969 - 24s - loss: 0.0118 - val_loss: 0.0227 - 24s/epoch - 4ms/step
Epoch 14/60
5969/5969 - 24s - loss: 0.0118 - val_loss: 0.0351 - 24s/epoch - 4ms/step
Epoch 15/60
5969/5969 - 24s - loss: 0.0118 - val_loss: 0.0345 - 24s/epoch - 4ms/step
Epoch 16/60
5969/5969 - 25s - loss: 0.0117 - val_loss: 0.0350 - 25s/epoch - 4ms/step
Epoch 17/60
5969/5969 - 25s - loss: 0.0117 - val_loss: 0.0385 - 25s/epoch - 4ms/step
Epoch 18/60
5969/5969 - 24s - loss: 0.0117 - val_loss: 0.0349 - 24s/epoch - 4ms/step
Epoch 19/60
5969/5969 - 24s - loss: 0.0117 - val_loss: 0.0568 - 24s/epoch - 4ms/step
Epoch 20/60
5969/5969 - 24s - loss: 0.0117 - val_loss: 0.0428 - 24s/epoch - 4ms/step
Epoch 21/60
5969/5969 - 25s - loss: 0.0117 - val_loss: 0.0506 - 25s/epoch - 4ms/step
Epoch 22/60
5969/5969 - 24s - loss: 0.0117 - val_loss: 0.0419 - 24s/epoch - 4ms/step
Epoch 23/60
5969/5969 - 25s - loss: 0.0116 - val_loss: 0.0466 - 25s/epoch - 4ms/step
Epoch 24/60
5969/5969 - 24s - loss: 0.0116 - val_loss: 0.0413 - 24s/epoch - 4ms/step
Epoch 25/60
5969/5969 - 25s - loss: 0.0116 - val_loss: 0.0404 - 25s/epoch - 4ms/step
Epoch 26/60
5969/5969 - 24s - loss: 0.0116 - val_loss: 0.0537 - 24s/epoch - 4ms/step
Epoch 27/60
5969/5969 - 24s - loss: 0.0116 - val_loss: 0.0618 - 24s/epoch - 4ms/step
Epoch 28/60
5969/5969 - 24s - loss: 0.0116 - val_loss: 0.0597 - 24s/epoch - 4ms/step
Epoch 29/60
5969/5969 - 24s - loss: 0.0116 - val_loss: 0.0502 - 24s/epoch - 4ms/step
Epoch 30/60
5969/5969 - 24s - loss: 0.0116 - val_loss: 0.0832 - 24s/epoch - 4ms/step
Epoch 31/60
5969/5969 - 24s - loss: 0.0116 - val_loss: 0.0589 - 24s/epoch - 4ms/step
Epoch 32/60
5969/5969 - 24s - loss: 0.0116 - val_loss: 0.1000 - 24s/epoch - 4ms/step
Epoch 33/60
5969/5969 - 24s - loss: 0.0116 - val_loss: 0.0557 - 24s/epoch - 4ms/step
Epoch 34/60
5969/5969 - 24s - loss: 0.0116 - val_loss: 0.0519 - 24s/epoch - 4ms/step
Epoch 35/60
5969/5969 - 25s - loss: 0.0116 - val_loss: 0.0652 - 25s/epoch - 4ms/step
Epoch 36/60
5969/5969 - 25s - loss: 0.0116 - val_loss: 0.0812 - 25s/epoch - 4ms/step
Epoch 37/60
5969/5969 - 24s - loss: 0.0116 - val_loss: 0.0794 - 24s/epoch - 4ms/step
Epoch 38/60
5969/5969 - 24s - loss: 0.0115 - val_loss: 0.1114 - 24s/epoch - 4ms/step
Epoch 39/60
5969/5969 - 24s - loss: 0.0115 - val_loss: 0.0717 - 24s/epoch - 4ms/step
Epoch 40/60
5969/5969 - 25s - loss: 0.0115 - val_loss: 0.0551 - 25s/epoch - 4ms/step
Epoch 41/60
5969/5969 - 25s - loss: 0.0115 - val_loss: 0.1134 - 25s/epoch - 4ms/step
Epoch 42/60
5969/5969 - 24s - loss: 0.0114 - val_loss: 0.0869 - 24s/epoch - 4ms/step
Epoch 43/60
5969/5969 - 24s - loss: 0.0114 - val_loss: 0.0954 - 24s/epoch - 4ms/step
Epoch 44/60
5969/5969 - 24s - loss: 0.0114 - val_loss: 0.0815 - 24s/epoch - 4ms/step
Epoch 45/60
5969/5969 - 24s - loss: 0.0114 - val_loss: 0.1095 - 24s/epoch - 4ms/step
Epoch 46/60
5969/5969 - 24s - loss: 0.0114 - val_loss: 0.1202 - 24s/epoch - 4ms/step
Epoch 47/60
5969/5969 - 24s - loss: 0.0114 - val_loss: 0.1132 - 24s/epoch - 4ms/step
Epoch 48/60
5969/5969 - 24s - loss: 0.0114 - val_loss: 0.1359 - 24s/epoch - 4ms/step
Epoch 49/60
5969/5969 - 24s - loss: 0.0113 - val_loss: 0.1080 - 24s/epoch - 4ms/step
Epoch 50/60
5969/5969 - 25s - loss: 0.0113 - val_loss: 0.1416 - 25s/epoch - 4ms/step
Epoch 51/60
5969/5969 - 25s - loss: 0.0113 - val_loss: 0.0908 - 25s/epoch - 4ms/step
Epoch 52/60
5969/5969 - 25s - loss: 0.0113 - val_loss: 0.1385 - 25s/epoch - 4ms/step
Epoch 53/60
5969/5969 - 24s - loss: 0.0113 - val_loss: 0.1135 - 24s/epoch - 4ms/step
Epoch 54/60
5969/5969 - 24s - loss: 0.0113 - val_loss: 0.0890 - 24s/epoch - 4ms/step
Epoch 55/60
5969/5969 - 24s - loss: 0.0113 - val_loss: 0.1247 - 24s/epoch - 4ms/step
Epoch 56/60
5969/5969 - 24s - loss: 0.0113 - val_loss: 0.1250 - 24s/epoch - 4ms/step
Epoch 57/60
5969/5969 - 24s - loss: 0.0113 - val_loss: 0.0869 - 24s/epoch - 4ms/step
Epoch 58/60
5969/5969 - 24s - loss: 0.0113 - val_loss: 0.1052 - 24s/epoch - 4ms/step
Epoch 59/60
5969/5969 - 25s - loss: 0.0113 - val_loss: 0.0831 - 25s/epoch - 4ms/step
Epoch 60/60
5969/5969 - 24s - loss: 0.0113 - val_loss: 0.1197 - 24s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.11974385380744934
  1/332 [..............................] - ETA: 49s 37/332 [==>...........................] - ETA: 0s  74/332 [=====>........................] - ETA: 0s 96/332 [=======>......................] - ETA: 0s133/332 [===========>..................] - ETA: 0s170/332 [==============>...............] - ETA: 0s207/332 [=================>............] - ETA: 0s244/332 [=====================>........] - ETA: 0s281/332 [========================>.....] - ETA: 0s318/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.10799615753655203
cosine 0.08476992609464488
MAE: 0.05460593
RMSE: 0.34194535
r2: -6.578597115704696
RMSE zero-vector: 0.2430644284356365
['2.0custom_VAE', 'mse', 16, 60, 0.001, 0.6, 758, 0.011283774860203266, 0.11974385380744934, 0.10799615753655203, 0.08476992609464488, 0.05460593104362488, 0.3419453501701355, -6.578597115704696, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 150 0.001 64 2] 12
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Epoch 1/150
1493/1493 - 8s - loss: 0.0154 - val_loss: 0.0119 - 8s/epoch - 5ms/step
Epoch 2/150
1493/1493 - 6s - loss: 0.0082 - val_loss: 0.0079 - 6s/epoch - 4ms/step
Epoch 3/150
1493/1493 - 6s - loss: 0.0073 - val_loss: 0.0071 - 6s/epoch - 4ms/step
Epoch 4/150
1493/1493 - 6s - loss: 0.0070 - val_loss: 0.0068 - 6s/epoch - 4ms/step
Epoch 5/150
1493/1493 - 6s - loss: 0.0068 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 6/150
1493/1493 - 6s - loss: 0.0068 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 7/150
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0065 - 6s/epoch - 4ms/step
Epoch 8/150
1493/1493 - 6s - loss: 0.0064 - val_loss: 0.0063 - 6s/epoch - 4ms/step
Epoch 9/150
1493/1493 - 6s - loss: 0.0064 - val_loss: 0.0063 - 6s/epoch - 4ms/step
Epoch 10/150
1493/1493 - 6s - loss: 0.0063 - val_loss: 0.0063 - 6s/epoch - 4ms/step
Epoch 11/150
1493/1493 - 6s - loss: 0.0063 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 12/150
1493/1493 - 6s - loss: 0.0063 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 13/150
1493/1493 - 6s - loss: 0.0063 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 14/150
1493/1493 - 6s - loss: 0.0063 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 15/150
1493/1493 - 6s - loss: 0.0063 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 16/150
1493/1493 - 6s - loss: 0.0063 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 17/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 18/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 19/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 20/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 21/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 22/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 23/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 24/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 25/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 26/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 27/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 28/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 29/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 30/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 31/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 32/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 33/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 34/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 35/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 36/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 37/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 38/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 39/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 40/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 41/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 42/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 43/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 44/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 45/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 46/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 47/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 48/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 49/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 50/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 51/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 52/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 53/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 54/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 55/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 56/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 57/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 58/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 59/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 60/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 61/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 62/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 63/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 64/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 65/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 66/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 67/150
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 68/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 69/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 70/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 71/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 72/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 73/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 74/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 75/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 76/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 77/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 78/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 79/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 80/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 81/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 82/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 83/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 84/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 85/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 86/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 87/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 88/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 89/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 90/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 91/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 92/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 93/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 94/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 95/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 96/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 97/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 98/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 99/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 100/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 101/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 102/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 103/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 104/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 105/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 106/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 107/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 108/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 109/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 110/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 111/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 112/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 113/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 114/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 115/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 116/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 117/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 118/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 119/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 120/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 121/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 122/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 123/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 124/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 125/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 126/150
1493/1493 - 6s - loss: 0.0060 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 127/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 128/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 129/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 130/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 131/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 132/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 133/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 134/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 135/150
1493/1493 - 6s - loss: 0.0060 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 136/150
1493/1493 - 6s - loss: 0.0061 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 137/150
1493/1493 - 6s - loss: 0.0060 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 138/150
1493/1493 - 6s - loss: 0.0060 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 139/150
1493/1493 - 6s - loss: 0.0060 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 140/150
1493/1493 - 6s - loss: 0.0060 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 141/150
1493/1493 - 6s - loss: 0.0060 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 142/150
1493/1493 - 6s - loss: 0.0060 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 143/150
1493/1493 - 6s - loss: 0.0060 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 144/150
1493/1493 - 6s - loss: 0.0060 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 145/150
1493/1493 - 6s - loss: 0.0060 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 146/150
1493/1493 - 6s - loss: 0.0060 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 147/150
1493/1493 - 6s - loss: 0.0060 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 148/150
1493/1493 - 6s - loss: 0.0060 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 149/150
1493/1493 - 6s - loss: 0.0060 - val_loss: 0.0060 - 6s/epoch - 4ms/step
Epoch 150/150
1493/1493 - 6s - loss: 0.0060 - val_loss: 0.0060 - 6s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.006012223195284605
  1/332 [..............................] - ETA: 54s 38/332 [==>...........................] - ETA: 0s  76/332 [=====>........................] - ETA: 0s114/332 [=========>....................] - ETA: 0s152/332 [============>.................] - ETA: 0s190/332 [================>.............] - ETA: 0s228/332 [===================>..........] - ETA: 0s265/332 [======================>.......] - ETA: 0s303/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.11439736434032097
cosine 0.08738352722304747
MAE: 0.04622327
RMSE: 0.09959347
r2: 0.35711520363577837
RMSE zero-vector: 0.2430644284356365
['1.5custom_VAE', 'logcosh', 64, 150, 0.001, 0.6, 758, 0.006036986596882343, 0.006012223195284605, 0.11439736434032097, 0.08738352722304747, 0.046223271638154984, 0.09959346801042557, 0.35711520363577837, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.0 90 0.001 64 2] 13
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          958870      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          958870      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3141746     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 6,663,502
Trainable params: 6,656,930
Non-trainable params: 6,572
__________________________________________________________________________________________________
Epoch 1/90
1493/1493 - 7s - loss: 0.0135 - val_loss: 0.0082 - 7s/epoch - 5ms/step
Epoch 2/90
1493/1493 - 6s - loss: 0.0078 - val_loss: 0.0076 - 6s/epoch - 4ms/step
Epoch 3/90
1493/1493 - 6s - loss: 0.0072 - val_loss: 0.0070 - 6s/epoch - 4ms/step
Epoch 4/90
1493/1493 - 6s - loss: 0.0069 - val_loss: 0.0068 - 6s/epoch - 4ms/step
Epoch 5/90
1493/1493 - 6s - loss: 0.0068 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 6/90
1493/1493 - 6s - loss: 0.0068 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 7/90
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 8/90
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 9/90
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 10/90
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0066 - 6s/epoch - 4ms/step
Epoch 11/90
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 12/90
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0066 - 6s/epoch - 4ms/step
Epoch 13/90
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 14/90
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0066 - 6s/epoch - 4ms/step
Epoch 15/90
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 16/90
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0066 - 6s/epoch - 4ms/step
Epoch 17/90
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0066 - 6s/epoch - 4ms/step
Epoch 18/90
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0066 - 6s/epoch - 4ms/step
Epoch 19/90
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0066 - 6s/epoch - 4ms/step
Epoch 20/90
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0066 - 6s/epoch - 4ms/step
Epoch 21/90
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0066 - 6s/epoch - 4ms/step
Epoch 22/90
1493/1493 - 6s - loss: 0.0066 - val_loss: 0.0065 - 6s/epoch - 4ms/step
Epoch 23/90
1493/1493 - 6s - loss: 0.0065 - val_loss: 0.0063 - 6s/epoch - 4ms/step
Epoch 24/90
1493/1493 - 6s - loss: 0.0064 - val_loss: 0.0063 - 6s/epoch - 4ms/step
Epoch 25/90
1493/1493 - 6s - loss: 0.0063 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 26/90
1493/1493 - 6s - loss: 0.0063 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 27/90
1493/1493 - 6s - loss: 0.0063 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 28/90
1493/1493 - 6s - loss: 0.0063 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 29/90
1493/1493 - 6s - loss: 0.0063 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 30/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 31/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 32/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 33/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 34/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 35/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 36/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 37/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 38/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 39/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 40/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 41/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 42/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 43/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 44/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 45/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 46/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 47/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 48/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 49/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 50/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 51/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 52/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 53/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 54/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 55/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 56/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 57/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 58/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 59/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 60/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 61/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 62/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 63/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 64/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 65/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 66/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 67/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 68/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 69/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 70/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 71/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 72/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 73/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 74/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 75/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 76/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 77/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 78/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 79/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 80/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 81/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 82/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 83/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0063 - 6s/epoch - 4ms/step
Epoch 84/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 85/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 86/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 87/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 88/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 89/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 90/90
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.006131364032626152
  1/332 [..............................] - ETA: 50s 38/332 [==>...........................] - ETA: 0s  76/332 [=====>........................] - ETA: 0s114/332 [=========>....................] - ETA: 0s153/332 [============>.................] - ETA: 0s192/332 [================>.............] - ETA: 0s220/332 [==================>...........] - ETA: 0s258/332 [======================>.......] - ETA: 0s296/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.12151824517819372
cosine 0.09284249844525924
MAE: 0.048031177
RMSE: 0.10237249
r2: 0.32073685256658535
RMSE zero-vector: 0.2430644284356365
['1.0custom_VAE', 'logcosh', 64, 90, 0.001, 0.6, 758, 0.006162172649055719, 0.006131364032626152, 0.12151824517819372, 0.09284249844525924, 0.04803117737174034, 0.10237248986959457, 0.32073685256658535, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 150 0.0005 64 1] 14
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1437926     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4422810     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,704,686
Trainable params: 9,695,586
Non-trainable params: 9,100
__________________________________________________________________________________________________
Epoch 1/150
1493/1493 - 8s - loss: 0.0267 - val_loss: 0.0248 - 8s/epoch - 5ms/step
Epoch 2/150
1493/1493 - 6s - loss: 0.0152 - val_loss: 0.0141 - 6s/epoch - 4ms/step
Epoch 3/150
1493/1493 - 6s - loss: 0.0136 - val_loss: 0.0130 - 6s/epoch - 4ms/step
Epoch 4/150
1493/1493 - 6s - loss: 0.0129 - val_loss: 0.0125 - 6s/epoch - 4ms/step
Epoch 5/150
1493/1493 - 6s - loss: 0.0126 - val_loss: 0.0123 - 6s/epoch - 4ms/step
Epoch 6/150
1493/1493 - 6s - loss: 0.0124 - val_loss: 0.0121 - 6s/epoch - 4ms/step
Epoch 7/150
1493/1493 - 6s - loss: 0.0123 - val_loss: 0.0121 - 6s/epoch - 4ms/step
Epoch 8/150
1493/1493 - 6s - loss: 0.0122 - val_loss: 0.0120 - 6s/epoch - 4ms/step
Epoch 9/150
1493/1493 - 6s - loss: 0.0121 - val_loss: 0.0119 - 6s/epoch - 4ms/step
Epoch 10/150
1493/1493 - 6s - loss: 0.0120 - val_loss: 0.0117 - 6s/epoch - 4ms/step
Epoch 11/150
1493/1493 - 6s - loss: 0.0117 - val_loss: 0.0114 - 6s/epoch - 4ms/step
Epoch 12/150
1493/1493 - 6s - loss: 0.0115 - val_loss: 0.0113 - 6s/epoch - 4ms/step
Epoch 13/150
1493/1493 - 6s - loss: 0.0114 - val_loss: 0.0111 - 6s/epoch - 4ms/step
Epoch 14/150
1493/1493 - 6s - loss: 0.0113 - val_loss: 0.0110 - 6s/epoch - 4ms/step
Epoch 15/150
1493/1493 - 6s - loss: 0.0112 - val_loss: 0.0109 - 6s/epoch - 4ms/step
Epoch 16/150
1493/1493 - 6s - loss: 0.0111 - val_loss: 0.0108 - 6s/epoch - 4ms/step
Epoch 17/150
1493/1493 - 6s - loss: 0.0110 - val_loss: 0.0108 - 6s/epoch - 4ms/step
Epoch 18/150
1493/1493 - 6s - loss: 0.0110 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 19/150
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 20/150
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0106 - 6s/epoch - 4ms/step
Epoch 21/150
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0106 - 6s/epoch - 4ms/step
Epoch 22/150
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0106 - 6s/epoch - 4ms/step
Epoch 23/150
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0105 - 6s/epoch - 4ms/step
Epoch 24/150
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0106 - 6s/epoch - 4ms/step
Epoch 25/150
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0105 - 6s/epoch - 4ms/step
Epoch 26/150
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0105 - 6s/epoch - 4ms/step
Epoch 27/150
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0105 - 6s/epoch - 4ms/step
Epoch 28/150
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0105 - 6s/epoch - 4ms/step
Epoch 29/150
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 30/150
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 31/150
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 32/150
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 33/150
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 34/150
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0105 - 6s/epoch - 4ms/step
Epoch 35/150
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 36/150
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 37/150
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 38/150
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 39/150
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 40/150
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 41/150
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 42/150
1493/1493 - 7s - loss: 0.0106 - val_loss: 0.0103 - 7s/epoch - 4ms/step
Epoch 43/150
1493/1493 - 7s - loss: 0.0105 - val_loss: 0.0103 - 7s/epoch - 4ms/step
Epoch 44/150
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 45/150
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 46/150
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 47/150
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 48/150
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 49/150
1493/1493 - 7s - loss: 0.0105 - val_loss: 0.0103 - 7s/epoch - 4ms/step
Epoch 50/150
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 51/150
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 52/150
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 53/150
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 54/150
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 55/150
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 56/150
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 57/150
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 58/150
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 59/150
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 60/150
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 61/150
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 62/150
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 63/150
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 64/150
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 65/150
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 66/150
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 67/150
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 68/150
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 69/150
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 70/150
1493/1493 - 7s - loss: 0.0104 - val_loss: 0.0102 - 7s/epoch - 4ms/step
Epoch 71/150
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 72/150
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 73/150
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 74/150
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 75/150
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 76/150
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 77/150
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 78/150
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 79/150
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 80/150
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 81/150
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 82/150
1493/1493 - 7s - loss: 0.0104 - val_loss: 0.0102 - 7s/epoch - 4ms/step
Epoch 83/150
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 84/150
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 85/150
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 86/150
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 87/150
1493/1493 - 6s - loss: 0.0103 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 88/150
1493/1493 - 6s - loss: 0.0103 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 89/150
1493/1493 - 6s - loss: 0.0103 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 90/150
1493/1493 - 6s - loss: 0.0103 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 91/150
1493/1493 - 6s - loss: 0.0103 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 92/150
1493/1493 - 6s - loss: 0.0103 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 93/150
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 94/150
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 95/150
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 96/150
1493/1493 - 7s - loss: 0.0102 - val_loss: 0.0100 - 7s/epoch - 4ms/step
Epoch 97/150
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 98/150
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 99/150
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 100/150
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 101/150
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 102/150
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 103/150
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 104/150
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 105/150
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 106/150
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 107/150
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 108/150
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 109/150
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 110/150
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 111/150
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 112/150
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 113/150
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 114/150
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 115/150
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 116/150
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 117/150
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 118/150
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 119/150
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 120/150
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 121/150
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 122/150
1493/1493 - 7s - loss: 0.0100 - val_loss: 0.0098 - 7s/epoch - 4ms/step
Epoch 123/150
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 124/150
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 125/150
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 126/150
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 127/150
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 128/150
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 129/150
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 130/150
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 131/150
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 132/150
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 133/150
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 134/150
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 135/150
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 136/150
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 137/150
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 138/150
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 139/150
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 140/150
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 141/150
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 142/150
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 143/150
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0096 - 7s/epoch - 4ms/step
Epoch 144/150
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 145/150
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 146/150
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 147/150
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 148/150
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 149/150
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 150/150
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009645297192037106
  1/332 [..............................] - ETA: 49s 37/332 [==>...........................] - ETA: 0s  74/332 [=====>........................] - ETA: 0s111/332 [=========>....................] - ETA: 0s148/332 [============>.................] - ETA: 0s185/332 [===============>..............] - ETA: 0s222/332 [===================>..........] - ETA: 0s259/332 [======================>.......] - ETA: 0s296/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.07662582700763879
cosine 0.058736564335264085
MAE: 0.037740443
RMSE: 0.08167534
r2: 0.5676324345982107
RMSE zero-vector: 0.2430644284356365
['1.5custom_VAE', 'mse', 64, 150, 0.0005, 0.6, 758, 0.009892446920275688, 0.009645297192037106, 0.07662582700763879, 0.058736564335264085, 0.03774044290184975, 0.08167534321546555, 0.5676324345982107, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 60 0.00030000000000000003 64 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          479814      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          479814      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1860682     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 3,622,318
Trainable params: 3,618,274
Non-trainable params: 4,044
__________________________________________________________________________________________________
Epoch 1/60
1493/1493 - 8s - loss: 0.0209 - val_loss: 0.0156 - 8s/epoch - 5ms/step
Epoch 2/60
1493/1493 - 6s - loss: 0.0142 - val_loss: 0.0144 - 6s/epoch - 4ms/step
Epoch 3/60
1493/1493 - 6s - loss: 0.0134 - val_loss: 0.0130 - 6s/epoch - 4ms/step
Epoch 4/60
1493/1493 - 6s - loss: 0.0129 - val_loss: 0.0126 - 6s/epoch - 4ms/step
Epoch 5/60
1493/1493 - 6s - loss: 0.0126 - val_loss: 0.0124 - 6s/epoch - 4ms/step
Epoch 6/60
1493/1493 - 7s - loss: 0.0125 - val_loss: 0.0122 - 7s/epoch - 4ms/step
Epoch 7/60
1493/1493 - 6s - loss: 0.0124 - val_loss: 0.0121 - 6s/epoch - 4ms/step
Epoch 8/60
1493/1493 - 6s - loss: 0.0123 - val_loss: 0.0121 - 6s/epoch - 4ms/step
Epoch 9/60
1493/1493 - 6s - loss: 0.0122 - val_loss: 0.0120 - 6s/epoch - 4ms/step
Epoch 10/60
1493/1493 - 6s - loss: 0.0122 - val_loss: 0.0120 - 6s/epoch - 4ms/step
Epoch 11/60
1493/1493 - 6s - loss: 0.0121 - val_loss: 0.0120 - 6s/epoch - 4ms/step
Epoch 12/60
1493/1493 - 6s - loss: 0.0121 - val_loss: 0.0118 - 6s/epoch - 4ms/step
Epoch 13/60
1493/1493 - 7s - loss: 0.0121 - val_loss: 0.0119 - 7s/epoch - 4ms/step
Epoch 14/60
1493/1493 - 6s - loss: 0.0121 - val_loss: 0.0119 - 6s/epoch - 4ms/step
Epoch 15/60
1493/1493 - 7s - loss: 0.0120 - val_loss: 0.0119 - 7s/epoch - 4ms/step
Epoch 16/60
1493/1493 - 6s - loss: 0.0120 - val_loss: 0.0119 - 6s/epoch - 4ms/step
Epoch 17/60
1493/1493 - 6s - loss: 0.0120 - val_loss: 0.0118 - 6s/epoch - 4ms/step
Epoch 18/60
1493/1493 - 6s - loss: 0.0120 - val_loss: 0.0118 - 6s/epoch - 4ms/step
Epoch 19/60
1493/1493 - 7s - loss: 0.0120 - val_loss: 0.0118 - 7s/epoch - 4ms/step
Epoch 20/60
1493/1493 - 6s - loss: 0.0119 - val_loss: 0.0118 - 6s/epoch - 4ms/step
Epoch 21/60
1493/1493 - 6s - loss: 0.0119 - val_loss: 0.0117 - 6s/epoch - 4ms/step
Epoch 22/60
1493/1493 - 6s - loss: 0.0119 - val_loss: 0.0117 - 6s/epoch - 4ms/step
Epoch 23/60
1493/1493 - 6s - loss: 0.0119 - val_loss: 0.0118 - 6s/epoch - 4ms/step
Epoch 24/60
1493/1493 - 6s - loss: 0.0119 - val_loss: 0.0118 - 6s/epoch - 4ms/step
Epoch 25/60
1493/1493 - 6s - loss: 0.0118 - val_loss: 0.0116 - 6s/epoch - 4ms/step
Epoch 26/60
1493/1493 - 6s - loss: 0.0117 - val_loss: 0.0115 - 6s/epoch - 4ms/step
Epoch 27/60
1493/1493 - 7s - loss: 0.0115 - val_loss: 0.0112 - 7s/epoch - 4ms/step
Epoch 28/60
1493/1493 - 6s - loss: 0.0114 - val_loss: 0.0112 - 6s/epoch - 4ms/step
Epoch 29/60
1493/1493 - 6s - loss: 0.0113 - val_loss: 0.0111 - 6s/epoch - 4ms/step
Epoch 30/60
1493/1493 - 6s - loss: 0.0112 - val_loss: 0.0110 - 6s/epoch - 4ms/step
Epoch 31/60
1493/1493 - 6s - loss: 0.0112 - val_loss: 0.0110 - 6s/epoch - 4ms/step
Epoch 32/60
1493/1493 - 6s - loss: 0.0111 - val_loss: 0.0109 - 6s/epoch - 4ms/step
Epoch 33/60
1493/1493 - 6s - loss: 0.0111 - val_loss: 0.0109 - 6s/epoch - 4ms/step
Epoch 34/60
1493/1493 - 6s - loss: 0.0111 - val_loss: 0.0110 - 6s/epoch - 4ms/step
Epoch 35/60
1493/1493 - 6s - loss: 0.0111 - val_loss: 0.0109 - 6s/epoch - 4ms/step
Epoch 36/60
1493/1493 - 6s - loss: 0.0110 - val_loss: 0.0108 - 6s/epoch - 4ms/step
Epoch 37/60
1493/1493 - 6s - loss: 0.0110 - val_loss: 0.0109 - 6s/epoch - 4ms/step
Epoch 38/60
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0108 - 6s/epoch - 4ms/step
Epoch 39/60
1493/1493 - 7s - loss: 0.0109 - val_loss: 0.0107 - 7s/epoch - 4ms/step
Epoch 40/60
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0106 - 6s/epoch - 4ms/step
Epoch 41/60
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 42/60
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0106 - 6s/epoch - 4ms/step
Epoch 43/60
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0106 - 6s/epoch - 4ms/step
Epoch 44/60
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0105 - 6s/epoch - 4ms/step
Epoch 45/60
1493/1493 - 7s - loss: 0.0107 - val_loss: 0.0105 - 7s/epoch - 4ms/step
Epoch 46/60
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0105 - 6s/epoch - 4ms/step
Epoch 47/60
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0106 - 6s/epoch - 4ms/step
Epoch 48/60
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0106 - 6s/epoch - 4ms/step
Epoch 49/60
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0105 - 6s/epoch - 4ms/step
Epoch 50/60
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0105 - 6s/epoch - 4ms/step
Epoch 51/60
1493/1493 - 7s - loss: 0.0106 - val_loss: 0.0105 - 7s/epoch - 4ms/step
Epoch 52/60
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0105 - 6s/epoch - 4ms/step
Epoch 53/60
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0105 - 6s/epoch - 4ms/step
Epoch 54/60
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 55/60
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 56/60
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 57/60
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 58/60
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 59/60
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 60/60
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.010241258889436722
  1/332 [..............................] - ETA: 57s 37/332 [==>...........................] - ETA: 0s  74/332 [=====>........................] - ETA: 0s111/332 [=========>....................] - ETA: 0s148/332 [============>.................] - ETA: 0s185/332 [===============>..............] - ETA: 0s222/332 [===================>..........] - ETA: 0s259/332 [======================>.......] - ETA: 0s296/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.08643968348295519
cosine 0.0661939498482225
MAE: 0.040475633
RMSE: 0.086924285
r2: 0.5102735511546059
RMSE zero-vector: 0.2430644284356365
['0.5custom_VAE', 'mse', 64, 60, 0.00030000000000000003, 0.6, 758, 0.010421671904623508, 0.010241258889436722, 0.08643968348295519, 0.0661939498482225, 0.04047563299536705, 0.08692428469657898, 0.5102735511546059, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 210 0.00030000000000000003 64 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2012490     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2012490     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5959276     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 13,352,182
Trainable params: 13,340,050
Non-trainable params: 12,132
__________________________________________________________________________________________________
Epoch 1/210
1493/1493 - 8s - loss: 0.0296 - val_loss: 0.0320 - 8s/epoch - 5ms/step
Epoch 2/210
1493/1493 - 6s - loss: 0.0159 - val_loss: 0.0154 - 6s/epoch - 4ms/step
Epoch 3/210
1493/1493 - 6s - loss: 0.0138 - val_loss: 0.0131 - 6s/epoch - 4ms/step
Epoch 4/210
1493/1493 - 6s - loss: 0.0129 - val_loss: 0.0124 - 6s/epoch - 4ms/step
Epoch 5/210
1493/1493 - 6s - loss: 0.0125 - val_loss: 0.0121 - 6s/epoch - 4ms/step
Epoch 6/210
1493/1493 - 6s - loss: 0.0121 - val_loss: 0.0116 - 6s/epoch - 4ms/step
Epoch 7/210
1493/1493 - 6s - loss: 0.0117 - val_loss: 0.0115 - 6s/epoch - 4ms/step
Epoch 8/210
1493/1493 - 6s - loss: 0.0116 - val_loss: 0.0113 - 6s/epoch - 4ms/step
Epoch 9/210
1493/1493 - 7s - loss: 0.0115 - val_loss: 0.0113 - 7s/epoch - 4ms/step
Epoch 10/210
1493/1493 - 6s - loss: 0.0114 - val_loss: 0.0111 - 6s/epoch - 4ms/step
Epoch 11/210
1493/1493 - 6s - loss: 0.0113 - val_loss: 0.0111 - 6s/epoch - 4ms/step
Epoch 12/210
1493/1493 - 6s - loss: 0.0113 - val_loss: 0.0110 - 6s/epoch - 4ms/step
Epoch 13/210
1493/1493 - 6s - loss: 0.0112 - val_loss: 0.0110 - 6s/epoch - 4ms/step
Epoch 14/210
1493/1493 - 6s - loss: 0.0112 - val_loss: 0.0110 - 6s/epoch - 4ms/step
Epoch 15/210
1493/1493 - 7s - loss: 0.0111 - val_loss: 0.0110 - 7s/epoch - 4ms/step
Epoch 16/210
1493/1493 - 6s - loss: 0.0111 - val_loss: 0.0109 - 6s/epoch - 4ms/step
Epoch 17/210
1493/1493 - 6s - loss: 0.0111 - val_loss: 0.0108 - 6s/epoch - 4ms/step
Epoch 18/210
1493/1493 - 6s - loss: 0.0111 - val_loss: 0.0108 - 6s/epoch - 4ms/step
Epoch 19/210
1493/1493 - 6s - loss: 0.0110 - val_loss: 0.0108 - 6s/epoch - 4ms/step
Epoch 20/210
1493/1493 - 6s - loss: 0.0110 - val_loss: 0.0108 - 6s/epoch - 4ms/step
Epoch 21/210
1493/1493 - 7s - loss: 0.0110 - val_loss: 0.0108 - 7s/epoch - 4ms/step
Epoch 22/210
1493/1493 - 6s - loss: 0.0110 - val_loss: 0.0108 - 6s/epoch - 4ms/step
Epoch 23/210
1493/1493 - 6s - loss: 0.0110 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 24/210
1493/1493 - 6s - loss: 0.0110 - val_loss: 0.0108 - 6s/epoch - 4ms/step
Epoch 25/210
1493/1493 - 6s - loss: 0.0110 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 26/210
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 27/210
1493/1493 - 7s - loss: 0.0109 - val_loss: 0.0107 - 7s/epoch - 4ms/step
Epoch 28/210
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 29/210
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 30/210
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 31/210
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0106 - 6s/epoch - 4ms/step
Epoch 32/210
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0105 - 6s/epoch - 4ms/step
Epoch 33/210
1493/1493 - 7s - loss: 0.0107 - val_loss: 0.0105 - 7s/epoch - 4ms/step
Epoch 34/210
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0105 - 6s/epoch - 4ms/step
Epoch 35/210
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0105 - 6s/epoch - 4ms/step
Epoch 36/210
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 37/210
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0105 - 6s/epoch - 4ms/step
Epoch 38/210
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 39/210
1493/1493 - 7s - loss: 0.0106 - val_loss: 0.0104 - 7s/epoch - 4ms/step
Epoch 40/210
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0105 - 6s/epoch - 4ms/step
Epoch 41/210
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0105 - 6s/epoch - 4ms/step
Epoch 42/210
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 43/210
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 44/210
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 45/210
1493/1493 - 7s - loss: 0.0106 - val_loss: 0.0103 - 7s/epoch - 4ms/step
Epoch 46/210
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 47/210
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 48/210
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 49/210
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 50/210
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 51/210
1493/1493 - 7s - loss: 0.0105 - val_loss: 0.0104 - 7s/epoch - 4ms/step
Epoch 52/210
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 53/210
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 54/210
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 55/210
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 56/210
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 57/210
1493/1493 - 7s - loss: 0.0105 - val_loss: 0.0103 - 7s/epoch - 4ms/step
Epoch 58/210
1493/1493 - 7s - loss: 0.0105 - val_loss: 0.0103 - 7s/epoch - 4ms/step
Epoch 59/210
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 60/210
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 61/210
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 62/210
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 63/210
1493/1493 - 7s - loss: 0.0105 - val_loss: 0.0102 - 7s/epoch - 4ms/step
Epoch 64/210
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 65/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 66/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 67/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 68/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 69/210
1493/1493 - 7s - loss: 0.0104 - val_loss: 0.0102 - 7s/epoch - 4ms/step
Epoch 70/210
1493/1493 - 7s - loss: 0.0104 - val_loss: 0.0103 - 7s/epoch - 4ms/step
Epoch 71/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 72/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 73/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 74/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 75/210
1493/1493 - 7s - loss: 0.0104 - val_loss: 0.0102 - 7s/epoch - 4ms/step
Epoch 76/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 77/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 78/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 79/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 80/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 81/210
1493/1493 - 7s - loss: 0.0104 - val_loss: 0.0102 - 7s/epoch - 4ms/step
Epoch 82/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 83/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 84/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 85/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 86/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 87/210
1493/1493 - 7s - loss: 0.0104 - val_loss: 0.0102 - 7s/epoch - 4ms/step
Epoch 88/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 89/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 90/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 91/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 92/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 93/210
1493/1493 - 7s - loss: 0.0104 - val_loss: 0.0102 - 7s/epoch - 4ms/step
Epoch 94/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 95/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 96/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 97/210
1493/1493 - 6s - loss: 0.0103 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 98/210
1493/1493 - 6s - loss: 0.0103 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 99/210
1493/1493 - 7s - loss: 0.0103 - val_loss: 0.0102 - 7s/epoch - 4ms/step
Epoch 100/210
1493/1493 - 6s - loss: 0.0103 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 101/210
1493/1493 - 6s - loss: 0.0103 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 102/210
1493/1493 - 6s - loss: 0.0103 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 103/210
1493/1493 - 6s - loss: 0.0103 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 104/210
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 105/210
1493/1493 - 6s - loss: 0.0103 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 106/210
1493/1493 - 7s - loss: 0.0102 - val_loss: 0.0101 - 7s/epoch - 4ms/step
Epoch 107/210
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 108/210
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 109/210
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 110/210
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 111/210
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 112/210
1493/1493 - 7s - loss: 0.0102 - val_loss: 0.0101 - 7s/epoch - 4ms/step
Epoch 113/210
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 114/210
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 115/210
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 116/210
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 117/210
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 118/210
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 119/210
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 120/210
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 121/210
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 122/210
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 123/210
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 124/210
1493/1493 - 7s - loss: 0.0102 - val_loss: 0.0100 - 7s/epoch - 4ms/step
Epoch 125/210
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 126/210
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 127/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 128/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 129/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 130/210
1493/1493 - 7s - loss: 0.0101 - val_loss: 0.0099 - 7s/epoch - 4ms/step
Epoch 131/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 132/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 133/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 134/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 135/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 136/210
1493/1493 - 7s - loss: 0.0101 - val_loss: 0.0100 - 7s/epoch - 4ms/step
Epoch 137/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 138/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 139/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 140/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 141/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 142/210
1493/1493 - 7s - loss: 0.0101 - val_loss: 0.0099 - 7s/epoch - 4ms/step
Epoch 143/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 144/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 145/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 146/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 147/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 148/210
1493/1493 - 7s - loss: 0.0100 - val_loss: 0.0098 - 7s/epoch - 4ms/step
Epoch 149/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 150/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 151/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 152/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 153/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 154/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 155/210
1493/1493 - 7s - loss: 0.0100 - val_loss: 0.0098 - 7s/epoch - 4ms/step
Epoch 156/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 157/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 158/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 159/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 160/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 161/210
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 4ms/step
Epoch 162/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 163/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 164/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 165/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 166/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 167/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 168/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 169/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 170/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 171/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 172/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 173/210
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 4ms/step
Epoch 174/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 175/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 176/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 177/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 178/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 179/210
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 4ms/step
Epoch 180/210
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0096 - 7s/epoch - 4ms/step
Epoch 181/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 182/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 183/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 184/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 185/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 186/210
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 4ms/step
Epoch 187/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 188/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 189/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 190/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 191/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 192/210
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 4ms/step
Epoch 193/210
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0096 - 7s/epoch - 5ms/step
Epoch 194/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0095 - 6s/epoch - 4ms/step
Epoch 195/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 196/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 197/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 198/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0095 - 6s/epoch - 4ms/step
Epoch 199/210
1493/1493 - 7s - loss: 0.0098 - val_loss: 0.0095 - 7s/epoch - 4ms/step
Epoch 200/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0095 - 6s/epoch - 4ms/step
Epoch 201/210
1493/1493 - 6s - loss: 0.0097 - val_loss: 0.0095 - 6s/epoch - 4ms/step
Epoch 202/210
1493/1493 - 6s - loss: 0.0097 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 203/210
1493/1493 - 6s - loss: 0.0097 - val_loss: 0.0095 - 6s/epoch - 4ms/step
Epoch 204/210
1493/1493 - 6s - loss: 0.0097 - val_loss: 0.0095 - 6s/epoch - 4ms/step
Epoch 205/210
1493/1493 - 7s - loss: 0.0097 - val_loss: 0.0095 - 7s/epoch - 4ms/step
Epoch 206/210
1493/1493 - 6s - loss: 0.0097 - val_loss: 0.0095 - 6s/epoch - 4ms/step
Epoch 207/210
1493/1493 - 6s - loss: 0.0097 - val_loss: 0.0095 - 6s/epoch - 4ms/step
Epoch 208/210
1493/1493 - 6s - loss: 0.0097 - val_loss: 0.0095 - 6s/epoch - 4ms/step
Epoch 209/210
1493/1493 - 6s - loss: 0.0097 - val_loss: 0.0095 - 6s/epoch - 4ms/step
Epoch 210/210
1493/1493 - 6s - loss: 0.0097 - val_loss: 0.0095 - 6s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.00948251411318779
  1/332 [..............................] - ETA: 52s 36/332 [==>...........................] - ETA: 0s  72/332 [=====>........................] - ETA: 0s108/332 [========>.....................] - ETA: 0s145/332 [============>.................] - ETA: 0s182/332 [===============>..............] - ETA: 0s219/332 [==================>...........] - ETA: 0s256/332 [======================>.......] - ETA: 0s293/332 [=========================>....] - ETA: 0s329/332 [============================>.] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.07254083047958307
cosine 0.05561077484107674
MAE: 0.036625475
RMSE: 0.079611786
r2: 0.5892041040333915
RMSE zero-vector: 0.2430644284356365
['2.1custom_VAE', 'mse', 64, 210, 0.00030000000000000003, 0.6, 758, 0.009710780344903469, 0.00948251411318779, 0.07254083047958307, 0.05561077484107674, 0.03662547469139099, 0.07961178570985794, 0.5892041040333915, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 210 0.0007 64 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1769)        7076        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1769)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1341660     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4165381     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 9,093,562
Trainable params: 9,084,970
Non-trainable params: 8,592
__________________________________________________________________________________________________
Epoch 1/210
1493/1493 - 8s - loss: 0.0259 - val_loss: 0.0190 - 8s/epoch - 5ms/step
Epoch 2/210
1493/1493 - 6s - loss: 0.0154 - val_loss: 0.0154 - 6s/epoch - 4ms/step
Epoch 3/210
1493/1493 - 6s - loss: 0.0137 - val_loss: 0.0130 - 6s/epoch - 4ms/step
Epoch 4/210
1493/1493 - 6s - loss: 0.0129 - val_loss: 0.0125 - 6s/epoch - 4ms/step
Epoch 5/210
1493/1493 - 6s - loss: 0.0126 - val_loss: 0.0122 - 6s/epoch - 4ms/step
Epoch 6/210
1493/1493 - 6s - loss: 0.0124 - val_loss: 0.0122 - 6s/epoch - 4ms/step
Epoch 7/210
1493/1493 - 6s - loss: 0.0123 - val_loss: 0.0121 - 6s/epoch - 4ms/step
Epoch 8/210
1493/1493 - 6s - loss: 0.0122 - val_loss: 0.0120 - 6s/epoch - 4ms/step
Epoch 9/210
1493/1493 - 6s - loss: 0.0122 - val_loss: 0.0119 - 6s/epoch - 4ms/step
Epoch 10/210
1493/1493 - 6s - loss: 0.0121 - val_loss: 0.0119 - 6s/epoch - 4ms/step
Epoch 11/210
1493/1493 - 6s - loss: 0.0121 - val_loss: 0.0119 - 6s/epoch - 4ms/step
Epoch 12/210
1493/1493 - 6s - loss: 0.0120 - val_loss: 0.0119 - 6s/epoch - 4ms/step
Epoch 13/210
1493/1493 - 6s - loss: 0.0120 - val_loss: 0.0118 - 6s/epoch - 4ms/step
Epoch 14/210
1493/1493 - 6s - loss: 0.0120 - val_loss: 0.0117 - 6s/epoch - 4ms/step
Epoch 15/210
1493/1493 - 6s - loss: 0.0118 - val_loss: 0.0115 - 6s/epoch - 4ms/step
Epoch 16/210
1493/1493 - 6s - loss: 0.0116 - val_loss: 0.0113 - 6s/epoch - 4ms/step
Epoch 17/210
1493/1493 - 6s - loss: 0.0114 - val_loss: 0.0112 - 6s/epoch - 4ms/step
Epoch 18/210
1493/1493 - 6s - loss: 0.0113 - val_loss: 0.0111 - 6s/epoch - 4ms/step
Epoch 19/210
1493/1493 - 6s - loss: 0.0113 - val_loss: 0.0111 - 6s/epoch - 4ms/step
Epoch 20/210
1493/1493 - 6s - loss: 0.0112 - val_loss: 0.0110 - 6s/epoch - 4ms/step
Epoch 21/210
1493/1493 - 6s - loss: 0.0112 - val_loss: 0.0110 - 6s/epoch - 4ms/step
Epoch 22/210
1493/1493 - 6s - loss: 0.0112 - val_loss: 0.0110 - 6s/epoch - 4ms/step
Epoch 23/210
1493/1493 - 6s - loss: 0.0111 - val_loss: 0.0109 - 6s/epoch - 4ms/step
Epoch 24/210
1493/1493 - 6s - loss: 0.0111 - val_loss: 0.0109 - 6s/epoch - 4ms/step
Epoch 25/210
1493/1493 - 6s - loss: 0.0110 - val_loss: 0.0108 - 6s/epoch - 4ms/step
Epoch 26/210
1493/1493 - 6s - loss: 0.0110 - val_loss: 0.0108 - 6s/epoch - 4ms/step
Epoch 27/210
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0106 - 6s/epoch - 4ms/step
Epoch 28/210
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 29/210
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0106 - 6s/epoch - 4ms/step
Epoch 30/210
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0106 - 6s/epoch - 4ms/step
Epoch 31/210
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0105 - 6s/epoch - 4ms/step
Epoch 32/210
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0105 - 6s/epoch - 4ms/step
Epoch 33/210
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0105 - 6s/epoch - 4ms/step
Epoch 34/210
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0105 - 6s/epoch - 4ms/step
Epoch 35/210
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0105 - 6s/epoch - 4ms/step
Epoch 36/210
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 37/210
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0105 - 6s/epoch - 4ms/step
Epoch 38/210
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 39/210
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 40/210
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 41/210
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 42/210
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 43/210
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 44/210
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 45/210
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 46/210
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 47/210
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 48/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 49/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 50/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 51/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 52/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 53/210
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 54/210
1493/1493 - 6s - loss: 0.0103 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 55/210
1493/1493 - 6s - loss: 0.0103 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 56/210
1493/1493 - 6s - loss: 0.0103 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 57/210
1493/1493 - 6s - loss: 0.0103 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 58/210
1493/1493 - 6s - loss: 0.0103 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 59/210
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 60/210
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 61/210
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 62/210
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 63/210
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 64/210
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 65/210
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 66/210
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 67/210
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 68/210
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 69/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 70/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 71/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 72/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 73/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 74/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 75/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 76/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 77/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 78/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 79/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 80/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 81/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 82/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 83/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 84/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 85/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 86/210
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 87/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 88/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 89/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 90/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 91/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 92/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 93/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 94/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 95/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 96/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 97/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 98/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 99/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 100/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 101/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 102/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 103/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 104/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 105/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 106/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 107/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 108/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 109/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 110/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 111/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 112/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 113/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 114/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 115/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 116/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 117/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 118/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 119/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 120/210
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 121/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 122/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 123/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 124/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 125/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 126/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 127/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 128/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 129/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 130/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 131/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 132/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 133/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 134/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 135/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 136/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 137/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 138/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 139/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 140/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 141/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 142/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 143/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 144/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 145/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 146/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 147/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 148/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 149/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 150/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 151/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 152/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 153/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 154/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 155/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 156/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 157/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 158/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 159/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 160/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 161/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 162/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 163/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 164/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 165/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 166/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 167/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 168/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 169/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 170/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 171/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 172/210
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 173/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 174/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 175/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 176/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 177/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 178/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 179/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 180/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 181/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 182/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 183/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 184/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 185/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 186/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 187/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 188/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 189/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 190/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 191/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 192/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 193/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 194/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 195/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 196/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 197/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 198/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 199/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 200/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 201/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 202/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 203/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 204/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 205/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 206/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 207/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 208/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 209/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0095 - 6s/epoch - 4ms/step
Epoch 210/210
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009586128406226635
  1/332 [..............................] - ETA: 49s 37/332 [==>...........................] - ETA: 0s  74/332 [=====>........................] - ETA: 0s111/332 [=========>....................] - ETA: 0s148/332 [============>.................] - ETA: 0s185/332 [===============>..............] - ETA: 0s222/332 [===================>..........] - ETA: 0s259/332 [======================>.......] - ETA: 0s296/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.075125941101713
cosine 0.057562136499557326
MAE: 0.03734271
RMSE: 0.08104626
r2: 0.5742669815285661
RMSE zero-vector: 0.2430644284356365
['1.4custom_VAE', 'mse', 64, 210, 0.0007, 0.6, 758, 0.009797920472919941, 0.009586128406226635, 0.075125941101713, 0.057562136499557326, 0.03734270855784416, 0.08104626089334488, 0.5742669815285661, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 55 0.0018 64 2] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          479814      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          479814      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1860682     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 3,622,318
Trainable params: 3,618,274
Non-trainable params: 4,044
__________________________________________________________________________________________________
Epoch 1/55
1493/1493 - 8s - loss: 0.0118 - val_loss: 0.0077 - 8s/epoch - 5ms/step
Epoch 2/55
1493/1493 - 6s - loss: 0.0074 - val_loss: 0.0075 - 6s/epoch - 4ms/step
Epoch 3/55
1493/1493 - 6s - loss: 0.0071 - val_loss: 0.0070 - 6s/epoch - 4ms/step
Epoch 4/55
1493/1493 - 6s - loss: 0.0069 - val_loss: 0.0068 - 6s/epoch - 4ms/step
Epoch 5/55
1493/1493 - 6s - loss: 0.0068 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 6/55
1493/1493 - 6s - loss: 0.0068 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 7/55
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 8/55
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 9/55
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 10/55
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 11/55
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 12/55
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0066 - 6s/epoch - 4ms/step
Epoch 13/55
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 14/55
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 15/55
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0066 - 6s/epoch - 4ms/step
Epoch 16/55
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0066 - 6s/epoch - 4ms/step
Epoch 17/55
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0066 - 6s/epoch - 4ms/step
Epoch 18/55
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 19/55
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0066 - 6s/epoch - 4ms/step
Epoch 20/55
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 21/55
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0066 - 6s/epoch - 4ms/step
Epoch 22/55
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0066 - 6s/epoch - 4ms/step
Epoch 23/55
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0066 - 6s/epoch - 4ms/step
Epoch 24/55
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0067 - 6s/epoch - 4ms/step
Epoch 25/55
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0066 - 6s/epoch - 4ms/step
Epoch 26/55
1493/1493 - 6s - loss: 0.0067 - val_loss: 0.0066 - 6s/epoch - 4ms/step
Epoch 27/55
1493/1493 - 6s - loss: 0.0066 - val_loss: 0.0065 - 6s/epoch - 4ms/step
Epoch 28/55
1493/1493 - 6s - loss: 0.0065 - val_loss: 0.0064 - 6s/epoch - 4ms/step
Epoch 29/55
1493/1493 - 6s - loss: 0.0064 - val_loss: 0.0063 - 6s/epoch - 4ms/step
Epoch 30/55
1493/1493 - 6s - loss: 0.0063 - val_loss: 0.0063 - 6s/epoch - 4ms/step
Epoch 31/55
1493/1493 - 6s - loss: 0.0063 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 32/55
1493/1493 - 6s - loss: 0.0063 - val_loss: 0.0063 - 6s/epoch - 4ms/step
Epoch 33/55
1493/1493 - 6s - loss: 0.0063 - val_loss: 0.0063 - 6s/epoch - 4ms/step
Epoch 34/55
1493/1493 - 6s - loss: 0.0063 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 35/55
1493/1493 - 6s - loss: 0.0063 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 36/55
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 37/55
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 38/55
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 39/55
1493/1493 - 7s - loss: 0.0062 - val_loss: 0.0062 - 7s/epoch - 4ms/step
Epoch 40/55
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 41/55
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 42/55
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 43/55
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 44/55
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 45/55
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 46/55
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 47/55
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 48/55
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 49/55
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 50/55
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 51/55
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
Epoch 52/55
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 53/55
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 54/55
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0062 - 6s/epoch - 4ms/step
Epoch 55/55
1493/1493 - 6s - loss: 0.0062 - val_loss: 0.0061 - 6s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.006149173714220524
  1/332 [..............................] - ETA: 50s 37/332 [==>...........................] - ETA: 0s  75/332 [=====>........................] - ETA: 0s113/332 [=========>....................] - ETA: 0s150/332 [============>.................] - ETA: 0s187/332 [===============>..............] - ETA: 0s225/332 [===================>..........] - ETA: 0s262/332 [======================>.......] - ETA: 0s300/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.12386810667237111
cosine 0.09465181396742654
MAE: 0.048553504
RMSE: 0.103193566
r2: 0.3097970344098825
RMSE zero-vector: 0.2430644284356365
['0.5custom_VAE', 'logcosh', 64, 55, 0.0018, 0.6, 758, 0.006200156174600124, 0.006149173714220524, 0.12386810667237111, 0.09465181396742654, 0.048553504049777985, 0.10319356620311737, 0.3097970344098825, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.1 205 0.0005 64 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2654)         3357310     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2654)        10616       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2654)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          2012490     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          2012490     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         5959276     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 13,352,182
Trainable params: 13,340,050
Non-trainable params: 12,132
__________________________________________________________________________________________________
Epoch 1/205
1493/1493 - 8s - loss: 0.0295 - val_loss: 0.0215 - 8s/epoch - 5ms/step
Epoch 2/205
1493/1493 - 6s - loss: 0.0159 - val_loss: 0.0188 - 6s/epoch - 4ms/step
Epoch 3/205
1493/1493 - 6s - loss: 0.0137 - val_loss: 0.0130 - 6s/epoch - 4ms/step
Epoch 4/205
1493/1493 - 6s - loss: 0.0128 - val_loss: 0.0124 - 6s/epoch - 4ms/step
Epoch 5/205
1493/1493 - 6s - loss: 0.0122 - val_loss: 0.0117 - 6s/epoch - 4ms/step
Epoch 6/205
1493/1493 - 6s - loss: 0.0118 - val_loss: 0.0115 - 6s/epoch - 4ms/step
Epoch 7/205
1493/1493 - 6s - loss: 0.0116 - val_loss: 0.0113 - 6s/epoch - 4ms/step
Epoch 8/205
1493/1493 - 6s - loss: 0.0115 - val_loss: 0.0112 - 6s/epoch - 4ms/step
Epoch 9/205
1493/1493 - 6s - loss: 0.0114 - val_loss: 0.0112 - 6s/epoch - 4ms/step
Epoch 10/205
1493/1493 - 6s - loss: 0.0114 - val_loss: 0.0111 - 6s/epoch - 4ms/step
Epoch 11/205
1493/1493 - 6s - loss: 0.0113 - val_loss: 0.0111 - 6s/epoch - 4ms/step
Epoch 12/205
1493/1493 - 6s - loss: 0.0112 - val_loss: 0.0110 - 6s/epoch - 4ms/step
Epoch 13/205
1493/1493 - 6s - loss: 0.0112 - val_loss: 0.0110 - 6s/epoch - 4ms/step
Epoch 14/205
1493/1493 - 6s - loss: 0.0113 - val_loss: 0.0110 - 6s/epoch - 4ms/step
Epoch 15/205
1493/1493 - 6s - loss: 0.0112 - val_loss: 0.0109 - 6s/epoch - 4ms/step
Epoch 16/205
1493/1493 - 6s - loss: 0.0111 - val_loss: 0.0108 - 6s/epoch - 4ms/step
Epoch 17/205
1493/1493 - 6s - loss: 0.0111 - val_loss: 0.0108 - 6s/epoch - 4ms/step
Epoch 18/205
1493/1493 - 6s - loss: 0.0111 - val_loss: 0.0109 - 6s/epoch - 4ms/step
Epoch 19/205
1493/1493 - 6s - loss: 0.0110 - val_loss: 0.0108 - 6s/epoch - 4ms/step
Epoch 20/205
1493/1493 - 6s - loss: 0.0110 - val_loss: 0.0108 - 6s/epoch - 4ms/step
Epoch 21/205
1493/1493 - 6s - loss: 0.0110 - val_loss: 0.0108 - 6s/epoch - 4ms/step
Epoch 22/205
1493/1493 - 6s - loss: 0.0110 - val_loss: 0.0108 - 6s/epoch - 4ms/step
Epoch 23/205
1493/1493 - 6s - loss: 0.0110 - val_loss: 0.0108 - 6s/epoch - 4ms/step
Epoch 24/205
1493/1493 - 6s - loss: 0.0110 - val_loss: 0.0108 - 6s/epoch - 4ms/step
Epoch 25/205
1493/1493 - 6s - loss: 0.0110 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 26/205
1493/1493 - 6s - loss: 0.0110 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 27/205
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 28/205
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 29/205
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 30/205
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 31/205
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 32/205
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 33/205
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 34/205
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 35/205
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 36/205
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 37/205
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0108 - 6s/epoch - 4ms/step
Epoch 38/205
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 39/205
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 40/205
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 41/205
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 42/205
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 43/205
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 44/205
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 45/205
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0106 - 6s/epoch - 4ms/step
Epoch 46/205
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 47/205
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 48/205
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 49/205
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 50/205
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 51/205
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 52/205
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0106 - 6s/epoch - 4ms/step
Epoch 53/205
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0106 - 6s/epoch - 4ms/step
Epoch 54/205
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0106 - 6s/epoch - 4ms/step
Epoch 55/205
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0106 - 6s/epoch - 4ms/step
Epoch 56/205
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0106 - 6s/epoch - 4ms/step
Epoch 57/205
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0106 - 6s/epoch - 4ms/step
Epoch 58/205
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 59/205
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0106 - 6s/epoch - 4ms/step
Epoch 60/205
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0106 - 6s/epoch - 4ms/step
Epoch 61/205
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0106 - 6s/epoch - 4ms/step
Epoch 62/205
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0105 - 6s/epoch - 4ms/step
Epoch 63/205
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0105 - 6s/epoch - 4ms/step
Epoch 64/205
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0105 - 6s/epoch - 4ms/step
Epoch 65/205
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 66/205
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 67/205
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 68/205
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 69/205
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 70/205
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 71/205
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 72/205
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 73/205
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 74/205
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 75/205
1493/1493 - 6s - loss: 0.0103 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 76/205
1493/1493 - 6s - loss: 0.0103 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 77/205
1493/1493 - 6s - loss: 0.0103 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 78/205
1493/1493 - 6s - loss: 0.0103 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 79/205
1493/1493 - 6s - loss: 0.0103 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 80/205
1493/1493 - 6s - loss: 0.0103 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 81/205
1493/1493 - 6s - loss: 0.0103 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 82/205
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 83/205
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 84/205
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 85/205
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 86/205
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 87/205
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 88/205
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 89/205
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 90/205
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 91/205
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 92/205
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 93/205
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 94/205
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 95/205
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 96/205
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 97/205
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 98/205
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 99/205
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 100/205
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 101/205
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 102/205
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 103/205
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 104/205
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 105/205
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 106/205
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 107/205
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 108/205
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 109/205
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 110/205
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 111/205
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 112/205
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 113/205
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 114/205
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 115/205
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 116/205
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 117/205
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 118/205
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 119/205
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 120/205
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 121/205
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 122/205
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 123/205
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 124/205
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 125/205
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 126/205
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 127/205
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 128/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 129/205
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 130/205
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 131/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 132/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 133/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 134/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 135/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 136/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 137/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 138/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 139/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 140/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 141/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 142/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 143/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 144/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 145/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 146/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 147/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 148/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 149/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 150/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 151/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 152/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 153/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 154/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 155/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 156/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 157/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 158/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 159/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 160/205
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 161/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 162/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 163/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 164/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 165/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 166/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 167/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 168/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 169/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 170/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 171/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 172/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 173/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 174/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 175/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 176/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 177/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 178/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 179/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 180/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 181/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 182/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 183/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 184/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 185/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 186/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 187/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 188/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 189/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 190/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 191/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 192/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 193/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 194/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 195/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 196/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 197/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 198/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0095 - 6s/epoch - 4ms/step
Epoch 199/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0095 - 6s/epoch - 4ms/step
Epoch 200/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 201/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 202/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 203/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 204/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0096 - 6s/epoch - 4ms/step
Epoch 205/205
1493/1493 - 6s - loss: 0.0098 - val_loss: 0.0095 - 6s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009527746587991714
  1/332 [..............................] - ETA: 47s 37/332 [==>...........................] - ETA: 0s  74/332 [=====>........................] - ETA: 0s111/332 [=========>....................] - ETA: 0s148/332 [============>.................] - ETA: 0s185/332 [===============>..............] - ETA: 0s222/332 [===================>..........] - ETA: 0s259/332 [======================>.......] - ETA: 0s296/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.07422779651012593
cosine 0.05692479369373444
MAE: 0.037003223
RMSE: 0.08053349
r2: 0.5796368984680961
RMSE zero-vector: 0.2430644284356365
['2.1custom_VAE', 'mse', 64, 205, 0.0005, 0.6, 758, 0.009761521592736244, 0.009527746587991714, 0.07422779651012593, 0.05692479369373444, 0.03700322285294533, 0.08053348958492279, 0.5796368984680961, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6 145 0.0005 64 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2022)         2557830     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2022)        8088        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2022)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          1533434     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          1533434     ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4678212     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 10,310,998
Trainable params: 10,301,394
Non-trainable params: 9,604
__________________________________________________________________________________________________
Epoch 1/145
1493/1493 - 8s - loss: 0.0271 - val_loss: 0.0169 - 8s/epoch - 5ms/step
Epoch 2/145
1493/1493 - 6s - loss: 0.0153 - val_loss: 0.0144 - 6s/epoch - 4ms/step
Epoch 3/145
1493/1493 - 6s - loss: 0.0136 - val_loss: 0.0129 - 6s/epoch - 4ms/step
Epoch 4/145
1493/1493 - 6s - loss: 0.0129 - val_loss: 0.0124 - 6s/epoch - 4ms/step
Epoch 5/145
1493/1493 - 7s - loss: 0.0126 - val_loss: 0.0123 - 7s/epoch - 4ms/step
Epoch 6/145
1493/1493 - 7s - loss: 0.0123 - val_loss: 0.0120 - 7s/epoch - 4ms/step
Epoch 7/145
1493/1493 - 6s - loss: 0.0119 - val_loss: 0.0116 - 6s/epoch - 4ms/step
Epoch 8/145
1493/1493 - 6s - loss: 0.0116 - val_loss: 0.0114 - 6s/epoch - 4ms/step
Epoch 9/145
1493/1493 - 6s - loss: 0.0115 - val_loss: 0.0112 - 6s/epoch - 4ms/step
Epoch 10/145
1493/1493 - 6s - loss: 0.0114 - val_loss: 0.0112 - 6s/epoch - 4ms/step
Epoch 11/145
1493/1493 - 6s - loss: 0.0114 - val_loss: 0.0111 - 6s/epoch - 4ms/step
Epoch 12/145
1493/1493 - 7s - loss: 0.0113 - val_loss: 0.0111 - 7s/epoch - 4ms/step
Epoch 13/145
1493/1493 - 6s - loss: 0.0113 - val_loss: 0.0111 - 6s/epoch - 4ms/step
Epoch 14/145
1493/1493 - 6s - loss: 0.0112 - val_loss: 0.0111 - 6s/epoch - 4ms/step
Epoch 15/145
1493/1493 - 6s - loss: 0.0113 - val_loss: 0.0110 - 6s/epoch - 4ms/step
Epoch 16/145
1493/1493 - 6s - loss: 0.0111 - val_loss: 0.0109 - 6s/epoch - 4ms/step
Epoch 17/145
1493/1493 - 6s - loss: 0.0111 - val_loss: 0.0109 - 6s/epoch - 4ms/step
Epoch 18/145
1493/1493 - 7s - loss: 0.0111 - val_loss: 0.0109 - 7s/epoch - 4ms/step
Epoch 19/145
1493/1493 - 6s - loss: 0.0111 - val_loss: 0.0109 - 6s/epoch - 4ms/step
Epoch 20/145
1493/1493 - 6s - loss: 0.0111 - val_loss: 0.0109 - 6s/epoch - 4ms/step
Epoch 21/145
1493/1493 - 7s - loss: 0.0111 - val_loss: 0.0109 - 7s/epoch - 4ms/step
Epoch 22/145
1493/1493 - 7s - loss: 0.0111 - val_loss: 0.0108 - 7s/epoch - 4ms/step
Epoch 23/145
1493/1493 - 7s - loss: 0.0110 - val_loss: 0.0109 - 7s/epoch - 4ms/step
Epoch 24/145
1493/1493 - 7s - loss: 0.0110 - val_loss: 0.0108 - 7s/epoch - 4ms/step
Epoch 25/145
1493/1493 - 6s - loss: 0.0110 - val_loss: 0.0108 - 6s/epoch - 4ms/step
Epoch 26/145
1493/1493 - 6s - loss: 0.0110 - val_loss: 0.0108 - 6s/epoch - 4ms/step
Epoch 27/145
1493/1493 - 6s - loss: 0.0110 - val_loss: 0.0108 - 6s/epoch - 4ms/step
Epoch 28/145
1493/1493 - 7s - loss: 0.0110 - val_loss: 0.0108 - 7s/epoch - 4ms/step
Epoch 29/145
1493/1493 - 6s - loss: 0.0110 - val_loss: 0.0108 - 6s/epoch - 4ms/step
Epoch 30/145
1493/1493 - 7s - loss: 0.0110 - val_loss: 0.0108 - 7s/epoch - 4ms/step
Epoch 31/145
1493/1493 - 6s - loss: 0.0110 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 32/145
1493/1493 - 6s - loss: 0.0110 - val_loss: 0.0108 - 6s/epoch - 4ms/step
Epoch 33/145
1493/1493 - 6s - loss: 0.0110 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 34/145
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0108 - 6s/epoch - 4ms/step
Epoch 35/145
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0108 - 6s/epoch - 4ms/step
Epoch 36/145
1493/1493 - 7s - loss: 0.0109 - val_loss: 0.0107 - 7s/epoch - 4ms/step
Epoch 37/145
1493/1493 - 7s - loss: 0.0109 - val_loss: 0.0108 - 7s/epoch - 4ms/step
Epoch 38/145
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 39/145
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 40/145
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 41/145
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 42/145
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 43/145
1493/1493 - 7s - loss: 0.0109 - val_loss: 0.0107 - 7s/epoch - 4ms/step
Epoch 44/145
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 45/145
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 46/145
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0108 - 6s/epoch - 4ms/step
Epoch 47/145
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 48/145
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0106 - 6s/epoch - 4ms/step
Epoch 49/145
1493/1493 - 7s - loss: 0.0108 - val_loss: 0.0107 - 7s/epoch - 4ms/step
Epoch 50/145
1493/1493 - 6s - loss: 0.0109 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 51/145
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 52/145
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0106 - 6s/epoch - 4ms/step
Epoch 53/145
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 54/145
1493/1493 - 7s - loss: 0.0108 - val_loss: 0.0106 - 7s/epoch - 4ms/step
Epoch 55/145
1493/1493 - 7s - loss: 0.0108 - val_loss: 0.0106 - 7s/epoch - 4ms/step
Epoch 56/145
1493/1493 - 7s - loss: 0.0108 - val_loss: 0.0106 - 7s/epoch - 4ms/step
Epoch 57/145
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0106 - 6s/epoch - 4ms/step
Epoch 58/145
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0107 - 6s/epoch - 4ms/step
Epoch 59/145
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0106 - 6s/epoch - 4ms/step
Epoch 60/145
1493/1493 - 6s - loss: 0.0108 - val_loss: 0.0106 - 6s/epoch - 4ms/step
Epoch 61/145
1493/1493 - 7s - loss: 0.0108 - val_loss: 0.0106 - 7s/epoch - 4ms/step
Epoch 62/145
1493/1493 - 7s - loss: 0.0107 - val_loss: 0.0105 - 7s/epoch - 4ms/step
Epoch 63/145
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0106 - 6s/epoch - 4ms/step
Epoch 64/145
1493/1493 - 6s - loss: 0.0107 - val_loss: 0.0105 - 6s/epoch - 4ms/step
Epoch 65/145
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 66/145
1493/1493 - 7s - loss: 0.0106 - val_loss: 0.0104 - 7s/epoch - 4ms/step
Epoch 67/145
1493/1493 - 7s - loss: 0.0106 - val_loss: 0.0104 - 7s/epoch - 4ms/step
Epoch 68/145
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 69/145
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 70/145
1493/1493 - 6s - loss: 0.0106 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 71/145
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0104 - 6s/epoch - 4ms/step
Epoch 72/145
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 73/145
1493/1493 - 7s - loss: 0.0105 - val_loss: 0.0104 - 7s/epoch - 4ms/step
Epoch 74/145
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 75/145
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 76/145
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 77/145
1493/1493 - 7s - loss: 0.0105 - val_loss: 0.0103 - 7s/epoch - 4ms/step
Epoch 78/145
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 79/145
1493/1493 - 7s - loss: 0.0105 - val_loss: 0.0103 - 7s/epoch - 4ms/step
Epoch 80/145
1493/1493 - 7s - loss: 0.0105 - val_loss: 0.0103 - 7s/epoch - 4ms/step
Epoch 81/145
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 82/145
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 83/145
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 84/145
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0103 - 6s/epoch - 4ms/step
Epoch 85/145
1493/1493 - 7s - loss: 0.0105 - val_loss: 0.0103 - 7s/epoch - 4ms/step
Epoch 86/145
1493/1493 - 6s - loss: 0.0105 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 87/145
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 88/145
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 89/145
1493/1493 - 7s - loss: 0.0104 - val_loss: 0.0102 - 7s/epoch - 4ms/step
Epoch 90/145
1493/1493 - 7s - loss: 0.0104 - val_loss: 0.0102 - 7s/epoch - 4ms/step
Epoch 91/145
1493/1493 - 7s - loss: 0.0104 - val_loss: 0.0102 - 7s/epoch - 4ms/step
Epoch 92/145
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 93/145
1493/1493 - 6s - loss: 0.0104 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 94/145
1493/1493 - 6s - loss: 0.0103 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 95/145
1493/1493 - 7s - loss: 0.0103 - val_loss: 0.0101 - 7s/epoch - 4ms/step
Epoch 96/145
1493/1493 - 6s - loss: 0.0103 - val_loss: 0.0101 - 6s/epoch - 4ms/step
Epoch 97/145
1493/1493 - 7s - loss: 0.0103 - val_loss: 0.0101 - 7s/epoch - 4ms/step
Epoch 98/145
1493/1493 - 6s - loss: 0.0103 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 99/145
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 100/145
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 101/145
1493/1493 - 7s - loss: 0.0102 - val_loss: 0.0100 - 7s/epoch - 4ms/step
Epoch 102/145
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 103/145
1493/1493 - 7s - loss: 0.0102 - val_loss: 0.0099 - 7s/epoch - 4ms/step
Epoch 104/145
1493/1493 - 6s - loss: 0.0102 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 105/145
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0100 - 6s/epoch - 4ms/step
Epoch 106/145
1493/1493 - 7s - loss: 0.0101 - val_loss: 0.0099 - 7s/epoch - 4ms/step
Epoch 107/145
1493/1493 - 7s - loss: 0.0101 - val_loss: 0.0099 - 7s/epoch - 4ms/step
Epoch 108/145
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 109/145
1493/1493 - 7s - loss: 0.0101 - val_loss: 0.0099 - 7s/epoch - 4ms/step
Epoch 110/145
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 111/145
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 112/145
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 113/145
1493/1493 - 6s - loss: 0.0101 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 114/145
1493/1493 - 7s - loss: 0.0101 - val_loss: 0.0099 - 7s/epoch - 4ms/step
Epoch 115/145
1493/1493 - 7s - loss: 0.0101 - val_loss: 0.0098 - 7s/epoch - 4ms/step
Epoch 116/145
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 117/145
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0099 - 6s/epoch - 4ms/step
Epoch 118/145
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 119/145
1493/1493 - 7s - loss: 0.0100 - val_loss: 0.0098 - 7s/epoch - 4ms/step
Epoch 120/145
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 121/145
1493/1493 - 7s - loss: 0.0100 - val_loss: 0.0098 - 7s/epoch - 4ms/step
Epoch 122/145
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 123/145
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 124/145
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0098 - 6s/epoch - 4ms/step
Epoch 125/145
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 126/145
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 127/145
1493/1493 - 7s - loss: 0.0100 - val_loss: 0.0097 - 7s/epoch - 4ms/step
Epoch 128/145
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 129/145
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 130/145
1493/1493 - 6s - loss: 0.0100 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 131/145
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 4ms/step
Epoch 132/145
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 4ms/step
Epoch 133/145
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 4ms/step
Epoch 134/145
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 135/145
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 136/145
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 4ms/step
Epoch 137/145
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 138/145
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 4ms/step
Epoch 139/145
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 140/145
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 141/145
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 142/145
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 143/145
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0097 - 6s/epoch - 4ms/step
Epoch 144/145
1493/1493 - 7s - loss: 0.0099 - val_loss: 0.0097 - 7s/epoch - 4ms/step
Epoch 145/145
1493/1493 - 6s - loss: 0.0099 - val_loss: 0.0096 - 6s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 758
Loss in the autoencoder: 0.009622481651604176
  1/332 [..............................] - ETA: 48s 38/332 [==>...........................] - ETA: 0s  75/332 [=====>........................] - ETA: 0s113/332 [=========>....................] - ETA: 0s151/332 [============>.................] - ETA: 0s189/332 [================>.............] - ETA: 0s227/332 [===================>..........] - ETA: 0s265/332 [======================>.......] - ETA: 0s303/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.07516733818444224
cosine 0.057598887902361194
MAE: 0.03739245
RMSE: 0.08102858
r2: 0.5744525843137573
RMSE zero-vector: 0.2430644284356365
['1.6custom_VAE', 'mse', 64, 145, 0.0005, 0.6, 758, 0.009886459447443485, 0.009622481651604176, 0.07516733818444224, 0.057598887902361194, 0.037392448633909225, 0.08102858066558838, 0.5744525843137573, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 60 0.0022 64 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          479814      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          479814      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1860682     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 3,622,318
Trainable params: 3,618,274
Non-trainable params: 4,044
__________________________________________________________________________________________________
Epoch 1/60
2023-02-23 07:25:33.666201: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 53654272 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 47906816/25447170048
2023-02-23 07:25:33.666620: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24595883644
MaxInUse:                  24595883644
NumAllocs:                   848939710
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 07:25:33.666689: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 07:25:33.666697: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 93
2023-02-23 07:25:33.666701: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 101
2023-02-23 07:25:33.666706: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 07:25:33.666710: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 154
2023-02-23 07:25:33.666714: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 374
2023-02-23 07:25:33.666718: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 154
2023-02-23 07:25:33.666722: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 22
2023-02-23 07:25:33.666726: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 88
2023-02-23 07:25:33.666730: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 22
2023-02-23 07:25:33.666734: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 66
2023-02-23 07:25:33.666738: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 44
2023-02-23 07:25:33.666743: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1916224, 63
2023-02-23 07:25:33.666747: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 66
2023-02-23 07:25:33.666751: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 42
2023-02-23 07:25:33.666755: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 36
2023-02-23 07:25:33.666759: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 9
2023-02-23 07:25:33.666763: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 36
2023-02-23 07:25:33.666767: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 9
2023-02-23 07:25:33.666771: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 24
2023-02-23 07:25:33.666775: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 27
2023-02-23 07:25:33.666779: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 18
2023-02-23 07:25:33.666783: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 6
2023-02-23 07:25:33.666787: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 24
2023-02-23 07:25:33.666791: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 6
2023-02-23 07:25:33.666796: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 18
2023-02-23 07:25:33.666800: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 12
2023-02-23 07:25:33.666804: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 23
2023-02-23 07:25:33.666808: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 64, 60, 0.0022, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 60 0.0022 64 1]) is not valid.
[0.4 60 0.0005 32 1] 10
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 505)          638825      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 505)         2020        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 505)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 758)          383548      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 758)          383548      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 758)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1603253     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 3,011,194
Trainable params: 3,007,658
Non-trainable params: 3,536
__________________________________________________________________________________________________
2023-02-23 07:25:39.146366: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 482853056 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 47906816/25447170048
2023-02-23 07:25:39.146454: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24554274092
MaxInUse:                  24595883644
NumAllocs:                   848939770
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 07:25:39.146528: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 07:25:39.146536: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 85
2023-02-23 07:25:39.146541: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 98
2023-02-23 07:25:39.146545: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 07:25:39.146550: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 10
2023-02-23 07:25:39.146554: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 154
2023-02-23 07:25:39.146558: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 381
2023-02-23 07:25:39.146562: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 155
2023-02-23 07:25:39.146566: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 22
2023-02-23 07:25:39.146570: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 88
2023-02-23 07:25:39.146574: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 22
2023-02-23 07:25:39.146578: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 66
2023-02-23 07:25:39.146582: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 44
2023-02-23 07:25:39.146586: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1531160, 3
2023-02-23 07:25:39.146590: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1916224, 63
2023-02-23 07:25:39.146595: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 67
2023-02-23 07:25:39.146599: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 2
2023-02-23 07:25:39.146603: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 42
2023-02-23 07:25:39.146607: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 36
2023-02-23 07:25:39.146611: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 9
2023-02-23 07:25:39.146615: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 36
2023-02-23 07:25:39.146619: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 9
2023-02-23 07:25:39.146623: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 24
2023-02-23 07:25:39.146627: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 27
2023-02-23 07:25:39.146631: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 18
2023-02-23 07:25:39.146635: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 6
2023-02-23 07:25:39.146639: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 24
2023-02-23 07:25:39.146643: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 6
2023-02-23 07:25:39.146648: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 18
2023-02-23 07:25:39.146652: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 12
2023-02-23 07:25:39.146656: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2023-02-23 07:25:39.146660: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.4custom_VAE', 'mse', 32, 60, 0.0005, 0.6, 758, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.4 60 0.0005 32 1]) is not valid.
[1.9 210 0.0005 64 1] 11
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
2023-02-23 07:25:41.496246: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 12139456 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 14352384/25447170048
2023-02-23 07:25:41.496597: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     23378722816
InUse:                     24602902732
MaxInUse:                  24605180240
NumAllocs:                   848939826
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-23 07:25:41.496653: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-23 07:25:41.496660: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 87
2023-02-23 07:25:41.496665: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 99
2023-02-23 07:25:41.496669: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16, 1
2023-02-23 07:25:41.496673: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-23 07:25:41.496677: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 154
2023-02-23 07:25:41.496681: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3032, 381
2023-02-23 07:25:41.496686: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 154
2023-02-23 07:25:41.496690: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 22
2023-02-23 07:25:41.496694: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 88
2023-02-23 07:25:41.496698: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 22
2023-02-23 07:25:41.496702: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9604, 10
2023-02-23 07:25:41.496706: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 66
2023-02-23 07:25:41.496710: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10616, 44
2023-02-23 07:25:41.496714: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1916224, 63
2023-02-23 07:25:41.496718: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2298256, 67
2023-02-23 07:25:41.496722: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 42
2023-02-23 07:25:41.496726: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3832448, 36
2023-02-23 07:25:41.496730: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5363608, 9
2023-02-23 07:25:41.496734: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5748672, 36
2023-02-23 07:25:41.496739: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6130704, 9
2023-02-23 07:25:41.496743: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 24
2023-02-23 07:25:41.496747: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7279832, 3
2023-02-23 07:25:41.496751: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7664896, 27
2023-02-23 07:25:41.496773: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8046928, 18
2023-02-23 07:25:41.496777: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 6
2023-02-23 07:25:41.496782: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 24
2023-02-23 07:25:41.496786: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 6
2023-02-23 07:25:41.496790: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12139456, 3
2023-02-23 07:25:41.496794: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 18
2023-02-23 07:25:41.496798: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 13418624, 12
2023-02-23 07:25:41.496802: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 22
2023-02-23 07:25:41.496806: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 44
2023-02-23 07:25:41.540111: W tensorflow/core/framework/op_kernel.cc:1768] RESOURCE_EXHAUSTED: failed to allocate memory
Traceback (most recent call last):
  File "genetic.py", line 67, in <module>
    main()
  File "genetic.py", line 43, in main
    genetic_hypertune_autoencoder(prefix_name = 'geneticVAE_MMmp_gap',
  File "/auto/globalscratch/users/r/g/rgouvea/ProjectVAE_MODNet/GeneticAlgorithmTest/GeneticVAE_MMmpgap/cr_0.6/../../genetic_hypertune.py", line 211, in genetic_hypertune_autoencoder
    # get scores and genes of ga_instance
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py", line 1413, in run
    self.last_generation_fitness = self.cal_pop_fitness()
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py", line 1235, in cal_pop_fitness
    fitness = self.fitness_func(sol, sol_idx)
  File "/auto/globalscratch/users/r/g/rgouvea/ProjectVAE_MODNet/GeneticAlgorithmTest/GeneticVAE_MMmpgap/cr_0.6/../../genetic_hypertune.py", line 73, in fitness_func
    results_dict=train_autoencoder(prefix_name = prefix_name, 
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/autoencoder_tools-0.0.2-py3.8.egg/autoencoder_tools/autoencoder_setup.py", line 357, in train_autoencoder
    #                            )
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/autoencoder_tools-0.0.2-py3.8.egg/autoencoder_tools/autoencoder_setup.py", line 590, in create_autoencoder
    # output_layer = Dense(n_inputs, activation='linear')(decoder_layers[-1])
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/keras/utils/traceback_utils.py", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/keras/backend.py", line 2100, in random_uniform
    return tf.random.stateless_uniform(
tensorflow.python.framework.errors_impl.ResourceExhaustedError: {{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:AddV2]
Thu Feb 23 07:25:56 CET 2023
done
