start
Tue Feb 21 19:06:53 CET 2023
2023-02-21 19:06:55.248224: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-21 19:06:56.064419: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-02-21 19:06:56.408007: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-02-21 19:07:32,535 - modnet - INFO - Loaded <modnet.preprocessing.MODData object at 0x7f8924d093a0> object, created with modnet version 0.1.12
NAN values: 12054
NAN values remaining: 0
        AtomicOrbitals|HOMO_character  ...  BondFractions|B - B bond frac.
id                                     ...                                
0                                 3.0  ...                             0.0
1                                 3.0  ...                             0.0
2                                 2.0  ...                             0.0
3                                 2.0  ...                             0.0
4                                 2.0  ...                             0.0
...                               ...  ...                             ...
106108                            3.0  ...                             0.0
106109                            2.0  ...                             0.0
106110                            3.0  ...                             0.0
106111                            3.0  ...                             0.0
106112                            1.0  ...                             0.0

[106113 rows x 1336 columns]
/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py:486: UserWarning: The percentage of genes to mutate (mutation_percent_genes=10) resutled in selecting (0) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).
If you do not want to mutate any gene, please set mutation_type=None.
  if not self.suppress_warnings: warnings.warn("The percentage of genes to mutate (mutation_percent_genes={mutation_percent}) resutled in selecting ({mutation_num}) genes. The number of genes to mutate is set to 1 (mutation_num_genes=1).\nIf you do not want to mutate any gene, please set mutation_type=None.".format(mutation_percent=mutation_percent_genes, mutation_num=mutation_num_genes))
/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py:764: UserWarning: Starting from PyGAD 2.6.0, the callback_generation parameter is deprecated and will be removed in a later release of PyGAD. Please use the on_generation parameter instead.
  if not self.suppress_warnings: warnings.warn("Starting from PyGAD 2.6.0, the callback_generation parameter is deprecated and will be removed in a later release of PyGAD. Please use the on_generation parameter instead.")
/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py:820: UserWarning: Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.")
/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py:828: UserWarning: Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.
  if not self.suppress_warnings: warnings.warn("Use the 'save_solutions' parameter with caution as it may cause memory overflow when either the number of generations, number of genes, or number of solutions in population is large.")
[[1.5 60 0.0005 64 1]
 [0.5 150 0.0005 256 1]
 [0.5 210 0.0005 256 1]
 [1.0 60 0.002 128 2]
 [2.5 150 0.0005 64 2]
 [0.5 60 0.0005 256 2]
 [1.5 180 0.001 256 2]
 [2.0 120 0.0005 128 1]
 [1.5 150 0.002 64 2]
 [1.0 60 0.0005 128 2]]
[1.5 60 0.0005 64 1] 0
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
2023-02-21 19:07:35.278467: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-02-21 19:07:36.818303: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0
2023-02-21 19:07:36.818427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20637 MB memory:  -> device: 0, name: NVIDIA A10, pci bus id: 0000:84:00.0, compute capability: 8.6
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          478044      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          478044      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2949844     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 6,311,956
Trainable params: 6,303,868
Non-trainable params: 8,088
__________________________________________________________________________________________________
Epoch 1/60
2023-02-21 19:07:39.521219: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
1493/1493 - 6s - loss: 0.0243 - val_loss: 0.0208 - 6s/epoch - 4ms/step
Epoch 2/60
1493/1493 - 5s - loss: 0.0147 - val_loss: 0.0137 - 5s/epoch - 3ms/step
Epoch 3/60
1493/1493 - 5s - loss: 0.0133 - val_loss: 0.0129 - 5s/epoch - 3ms/step
Epoch 4/60
1493/1493 - 5s - loss: 0.0125 - val_loss: 0.0118 - 5s/epoch - 3ms/step
Epoch 5/60
1493/1493 - 5s - loss: 0.0118 - val_loss: 0.0114 - 5s/epoch - 3ms/step
Epoch 6/60
1493/1493 - 5s - loss: 0.0115 - val_loss: 0.0111 - 5s/epoch - 3ms/step
Epoch 7/60
1493/1493 - 5s - loss: 0.0112 - val_loss: 0.0107 - 5s/epoch - 3ms/step
Epoch 8/60
1493/1493 - 5s - loss: 0.0109 - val_loss: 0.0105 - 5s/epoch - 3ms/step
Epoch 9/60
1493/1493 - 5s - loss: 0.0107 - val_loss: 0.0104 - 5s/epoch - 3ms/step
Epoch 10/60
1493/1493 - 5s - loss: 0.0106 - val_loss: 0.0103 - 5s/epoch - 3ms/step
Epoch 11/60
1493/1493 - 5s - loss: 0.0105 - val_loss: 0.0102 - 5s/epoch - 3ms/step
Epoch 12/60
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 13/60
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 14/60
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 15/60
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 16/60
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 17/60
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 18/60
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 19/60
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 20/60
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 21/60
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 22/60
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 23/60
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 24/60
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 25/60
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 26/60
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 27/60
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 28/60
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 29/60
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 30/60
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 31/60
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 32/60
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 33/60
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 34/60
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 35/60
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 36/60
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 37/60
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 38/60
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 39/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 40/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 41/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 42/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 43/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 44/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 45/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 46/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 47/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 48/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 49/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 50/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 51/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 52/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 53/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 54/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 55/60
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 56/60
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 57/60
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 58/60
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 59/60
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 60/60
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009438239969313145
  1/332 [..............................] - ETA: 42s 45/332 [===>..........................] - ETA: 0s  89/332 [=======>......................] - ETA: 0s133/332 [===========>..................] - ETA: 0s177/332 [==============>...............] - ETA: 0s222/332 [===================>..........] - ETA: 0s266/332 [=======================>......] - ETA: 0s310/332 [===========================>..] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.07158272729992103
cosine 0.05483423558734652
MAE: 0.036448367
RMSE: 0.079077445
r2: 0.5947000928045034
RMSE zero-vector: 0.2430644284356365
['1.5custom_VAE', 'mse', 64, 60, 0.0005, 0.2, 252, 0.00971084926277399, 0.009438239969313145, 0.07158272729992103, 0.05483423558734652, 0.036448366940021515, 0.07907744497060776, 0.5947000928045034, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 150 0.0005 256 1] 1
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
Epoch 1/150
374/374 - 2s - loss: 0.0287 - val_loss: 0.0177 - 2s/epoch - 6ms/step
Epoch 2/150
374/374 - 1s - loss: 0.0144 - val_loss: 0.0148 - 1s/epoch - 3ms/step
Epoch 3/150
374/374 - 1s - loss: 0.0138 - val_loss: 0.0157 - 1s/epoch - 3ms/step
Epoch 4/150
374/374 - 1s - loss: 0.0135 - val_loss: 0.0149 - 1s/epoch - 3ms/step
Epoch 5/150
374/374 - 1s - loss: 0.0134 - val_loss: 0.0136 - 1s/epoch - 3ms/step
Epoch 6/150
374/374 - 1s - loss: 0.0132 - val_loss: 0.0146 - 1s/epoch - 3ms/step
Epoch 7/150
374/374 - 1s - loss: 0.0131 - val_loss: 0.0140 - 1s/epoch - 3ms/step
Epoch 8/150
374/374 - 1s - loss: 0.0129 - val_loss: 0.0141 - 1s/epoch - 3ms/step
Epoch 9/150
374/374 - 1s - loss: 0.0128 - val_loss: 0.0128 - 1s/epoch - 3ms/step
Epoch 10/150
374/374 - 1s - loss: 0.0124 - val_loss: 0.0124 - 1s/epoch - 3ms/step
Epoch 11/150
374/374 - 1s - loss: 0.0121 - val_loss: 0.0121 - 1s/epoch - 3ms/step
Epoch 12/150
374/374 - 1s - loss: 0.0118 - val_loss: 0.0122 - 1s/epoch - 3ms/step
Epoch 13/150
374/374 - 1s - loss: 0.0116 - val_loss: 0.0116 - 1s/epoch - 3ms/step
Epoch 14/150
374/374 - 1s - loss: 0.0114 - val_loss: 0.0115 - 1s/epoch - 3ms/step
Epoch 15/150
374/374 - 1s - loss: 0.0112 - val_loss: 0.0116 - 1s/epoch - 3ms/step
Epoch 16/150
374/374 - 1s - loss: 0.0110 - val_loss: 0.0112 - 1s/epoch - 3ms/step
Epoch 17/150
374/374 - 1s - loss: 0.0108 - val_loss: 0.0108 - 1s/epoch - 3ms/step
Epoch 18/150
374/374 - 1s - loss: 0.0107 - val_loss: 0.0114 - 1s/epoch - 3ms/step
Epoch 19/150
374/374 - 1s - loss: 0.0107 - val_loss: 0.0105 - 1s/epoch - 3ms/step
Epoch 20/150
374/374 - 1s - loss: 0.0105 - val_loss: 0.0105 - 1s/epoch - 3ms/step
Epoch 21/150
374/374 - 1s - loss: 0.0104 - val_loss: 0.0103 - 1s/epoch - 3ms/step
Epoch 22/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 3ms/step
Epoch 23/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 3ms/step
Epoch 24/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0102 - 1s/epoch - 3ms/step
Epoch 25/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0102 - 1s/epoch - 3ms/step
Epoch 26/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0101 - 1s/epoch - 3ms/step
Epoch 27/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 3ms/step
Epoch 28/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0100 - 1s/epoch - 3ms/step
Epoch 29/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 3ms/step
Epoch 30/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0098 - 1s/epoch - 3ms/step
Epoch 31/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0099 - 1s/epoch - 3ms/step
Epoch 32/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0099 - 1s/epoch - 3ms/step
Epoch 33/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 3ms/step
Epoch 34/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 3ms/step
Epoch 35/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 3ms/step
Epoch 36/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 3ms/step
Epoch 37/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 3ms/step
Epoch 38/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 3ms/step
Epoch 39/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 3ms/step
Epoch 40/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 3ms/step
Epoch 41/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 42/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 43/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 44/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 45/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 46/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 47/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 48/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 49/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 50/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 51/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 52/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 53/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 54/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 55/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 56/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 57/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 58/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 59/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 60/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 61/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 62/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 63/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 64/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 65/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 66/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 67/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 68/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 69/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 70/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 71/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 72/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 73/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 74/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 75/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 76/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 77/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 78/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 79/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 80/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 81/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 82/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 83/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 84/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 85/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 86/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 87/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 88/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 89/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 90/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 91/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 92/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 93/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 94/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 95/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 96/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 97/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 98/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 99/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 100/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 101/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 102/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 103/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 104/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 105/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 106/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 107/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 108/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 109/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 110/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 111/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 112/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 113/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 114/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 115/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 116/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 117/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 118/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 119/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 120/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 121/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 122/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 123/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 124/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 125/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 126/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 127/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 128/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 129/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 130/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 131/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 132/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 133/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 134/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 135/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 136/150
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 137/150
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 138/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 139/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 140/150
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 141/150
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 142/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 143/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 144/150
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 145/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 146/150
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 147/150
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 148/150
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 149/150
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 150/150
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009129649959504604
  1/332 [..............................] - ETA: 34s 45/332 [===>..........................] - ETA: 0s  89/332 [=======>......................] - ETA: 0s133/332 [===========>..................] - ETA: 0s177/332 [==============>...............] - ETA: 0s222/332 [===================>..........] - ETA: 0s267/332 [=======================>......] - ETA: 0s312/332 [===========================>..] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.06532207472882448
cosine 0.05004268702115376
MAE: 0.03466824
RMSE: 0.07563158
r2: 0.6292528715195276
RMSE zero-vector: 0.2430644284356365
['0.5custom_VAE', 'mse', 256, 150, 0.0005, 0.2, 252, 0.00923057273030281, 0.009129649959504604, 0.06532207472882448, 0.05004268702115376, 0.03466824069619179, 0.07563158124685287, 0.6292528715195276, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 210 0.0005 256 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
Epoch 1/210
374/374 - 2s - loss: 0.0289 - val_loss: 0.0153 - 2s/epoch - 6ms/step
Epoch 2/210
374/374 - 1s - loss: 0.0142 - val_loss: 0.0146 - 1s/epoch - 3ms/step
Epoch 3/210
374/374 - 1s - loss: 0.0138 - val_loss: 0.0171 - 1s/epoch - 3ms/step
Epoch 4/210
374/374 - 1s - loss: 0.0136 - val_loss: 0.0144 - 1s/epoch - 3ms/step
Epoch 5/210
374/374 - 1s - loss: 0.0133 - val_loss: 0.0135 - 1s/epoch - 3ms/step
Epoch 6/210
374/374 - 1s - loss: 0.0132 - val_loss: 0.0137 - 1s/epoch - 3ms/step
Epoch 7/210
374/374 - 1s - loss: 0.0130 - val_loss: 0.0134 - 1s/epoch - 3ms/step
Epoch 8/210
374/374 - 1s - loss: 0.0129 - val_loss: 0.0132 - 1s/epoch - 3ms/step
Epoch 9/210
374/374 - 1s - loss: 0.0128 - val_loss: 0.0131 - 1s/epoch - 3ms/step
Epoch 10/210
374/374 - 1s - loss: 0.0126 - val_loss: 0.0127 - 1s/epoch - 3ms/step
Epoch 11/210
374/374 - 1s - loss: 0.0122 - val_loss: 0.0125 - 1s/epoch - 3ms/step
Epoch 12/210
374/374 - 1s - loss: 0.0118 - val_loss: 0.0121 - 1s/epoch - 3ms/step
Epoch 13/210
374/374 - 1s - loss: 0.0115 - val_loss: 0.0180 - 1s/epoch - 3ms/step
Epoch 14/210
374/374 - 1s - loss: 0.0115 - val_loss: 0.0113 - 1s/epoch - 3ms/step
Epoch 15/210
374/374 - 1s - loss: 0.0110 - val_loss: 0.0111 - 1s/epoch - 3ms/step
Epoch 16/210
374/374 - 1s - loss: 0.0109 - val_loss: 0.0107 - 1s/epoch - 3ms/step
Epoch 17/210
374/374 - 1s - loss: 0.0107 - val_loss: 0.0111 - 1s/epoch - 3ms/step
Epoch 18/210
374/374 - 1s - loss: 0.0106 - val_loss: 0.0115 - 1s/epoch - 3ms/step
Epoch 19/210
374/374 - 1s - loss: 0.0106 - val_loss: 0.0103 - 1s/epoch - 3ms/step
Epoch 20/210
374/374 - 1s - loss: 0.0104 - val_loss: 0.0103 - 1s/epoch - 3ms/step
Epoch 21/210
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 3ms/step
Epoch 22/210
374/374 - 1s - loss: 0.0102 - val_loss: 0.0106 - 1s/epoch - 3ms/step
Epoch 23/210
374/374 - 1s - loss: 0.0103 - val_loss: 0.0101 - 1s/epoch - 3ms/step
Epoch 24/210
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 3ms/step
Epoch 25/210
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 3ms/step
Epoch 26/210
374/374 - 1s - loss: 0.0100 - val_loss: 0.0100 - 1s/epoch - 3ms/step
Epoch 27/210
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 3ms/step
Epoch 28/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0099 - 1s/epoch - 3ms/step
Epoch 29/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 3ms/step
Epoch 30/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 3ms/step
Epoch 31/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0098 - 1s/epoch - 3ms/step
Epoch 32/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 3ms/step
Epoch 33/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 3ms/step
Epoch 34/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 3ms/step
Epoch 35/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 3ms/step
Epoch 36/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 37/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0097 - 1s/epoch - 3ms/step
Epoch 38/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 39/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 40/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 41/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 42/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 43/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 44/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 45/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 46/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 47/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 48/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 49/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 50/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 51/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 52/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 53/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 54/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 55/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 56/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 57/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 58/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 59/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 60/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 61/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 62/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 63/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 64/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 65/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 66/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 67/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 68/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 69/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 70/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 71/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 72/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 73/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 74/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 75/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 76/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 77/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 78/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 79/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 80/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 81/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 82/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 83/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 84/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 85/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 86/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 87/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 88/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 89/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 90/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 91/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 92/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 93/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 94/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 95/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 96/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 97/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 98/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 99/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 100/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 101/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 102/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 103/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 104/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 105/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 106/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 107/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 108/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 109/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 110/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 111/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 112/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 113/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 114/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 115/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 116/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 117/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 118/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 119/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 120/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 121/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 122/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 123/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 124/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 125/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 126/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 127/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 128/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 129/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 130/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 131/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 132/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 133/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 134/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 135/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 136/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 137/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 138/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 139/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 140/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 141/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 142/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 143/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 144/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 145/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 146/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 147/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 148/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 149/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 150/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 151/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 152/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 153/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 154/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 155/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 156/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 157/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 158/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 159/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 160/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 161/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 162/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 163/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 164/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 165/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 166/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 167/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 168/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 169/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 170/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 171/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 172/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 173/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 174/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 175/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 176/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 177/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 178/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 179/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 180/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 181/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 182/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 183/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 184/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 185/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 186/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 187/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 188/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 189/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 190/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 191/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 192/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 193/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 194/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 195/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 196/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 197/210
374/374 - 1s - loss: 0.0091 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 198/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 199/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 200/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 201/210
374/374 - 1s - loss: 0.0091 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 202/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 203/210
374/374 - 1s - loss: 0.0091 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 204/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 205/210
374/374 - 1s - loss: 0.0091 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 206/210
374/374 - 1s - loss: 0.0091 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 207/210
374/374 - 1s - loss: 0.0091 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 208/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 209/210
374/374 - 1s - loss: 0.0091 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 210/210
374/374 - 1s - loss: 0.0091 - val_loss: 0.0091 - 1s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009070364758372307
  1/332 [..............................] - ETA: 35s 45/332 [===>..........................] - ETA: 0s  89/332 [=======>......................] - ETA: 0s133/332 [===========>..................] - ETA: 0s178/332 [===============>..............] - ETA: 0s223/332 [===================>..........] - ETA: 0s268/332 [=======================>......] - ETA: 0s313/332 [===========================>..] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.0635686559336526
cosine 0.04870769084960256
MAE: 0.034070168
RMSE: 0.074625336
r2: 0.6390525515866164
RMSE zero-vector: 0.2430644284356365
['0.5custom_VAE', 'mse', 256, 210, 0.0005, 0.2, 252, 0.009139631874859333, 0.009070364758372307, 0.0635686559336526, 0.04870769084960256, 0.03407016769051552, 0.07462533563375473, 0.6390525515866164, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.0 60 0.002 128 2] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1988572     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 4,230,148
Trainable params: 4,224,588
Non-trainable params: 5,560
__________________________________________________________________________________________________
Epoch 1/60
747/747 - 3s - loss: 0.0144 - val_loss: 0.0085 - 3s/epoch - 5ms/step
Epoch 2/60
747/747 - 2s - loss: 0.0078 - val_loss: 0.0080 - 2s/epoch - 3ms/step
Epoch 3/60
747/747 - 2s - loss: 0.0075 - val_loss: 0.0079 - 2s/epoch - 3ms/step
Epoch 4/60
747/747 - 2s - loss: 0.0073 - val_loss: 0.0079 - 2s/epoch - 3ms/step
Epoch 5/60
747/747 - 2s - loss: 0.0071 - val_loss: 0.0070 - 2s/epoch - 3ms/step
Epoch 6/60
747/747 - 2s - loss: 0.0069 - val_loss: 0.0070 - 2s/epoch - 3ms/step
Epoch 7/60
747/747 - 2s - loss: 0.0069 - val_loss: 0.0069 - 2s/epoch - 3ms/step
Epoch 8/60
747/747 - 2s - loss: 0.0068 - val_loss: 0.0068 - 2s/epoch - 3ms/step
Epoch 9/60
747/747 - 2s - loss: 0.0068 - val_loss: 0.0067 - 2s/epoch - 3ms/step
Epoch 10/60
747/747 - 2s - loss: 0.0067 - val_loss: 0.0067 - 2s/epoch - 3ms/step
Epoch 11/60
747/747 - 2s - loss: 0.0066 - val_loss: 0.0065 - 2s/epoch - 3ms/step
Epoch 12/60
747/747 - 2s - loss: 0.0064 - val_loss: 0.0064 - 2s/epoch - 3ms/step
Epoch 13/60
747/747 - 2s - loss: 0.0064 - val_loss: 0.0062 - 2s/epoch - 3ms/step
Epoch 14/60
747/747 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 3ms/step
Epoch 15/60
747/747 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 3ms/step
Epoch 16/60
747/747 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 3ms/step
Epoch 17/60
747/747 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 3ms/step
Epoch 18/60
747/747 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 3ms/step
Epoch 19/60
747/747 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 3ms/step
Epoch 20/60
747/747 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 3ms/step
Epoch 21/60
747/747 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 3ms/step
Epoch 22/60
747/747 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 23/60
747/747 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 24/60
747/747 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 25/60
747/747 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 26/60
747/747 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 27/60
747/747 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 28/60
747/747 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 29/60
747/747 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 30/60
747/747 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 31/60
747/747 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 32/60
747/747 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 33/60
747/747 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 34/60
747/747 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 35/60
747/747 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 36/60
747/747 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 37/60
747/747 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 38/60
747/747 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 39/60
747/747 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 40/60
747/747 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 41/60
747/747 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 42/60
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 43/60
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 44/60
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 45/60
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 46/60
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 47/60
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 48/60
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 49/60
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 50/60
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 51/60
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 52/60
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 53/60
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 54/60
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 55/60
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 56/60
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 57/60
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 58/60
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 59/60
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 60/60
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.005932534113526344
  1/332 [..............................] - ETA: 35s 45/332 [===>..........................] - ETA: 0s  89/332 [=======>......................] - ETA: 0s134/332 [===========>..................] - ETA: 0s179/332 [===============>..............] - ETA: 0s224/332 [===================>..........] - ETA: 0s269/332 [=======================>......] - ETA: 0s314/332 [===========================>..] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.11283159784514905
cosine 0.08616698393721064
MAE: 0.045856643
RMSE: 0.098337494
r2: 0.37322768861584044
RMSE zero-vector: 0.2430644284356365
['1.0custom_VAE', 'logcosh', 128, 60, 0.002, 0.2, 252, 0.006006147712469101, 0.005932534113526344, 0.11283159784514905, 0.08616698393721064, 0.045856643468141556, 0.09833749383687973, 0.37322768861584044, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.5 150 0.0005 64 2] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 3160)         3997400     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 3160)        12640       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 3160)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          796572      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          796572      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         4872388     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 10,475,572
Trainable params: 10,462,428
Non-trainable params: 13,144
__________________________________________________________________________________________________
Epoch 1/150
1493/1493 - 6s - loss: 0.0146 - val_loss: 0.0102 - 6s/epoch - 4ms/step
Epoch 2/150
1493/1493 - 5s - loss: 0.0094 - val_loss: 0.0076 - 5s/epoch - 3ms/step
Epoch 3/150
1493/1493 - 5s - loss: 0.0071 - val_loss: 0.0069 - 5s/epoch - 3ms/step
Epoch 4/150
1493/1493 - 5s - loss: 0.0066 - val_loss: 0.0064 - 5s/epoch - 3ms/step
Epoch 5/150
1493/1493 - 5s - loss: 0.0064 - val_loss: 0.0063 - 5s/epoch - 3ms/step
Epoch 6/150
1493/1493 - 5s - loss: 0.0064 - val_loss: 0.0064 - 5s/epoch - 3ms/step
Epoch 7/150
1493/1493 - 5s - loss: 0.0064 - val_loss: 0.0064 - 5s/epoch - 3ms/step
Epoch 8/150
1493/1493 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 9/150
1493/1493 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 10/150
1493/1493 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 11/150
1493/1493 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 12/150
1493/1493 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 13/150
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 14/150
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 15/150
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 16/150
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 17/150
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 18/150
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 19/150
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 20/150
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 21/150
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 22/150
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 23/150
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 24/150
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 25/150
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 26/150
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 27/150
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 28/150
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 29/150
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 30/150
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 31/150
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 32/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 33/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 34/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 35/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 36/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 37/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 38/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 39/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 40/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 41/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 42/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 43/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 44/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 45/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 46/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 47/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 48/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 49/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 50/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 51/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 52/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 53/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 54/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 55/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 56/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 57/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 58/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 59/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 60/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 61/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 62/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 63/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 64/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 65/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 66/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 67/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 68/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 69/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 70/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 71/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 72/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 73/150
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 74/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 75/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 76/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 77/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 78/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 79/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 80/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 81/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 82/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 83/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 84/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 85/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 86/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 87/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 88/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 89/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 90/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 91/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 92/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 93/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 94/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 95/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 96/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 97/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 98/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 99/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 100/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 101/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 102/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 103/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 104/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 105/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 106/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 107/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 108/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 109/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 110/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 111/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 112/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 113/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 114/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 115/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 116/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 117/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 118/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 119/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 120/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 121/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 122/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 123/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 124/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 125/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 126/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 127/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 128/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 129/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 130/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 131/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 132/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 133/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 134/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 135/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 136/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 137/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 138/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 139/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 140/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 141/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 142/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 143/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 144/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 145/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 146/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 147/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 148/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 149/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 150/150
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.00598424905911088
  1/332 [..............................] - ETA: 36s 45/332 [===>..........................] - ETA: 0s  90/332 [=======>......................] - ETA: 0s135/332 [===========>..................] - ETA: 0s180/332 [===============>..............] - ETA: 0s224/332 [===================>..........] - ETA: 0s269/332 [=======================>......] - ETA: 0s314/332 [===========================>..] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.1140400727728471
cosine 0.0871508307853286
MAE: 0.046176236
RMSE: 0.09918094
r2: 0.3624298697380345
RMSE zero-vector: 0.2430644284356365
['2.5custom_VAE', 'logcosh', 64, 150, 0.0005, 0.2, 252, 0.006015771068632603, 0.00598424905911088, 0.1140400727728471, 0.0871508307853286, 0.0461762361228466, 0.09918093681335449, 0.3624298697380345, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 60 0.0005 256 2] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
Epoch 1/60
374/374 - 2s - loss: 0.0161 - val_loss: 0.0080 - 2s/epoch - 5ms/step
Epoch 2/60
374/374 - 1s - loss: 0.0074 - val_loss: 0.0077 - 1s/epoch - 3ms/step
Epoch 3/60
374/374 - 1s - loss: 0.0073 - val_loss: 0.0079 - 1s/epoch - 3ms/step
Epoch 4/60
374/374 - 1s - loss: 0.0072 - val_loss: 0.0076 - 1s/epoch - 3ms/step
Epoch 5/60
374/374 - 1s - loss: 0.0071 - val_loss: 0.0073 - 1s/epoch - 3ms/step
Epoch 6/60
374/374 - 1s - loss: 0.0071 - val_loss: 0.0076 - 1s/epoch - 3ms/step
Epoch 7/60
374/374 - 1s - loss: 0.0070 - val_loss: 0.0074 - 1s/epoch - 3ms/step
Epoch 8/60
374/374 - 1s - loss: 0.0070 - val_loss: 0.0073 - 1s/epoch - 3ms/step
Epoch 9/60
374/374 - 1s - loss: 0.0070 - val_loss: 0.0073 - 1s/epoch - 3ms/step
Epoch 10/60
374/374 - 1s - loss: 0.0069 - val_loss: 0.0071 - 1s/epoch - 3ms/step
Epoch 11/60
374/374 - 1s - loss: 0.0069 - val_loss: 0.0069 - 1s/epoch - 3ms/step
Epoch 12/60
374/374 - 1s - loss: 0.0068 - val_loss: 0.0069 - 1s/epoch - 3ms/step
Epoch 13/60
374/374 - 1s - loss: 0.0067 - val_loss: 0.0067 - 1s/epoch - 3ms/step
Epoch 14/60
374/374 - 1s - loss: 0.0065 - val_loss: 0.0066 - 1s/epoch - 3ms/step
Epoch 15/60
374/374 - 1s - loss: 0.0065 - val_loss: 0.0065 - 1s/epoch - 3ms/step
Epoch 16/60
374/374 - 1s - loss: 0.0064 - val_loss: 0.0066 - 1s/epoch - 3ms/step
Epoch 17/60
374/374 - 1s - loss: 0.0064 - val_loss: 0.0063 - 1s/epoch - 3ms/step
Epoch 18/60
374/374 - 1s - loss: 0.0063 - val_loss: 0.0068 - 1s/epoch - 3ms/step
Epoch 19/60
374/374 - 1s - loss: 0.0063 - val_loss: 0.0063 - 1s/epoch - 3ms/step
Epoch 20/60
374/374 - 1s - loss: 0.0063 - val_loss: 0.0063 - 1s/epoch - 3ms/step
Epoch 21/60
374/374 - 1s - loss: 0.0063 - val_loss: 0.0062 - 1s/epoch - 3ms/step
Epoch 22/60
374/374 - 1s - loss: 0.0063 - val_loss: 0.0063 - 1s/epoch - 3ms/step
Epoch 23/60
374/374 - 1s - loss: 0.0062 - val_loss: 0.0065 - 1s/epoch - 3ms/step
Epoch 24/60
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 3ms/step
Epoch 25/60
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 3ms/step
Epoch 26/60
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 3ms/step
Epoch 27/60
374/374 - 1s - loss: 0.0062 - val_loss: 0.0065 - 1s/epoch - 3ms/step
Epoch 28/60
374/374 - 1s - loss: 0.0062 - val_loss: 0.0063 - 1s/epoch - 3ms/step
Epoch 29/60
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 30/60
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 31/60
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 32/60
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 33/60
374/374 - 1s - loss: 0.0061 - val_loss: 0.0062 - 1s/epoch - 3ms/step
Epoch 34/60
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 35/60
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 36/60
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 37/60
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 38/60
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 39/60
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 40/60
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 41/60
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 42/60
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 43/60
374/374 - 1s - loss: 0.0060 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 44/60
374/374 - 1s - loss: 0.0061 - val_loss: 0.0062 - 1s/epoch - 3ms/step
Epoch 45/60
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 46/60
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 47/60
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 48/60
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 49/60
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 50/60
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 51/60
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 52/60
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 53/60
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 54/60
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 55/60
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 56/60
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 57/60
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 58/60
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 59/60
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 60/60
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.005930242594331503
  1/332 [..............................] - ETA: 33s 45/332 [===>..........................] - ETA: 0s  89/332 [=======>......................] - ETA: 0s134/332 [===========>..................] - ETA: 0s179/332 [===============>..............] - ETA: 0s224/332 [===================>..........] - ETA: 0s268/332 [=======================>......] - ETA: 0s312/332 [===========================>..] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.11314233492173836
cosine 0.08639605163045079
MAE: 0.045887604
RMSE: 0.09845615
r2: 0.37171426069627733
RMSE zero-vector: 0.2430644284356365
['0.5custom_VAE', 'logcosh', 256, 60, 0.0005, 0.2, 252, 0.005987787153571844, 0.005930242594331503, 0.11314233492173836, 0.08639605163045079, 0.04588760435581207, 0.09845615178346634, 0.37171426069627733, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 180 0.001 256 2] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          478044      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          478044      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2949844     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 6,311,956
Trainable params: 6,303,868
Non-trainable params: 8,088
__________________________________________________________________________________________________
Epoch 1/180
374/374 - 2s - loss: 0.0198 - val_loss: 0.0214 - 2s/epoch - 6ms/step
Epoch 2/180
374/374 - 1s - loss: 0.0085 - val_loss: 0.0129 - 1s/epoch - 3ms/step
Epoch 3/180
374/374 - 1s - loss: 0.0079 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 4/180
374/374 - 1s - loss: 0.0077 - val_loss: 0.0104 - 1s/epoch - 3ms/step
Epoch 5/180
374/374 - 1s - loss: 0.0075 - val_loss: 0.0081 - 1s/epoch - 3ms/step
Epoch 6/180
374/374 - 1s - loss: 0.0074 - val_loss: 0.0085 - 1s/epoch - 3ms/step
Epoch 7/180
374/374 - 1s - loss: 0.0073 - val_loss: 0.0084 - 1s/epoch - 3ms/step
Epoch 8/180
374/374 - 1s - loss: 0.0073 - val_loss: 0.0088 - 1s/epoch - 3ms/step
Epoch 9/180
374/374 - 1s - loss: 0.0072 - val_loss: 0.0080 - 1s/epoch - 3ms/step
Epoch 10/180
374/374 - 1s - loss: 0.0070 - val_loss: 0.0075 - 1s/epoch - 3ms/step
Epoch 11/180
374/374 - 1s - loss: 0.0070 - val_loss: 0.0072 - 1s/epoch - 3ms/step
Epoch 12/180
374/374 - 1s - loss: 0.0069 - val_loss: 0.0073 - 1s/epoch - 3ms/step
Epoch 13/180
374/374 - 1s - loss: 0.0069 - val_loss: 0.0219 - 1s/epoch - 3ms/step
Epoch 14/180
374/374 - 1s - loss: 0.0073 - val_loss: 0.0070 - 1s/epoch - 3ms/step
Epoch 15/180
374/374 - 1s - loss: 0.0066 - val_loss: 0.0079 - 1s/epoch - 3ms/step
Epoch 16/180
374/374 - 1s - loss: 0.0066 - val_loss: 0.0065 - 1s/epoch - 3ms/step
Epoch 17/180
374/374 - 1s - loss: 0.0064 - val_loss: 0.0065 - 1s/epoch - 3ms/step
Epoch 18/180
374/374 - 1s - loss: 0.0064 - val_loss: 0.0063 - 1s/epoch - 3ms/step
Epoch 19/180
374/374 - 1s - loss: 0.0063 - val_loss: 0.0063 - 1s/epoch - 3ms/step
Epoch 20/180
374/374 - 1s - loss: 0.0063 - val_loss: 0.0068 - 1s/epoch - 3ms/step
Epoch 21/180
374/374 - 1s - loss: 0.0068 - val_loss: 0.0063 - 1s/epoch - 3ms/step
Epoch 22/180
374/374 - 1s - loss: 0.0064 - val_loss: 0.0063 - 1s/epoch - 3ms/step
Epoch 23/180
374/374 - 1s - loss: 0.0066 - val_loss: 0.0066 - 1s/epoch - 3ms/step
Epoch 24/180
374/374 - 1s - loss: 0.0080 - val_loss: 0.0082 - 1s/epoch - 3ms/step
Epoch 25/180
374/374 - 1s - loss: 0.0074 - val_loss: 0.0065 - 1s/epoch - 3ms/step
Epoch 26/180
374/374 - 1s - loss: 0.0067 - val_loss: 0.0064 - 1s/epoch - 3ms/step
Epoch 27/180
374/374 - 1s - loss: 0.0064 - val_loss: 0.0063 - 1s/epoch - 3ms/step
Epoch 28/180
374/374 - 1s - loss: 0.0064 - val_loss: 0.0065 - 1s/epoch - 3ms/step
Epoch 29/180
374/374 - 1s - loss: 0.0069 - val_loss: 0.0063 - 1s/epoch - 3ms/step
Epoch 30/180
374/374 - 1s - loss: 0.0064 - val_loss: 0.0063 - 1s/epoch - 3ms/step
Epoch 31/180
374/374 - 1s - loss: 0.0063 - val_loss: 0.0063 - 1s/epoch - 3ms/step
Epoch 32/180
374/374 - 1s - loss: 0.0065 - val_loss: 0.0062 - 1s/epoch - 3ms/step
Epoch 33/180
374/374 - 1s - loss: 0.0063 - val_loss: 0.0062 - 1s/epoch - 3ms/step
Epoch 34/180
374/374 - 1s - loss: 0.0063 - val_loss: 0.0062 - 1s/epoch - 3ms/step
Epoch 35/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 3ms/step
Epoch 36/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 37/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 3ms/step
Epoch 38/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 3ms/step
Epoch 39/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 40/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0063 - 1s/epoch - 3ms/step
Epoch 41/180
374/374 - 1s - loss: 0.0065 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 42/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 43/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 44/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0062 - 1s/epoch - 3ms/step
Epoch 45/180
374/374 - 1s - loss: 0.0063 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 46/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 47/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 48/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 49/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 50/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 51/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 52/180
374/374 - 1s - loss: 0.0063 - val_loss: 0.0063 - 1s/epoch - 3ms/step
Epoch 53/180
374/374 - 1s - loss: 0.0077 - val_loss: 0.0069 - 1s/epoch - 3ms/step
Epoch 54/180
374/374 - 1s - loss: 0.0094 - val_loss: 0.0064 - 1s/epoch - 3ms/step
Epoch 55/180
374/374 - 1s - loss: 0.0063 - val_loss: 0.0063 - 1s/epoch - 3ms/step
Epoch 56/180
374/374 - 1s - loss: 0.0063 - val_loss: 0.0062 - 1s/epoch - 3ms/step
Epoch 57/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 3ms/step
Epoch 58/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 3ms/step
Epoch 59/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 60/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 61/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 62/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 63/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0064 - 1s/epoch - 3ms/step
Epoch 64/180
374/374 - 1s - loss: 0.0063 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 65/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 66/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 67/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 68/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 69/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 70/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 71/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0069 - 1s/epoch - 3ms/step
Epoch 72/180
374/374 - 1s - loss: 0.0066 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 73/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0066 - 1s/epoch - 3ms/step
Epoch 74/180
374/374 - 1s - loss: 0.0064 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 75/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 76/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 77/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 78/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 79/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 80/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 81/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 82/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 83/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 84/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 85/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 86/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 87/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 88/180
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 89/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 90/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 91/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 92/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 93/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0063 - 1s/epoch - 3ms/step
Epoch 94/180
374/374 - 1s - loss: 0.0064 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 95/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 96/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 97/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 98/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 99/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 100/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 101/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 102/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 103/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 104/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 105/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0063 - 1s/epoch - 3ms/step
Epoch 106/180
374/374 - 1s - loss: 0.0065 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 107/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 108/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 3ms/step
Epoch 109/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 110/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 111/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 112/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 113/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 114/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 115/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0062 - 1s/epoch - 3ms/step
Epoch 116/180
374/374 - 1s - loss: 0.0062 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 117/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 118/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 119/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 120/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 121/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 122/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 123/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 124/180
374/374 - 1s - loss: 0.0064 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 125/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 126/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 127/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 128/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0061 - 1s/epoch - 3ms/step
Epoch 129/180
374/374 - 1s - loss: 0.0063 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 130/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 131/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 132/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 133/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 134/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 135/180
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 136/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 137/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 138/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 139/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 140/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 141/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 142/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 143/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 144/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 145/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 146/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 147/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 148/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 149/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 150/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 151/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 152/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 153/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 154/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 155/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 156/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 157/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 158/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 159/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 160/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 161/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 3ms/step
Epoch 162/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 163/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 3ms/step
Epoch 164/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 165/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 166/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 167/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 3ms/step
Epoch 168/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 3ms/step
Epoch 169/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 170/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 171/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 172/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 173/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 3ms/step
Epoch 174/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 175/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 3ms/step
Epoch 176/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 3ms/step
Epoch 177/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 178/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 3ms/step
Epoch 179/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 3ms/step
Epoch 180/180
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.005842146929353476
  1/332 [..............................] - ETA: 34s 45/332 [===>..........................] - ETA: 0s  90/332 [=======>......................] - ETA: 0s134/332 [===========>..................] - ETA: 0s179/332 [===============>..............] - ETA: 0s224/332 [===================>..........] - ETA: 0s269/332 [=======================>......] - ETA: 0s314/332 [===========================>..] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.10606038101407837
cosine 0.08098879619966666
MAE: 0.04427755
RMSE: 0.09542691
r2: 0.40978073492670297
RMSE zero-vector: 0.2430644284356365
['1.5custom_VAE', 'logcosh', 256, 180, 0.001, 0.2, 252, 0.00588833587244153, 0.005842146929353476, 0.10606038101407837, 0.08098879619966666, 0.04427754878997803, 0.09542690962553024, 0.40978073492670297, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[2.0 120 0.0005 128 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2528)         3197920     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2528)        10112       ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2528)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          637308      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3911116     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 8,393,764
Trainable params: 8,383,148
Non-trainable params: 10,616
__________________________________________________________________________________________________
Epoch 1/120
747/747 - 3s - loss: 0.0308 - val_loss: 0.0200 - 3s/epoch - 4ms/step
Epoch 2/120
747/747 - 3s - loss: 0.0157 - val_loss: 0.0432 - 3s/epoch - 3ms/step
Epoch 3/120
747/747 - 3s - loss: 0.0153 - val_loss: 0.0263 - 3s/epoch - 3ms/step
Epoch 4/120
747/747 - 3s - loss: 0.0141 - val_loss: 0.0169 - 3s/epoch - 3ms/step
Epoch 5/120
747/747 - 3s - loss: 0.0133 - val_loss: 0.0167 - 3s/epoch - 3ms/step
Epoch 6/120
747/747 - 3s - loss: 0.0124 - val_loss: 0.0122 - 3s/epoch - 3ms/step
Epoch 7/120
747/747 - 3s - loss: 0.0117 - val_loss: 0.0116 - 3s/epoch - 3ms/step
Epoch 8/120
747/747 - 3s - loss: 0.0114 - val_loss: 0.0110 - 3s/epoch - 3ms/step
Epoch 9/120
747/747 - 3s - loss: 0.0111 - val_loss: 0.0140 - 3s/epoch - 3ms/step
Epoch 10/120
747/747 - 3s - loss: 0.0125 - val_loss: 0.0110 - 3s/epoch - 3ms/step
Epoch 11/120
747/747 - 3s - loss: 0.0110 - val_loss: 0.0125 - 3s/epoch - 3ms/step
Epoch 12/120
747/747 - 3s - loss: 0.0140 - val_loss: 0.0111 - 3s/epoch - 3ms/step
Epoch 13/120
747/747 - 3s - loss: 0.0111 - val_loss: 0.0106 - 3s/epoch - 3ms/step
Epoch 14/120
747/747 - 3s - loss: 0.0108 - val_loss: 0.0104 - 3s/epoch - 3ms/step
Epoch 15/120
747/747 - 3s - loss: 0.0106 - val_loss: 0.0103 - 3s/epoch - 3ms/step
Epoch 16/120
747/747 - 3s - loss: 0.0106 - val_loss: 0.0103 - 3s/epoch - 3ms/step
Epoch 17/120
747/747 - 3s - loss: 0.0105 - val_loss: 0.0103 - 3s/epoch - 3ms/step
Epoch 18/120
747/747 - 3s - loss: 0.0104 - val_loss: 0.0101 - 3s/epoch - 3ms/step
Epoch 19/120
747/747 - 3s - loss: 0.0104 - val_loss: 0.0101 - 3s/epoch - 3ms/step
Epoch 20/120
747/747 - 3s - loss: 0.0103 - val_loss: 0.0101 - 3s/epoch - 3ms/step
Epoch 21/120
747/747 - 3s - loss: 0.0102 - val_loss: 0.0101 - 3s/epoch - 3ms/step
Epoch 22/120
747/747 - 3s - loss: 0.0102 - val_loss: 0.0099 - 3s/epoch - 3ms/step
Epoch 23/120
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 3ms/step
Epoch 24/120
747/747 - 3s - loss: 0.0103 - val_loss: 0.0113 - 3s/epoch - 3ms/step
Epoch 25/120
747/747 - 3s - loss: 0.0118 - val_loss: 0.0104 - 3s/epoch - 3ms/step
Epoch 26/120
747/747 - 3s - loss: 0.0106 - val_loss: 0.0101 - 3s/epoch - 3ms/step
Epoch 27/120
747/747 - 3s - loss: 0.0102 - val_loss: 0.0103 - 3s/epoch - 3ms/step
Epoch 28/120
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 3ms/step
Epoch 29/120
747/747 - 3s - loss: 0.0101 - val_loss: 0.0099 - 3s/epoch - 3ms/step
Epoch 30/120
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 3ms/step
Epoch 31/120
747/747 - 3s - loss: 0.0104 - val_loss: 0.0099 - 3s/epoch - 3ms/step
Epoch 32/120
747/747 - 3s - loss: 0.0100 - val_loss: 0.0099 - 3s/epoch - 3ms/step
Epoch 33/120
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 3ms/step
Epoch 34/120
747/747 - 3s - loss: 0.0100 - val_loss: 0.0116 - 3s/epoch - 3ms/step
Epoch 35/120
747/747 - 3s - loss: 0.0117 - val_loss: 0.0099 - 3s/epoch - 3ms/step
Epoch 36/120
747/747 - 3s - loss: 0.0101 - val_loss: 0.0106 - 3s/epoch - 3ms/step
Epoch 37/120
747/747 - 3s - loss: 0.0105 - val_loss: 0.0098 - 3s/epoch - 3ms/step
Epoch 38/120
747/747 - 3s - loss: 0.0101 - val_loss: 0.0120 - 3s/epoch - 3ms/step
Epoch 39/120
747/747 - 3s - loss: 0.0116 - val_loss: 0.0099 - 3s/epoch - 3ms/step
Epoch 40/120
747/747 - 3s - loss: 0.0101 - val_loss: 0.0100 - 3s/epoch - 3ms/step
Epoch 41/120
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 3ms/step
Epoch 42/120
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 3ms/step
Epoch 43/120
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 3ms/step
Epoch 44/120
747/747 - 3s - loss: 0.0099 - val_loss: 0.0099 - 3s/epoch - 3ms/step
Epoch 45/120
747/747 - 3s - loss: 0.0099 - val_loss: 0.0097 - 3s/epoch - 3ms/step
Epoch 46/120
747/747 - 3s - loss: 0.0103 - val_loss: 0.0116 - 3s/epoch - 3ms/step
Epoch 47/120
747/747 - 3s - loss: 0.0134 - val_loss: 0.0100 - 3s/epoch - 3ms/step
Epoch 48/120
747/747 - 3s - loss: 0.0101 - val_loss: 0.0110 - 3s/epoch - 3ms/step
Epoch 49/120
747/747 - 3s - loss: 0.0112 - val_loss: 0.0108 - 3s/epoch - 3ms/step
Epoch 50/120
747/747 - 3s - loss: 0.0119 - val_loss: 0.0099 - 3s/epoch - 3ms/step
Epoch 51/120
747/747 - 3s - loss: 0.0101 - val_loss: 0.0107 - 3s/epoch - 3ms/step
Epoch 52/120
747/747 - 3s - loss: 0.0119 - val_loss: 0.0101 - 3s/epoch - 3ms/step
Epoch 53/120
747/747 - 3s - loss: 0.0102 - val_loss: 0.0098 - 3s/epoch - 3ms/step
Epoch 54/120
747/747 - 3s - loss: 0.0100 - val_loss: 0.0098 - 3s/epoch - 3ms/step
Epoch 55/120
747/747 - 3s - loss: 0.0100 - val_loss: 0.0097 - 3s/epoch - 3ms/step
Epoch 56/120
747/747 - 3s - loss: 0.0099 - val_loss: 0.0098 - 3s/epoch - 3ms/step
Epoch 57/120
747/747 - 3s - loss: 0.0099 - val_loss: 0.0114 - 3s/epoch - 3ms/step
Epoch 58/120
747/747 - 3s - loss: 0.0104 - val_loss: 0.0097 - 3s/epoch - 3ms/step
Epoch 59/120
747/747 - 3s - loss: 0.0099 - val_loss: 0.0096 - 3s/epoch - 3ms/step
Epoch 60/120
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 3ms/step
Epoch 61/120
747/747 - 3s - loss: 0.0098 - val_loss: 0.0102 - 3s/epoch - 3ms/step
Epoch 62/120
747/747 - 3s - loss: 0.0099 - val_loss: 0.0108 - 3s/epoch - 3ms/step
Epoch 63/120
747/747 - 3s - loss: 0.0106 - val_loss: 0.0096 - 3s/epoch - 3ms/step
Epoch 64/120
747/747 - 3s - loss: 0.0098 - val_loss: 0.0097 - 3s/epoch - 3ms/step
Epoch 65/120
747/747 - 3s - loss: 0.0098 - val_loss: 0.0096 - 3s/epoch - 3ms/step
Epoch 66/120
747/747 - 3s - loss: 0.0098 - val_loss: 0.0115 - 3s/epoch - 3ms/step
Epoch 67/120
747/747 - 3s - loss: 0.0109 - val_loss: 0.0096 - 3s/epoch - 3ms/step
Epoch 68/120
747/747 - 3s - loss: 0.0098 - val_loss: 0.0095 - 3s/epoch - 3ms/step
Epoch 69/120
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 3ms/step
Epoch 70/120
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 3ms/step
Epoch 71/120
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 3ms/step
Epoch 72/120
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 3ms/step
Epoch 73/120
747/747 - 3s - loss: 0.0097 - val_loss: 0.0095 - 3s/epoch - 3ms/step
Epoch 74/120
747/747 - 3s - loss: 0.0096 - val_loss: 0.0095 - 3s/epoch - 3ms/step
Epoch 75/120
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 3ms/step
Epoch 76/120
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 3ms/step
Epoch 77/120
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 3ms/step
Epoch 78/120
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 3ms/step
Epoch 79/120
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 3ms/step
Epoch 80/120
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 3ms/step
Epoch 81/120
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 3ms/step
Epoch 82/120
747/747 - 3s - loss: 0.0096 - val_loss: 0.0094 - 3s/epoch - 3ms/step
Epoch 83/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0094 - 3s/epoch - 3ms/step
Epoch 84/120
747/747 - 3s - loss: 0.0096 - val_loss: 0.0093 - 3s/epoch - 3ms/step
Epoch 85/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0094 - 3s/epoch - 3ms/step
Epoch 86/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0094 - 3s/epoch - 3ms/step
Epoch 87/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 3ms/step
Epoch 88/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 3ms/step
Epoch 89/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 3ms/step
Epoch 90/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 3ms/step
Epoch 91/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 3ms/step
Epoch 92/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 3ms/step
Epoch 93/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 3ms/step
Epoch 94/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 3ms/step
Epoch 95/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 3ms/step
Epoch 96/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 3ms/step
Epoch 97/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 3ms/step
Epoch 98/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 3ms/step
Epoch 99/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 3ms/step
Epoch 100/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 3ms/step
Epoch 101/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 3ms/step
Epoch 102/120
747/747 - 3s - loss: 0.0095 - val_loss: 0.0093 - 3s/epoch - 3ms/step
Epoch 103/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 3ms/step
Epoch 104/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 3ms/step
Epoch 105/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 3ms/step
Epoch 106/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 3ms/step
Epoch 107/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 3ms/step
Epoch 108/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 3ms/step
Epoch 109/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 3ms/step
Epoch 110/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 3ms/step
Epoch 111/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 3ms/step
Epoch 112/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 3ms/step
Epoch 113/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 3ms/step
Epoch 114/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 3ms/step
Epoch 115/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 3ms/step
Epoch 116/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 3ms/step
Epoch 117/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 3ms/step
Epoch 118/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 3ms/step
Epoch 119/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0092 - 3s/epoch - 3ms/step
Epoch 120/120
747/747 - 3s - loss: 0.0094 - val_loss: 0.0093 - 3s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.00926294270902872
  1/332 [..............................] - ETA: 34s 45/332 [===>..........................] - ETA: 0s  90/332 [=======>......................] - ETA: 0s135/332 [===========>..................] - ETA: 0s180/332 [===============>..............] - ETA: 0s225/332 [===================>..........] - ETA: 0s270/332 [=======================>......] - ETA: 0s315/332 [===========================>..] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.06817513179698172
cosine 0.052256048388450584
MAE: 0.035287056
RMSE: 0.07720458
r2: 0.613671494260722
RMSE zero-vector: 0.2430644284356365
['2.0custom_VAE', 'mse', 128, 120, 0.0005, 0.2, 252, 0.009401467628777027, 0.00926294270902872, 0.06817513179698172, 0.052256048388450584, 0.03528705611824989, 0.07720457762479782, 0.613671494260722, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 150 0.002 64 2] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          478044      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          478044      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2949844     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 6,311,956
Trainable params: 6,303,868
Non-trainable params: 8,088
__________________________________________________________________________________________________
Epoch 1/150
1493/1493 - 5s - loss: 0.0130 - val_loss: 0.0088 - 5s/epoch - 4ms/step
Epoch 2/150
1493/1493 - 4s - loss: 0.0077 - val_loss: 0.0074 - 4s/epoch - 3ms/step
Epoch 3/150
1493/1493 - 4s - loss: 0.0071 - val_loss: 0.0069 - 4s/epoch - 3ms/step
Epoch 4/150
1493/1493 - 4s - loss: 0.0067 - val_loss: 0.0064 - 4s/epoch - 3ms/step
Epoch 5/150
1493/1493 - 4s - loss: 0.0065 - val_loss: 0.0063 - 4s/epoch - 3ms/step
Epoch 6/150
1493/1493 - 4s - loss: 0.0064 - val_loss: 0.0063 - 4s/epoch - 3ms/step
Epoch 7/150
1493/1493 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 3ms/step
Epoch 8/150
1493/1493 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 3ms/step
Epoch 9/150
1493/1493 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 3ms/step
Epoch 10/150
1493/1493 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 3ms/step
Epoch 11/150
1493/1493 - 4s - loss: 0.0063 - val_loss: 0.0062 - 4s/epoch - 3ms/step
Epoch 12/150
1493/1493 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 3ms/step
Epoch 13/150
1493/1493 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 3ms/step
Epoch 14/150
1493/1493 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 3ms/step
Epoch 15/150
1493/1493 - 4s - loss: 0.0062 - val_loss: 0.0062 - 4s/epoch - 3ms/step
Epoch 16/150
1493/1493 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 3ms/step
Epoch 17/150
1493/1493 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 3ms/step
Epoch 18/150
1493/1493 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 3ms/step
Epoch 19/150
1493/1493 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 3ms/step
Epoch 20/150
1493/1493 - 4s - loss: 0.0062 - val_loss: 0.0061 - 4s/epoch - 3ms/step
Epoch 21/150
1493/1493 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 3ms/step
Epoch 22/150
1493/1493 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 3ms/step
Epoch 23/150
1493/1493 - 4s - loss: 0.0061 - val_loss: 0.0061 - 4s/epoch - 3ms/step
Epoch 24/150
1493/1493 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 3ms/step
Epoch 25/150
1493/1493 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 3ms/step
Epoch 26/150
1493/1493 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 3ms/step
Epoch 27/150
1493/1493 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 3ms/step
Epoch 28/150
1493/1493 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 3ms/step
Epoch 29/150
1493/1493 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 3ms/step
Epoch 30/150
1493/1493 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 3ms/step
Epoch 31/150
1493/1493 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 3ms/step
Epoch 32/150
1493/1493 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 3ms/step
Epoch 33/150
1493/1493 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 3ms/step
Epoch 34/150
1493/1493 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 3ms/step
Epoch 35/150
1493/1493 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 3ms/step
Epoch 36/150
1493/1493 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 3ms/step
Epoch 37/150
1493/1493 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 3ms/step
Epoch 38/150
1493/1493 - 4s - loss: 0.0061 - val_loss: 0.0060 - 4s/epoch - 3ms/step
Epoch 39/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 3ms/step
Epoch 40/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 3ms/step
Epoch 41/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 3ms/step
Epoch 42/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 3ms/step
Epoch 43/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 3ms/step
Epoch 44/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 3ms/step
Epoch 45/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 3ms/step
Epoch 46/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 3ms/step
Epoch 47/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 48/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 49/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 3ms/step
Epoch 50/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 3ms/step
Epoch 51/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 3ms/step
Epoch 52/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 53/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 54/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 55/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 56/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 57/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 58/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 59/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 60/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 61/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 62/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 3ms/step
Epoch 63/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 64/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 65/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 66/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 67/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 68/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 69/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 70/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 71/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 72/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 3ms/step
Epoch 73/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0060 - 4s/epoch - 3ms/step
Epoch 74/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 75/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 76/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 77/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 78/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 79/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 80/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 81/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 82/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 83/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 84/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 85/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 86/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 87/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 88/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 89/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 90/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 91/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 92/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 93/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 94/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 95/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 96/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 97/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 98/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 99/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 100/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 101/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 102/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 103/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 104/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 105/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 106/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 107/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 108/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 109/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 110/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 111/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 112/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 113/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 114/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 115/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 116/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 117/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 118/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 119/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 120/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 121/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 122/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 123/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 124/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 125/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 126/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 127/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 128/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 129/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 130/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 131/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 132/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 133/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 134/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 135/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 136/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 137/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 138/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 139/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 140/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 141/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 142/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 143/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 144/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 145/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 146/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 147/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 148/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 149/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
Epoch 150/150
1493/1493 - 4s - loss: 0.0060 - val_loss: 0.0059 - 4s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.005900227930396795
  1/332 [..............................] - ETA: 39s 45/332 [===>..........................] - ETA: 0s  90/332 [=======>......................] - ETA: 0s135/332 [===========>..................] - ETA: 0s180/332 [===============>..............] - ETA: 0s225/332 [===================>..........] - ETA: 0s270/332 [=======================>......] - ETA: 0s315/332 [===========================>..] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.10896616001377185
cosine 0.08325021514859327
MAE: 0.0450431
RMSE: 0.096736886
r2: 0.39346517601141634
RMSE zero-vector: 0.2430644284356365
['1.5custom_VAE', 'logcosh', 64, 150, 0.002, 0.2, 252, 0.00596537534147501, 0.005900227930396795, 0.10896616001377185, 0.08325021514859327, 0.04504309967160225, 0.09673688560724258, 0.39346517601141634, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.0 60 0.0005 128 2] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1264)         1598960     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1264)        5056        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1264)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          318780      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1988572     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 4,230,148
Trainable params: 4,224,588
Non-trainable params: 5,560
__________________________________________________________________________________________________
File geneticVAE_MMmp_gap_custom_VAE1.0_cr0.2_bs128_ep60_loss_logcosh_lr0.0005_AutoEncoder.h5 exists in folder already, skiping this calculation.
  1/332 [..............................] - ETA: 35s 45/332 [===>..........................] - ETA: 0s  90/332 [=======>......................] - ETA: 0s134/332 [===========>..................] - ETA: 0s179/332 [===============>..............] - ETA: 0s224/332 [===================>..........] - ETA: 0s269/332 [=======================>......] - ETA: 0s314/332 [===========================>..] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.11220807114547934
cosine 0.08569491315103682
MAE: 0.045726992
RMSE: 0.098084874
r2: 0.3764437403052875
RMSE zero-vector: 0.2430644284356365
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
['1.0custom_VAE', 'logcosh', 128, 60, 0.0005, 0.2, 252, '--', '--', 0.11220807114547934, 0.08569491315103682, 0.045726992189884186, 0.09808487445116043, 0.3764437403052875, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 60 0.0005 256 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          478044      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          478044      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2949844     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 6,311,956
Trainable params: 6,303,868
Non-trainable params: 8,088
__________________________________________________________________________________________________
Epoch 1/60
374/374 - 2s - loss: 0.0358 - val_loss: 0.0176 - 2s/epoch - 6ms/step
Epoch 2/60
374/374 - 1s - loss: 0.0157 - val_loss: 0.0191 - 1s/epoch - 4ms/step
Epoch 3/60
374/374 - 1s - loss: 0.0149 - val_loss: 0.0264 - 1s/epoch - 4ms/step
Epoch 4/60
374/374 - 1s - loss: 0.0145 - val_loss: 0.0237 - 1s/epoch - 4ms/step
Epoch 5/60
374/374 - 1s - loss: 0.0141 - val_loss: 0.0152 - 1s/epoch - 4ms/step
Epoch 6/60
374/374 - 1s - loss: 0.0139 - val_loss: 0.0178 - 1s/epoch - 4ms/step
Epoch 7/60
374/374 - 1s - loss: 0.0135 - val_loss: 0.0167 - 1s/epoch - 4ms/step
Epoch 8/60
374/374 - 1s - loss: 0.0133 - val_loss: 0.0169 - 1s/epoch - 4ms/step
Epoch 9/60
374/374 - 1s - loss: 0.0131 - val_loss: 0.0136 - 1s/epoch - 4ms/step
Epoch 10/60
374/374 - 1s - loss: 0.0124 - val_loss: 0.0443 - 1s/epoch - 4ms/step
Epoch 11/60
374/374 - 1s - loss: 0.0131 - val_loss: 0.0123 - 1s/epoch - 4ms/step
Epoch 12/60
374/374 - 1s - loss: 0.0117 - val_loss: 0.0132 - 1s/epoch - 4ms/step
Epoch 13/60
374/374 - 1s - loss: 0.0116 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 14/60
374/374 - 1s - loss: 0.0112 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 15/60
374/374 - 1s - loss: 0.0110 - val_loss: 0.0121 - 1s/epoch - 4ms/step
Epoch 16/60
374/374 - 1s - loss: 0.0111 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 17/60
374/374 - 1s - loss: 0.0108 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 18/60
374/374 - 1s - loss: 0.0107 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 19/60
374/374 - 1s - loss: 0.0105 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 20/60
374/374 - 1s - loss: 0.0105 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 21/60
374/374 - 1s - loss: 0.0106 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 22/60
374/374 - 1s - loss: 0.0104 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 23/60
374/374 - 1s - loss: 0.0103 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 24/60
374/374 - 1s - loss: 0.0108 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 25/60
374/374 - 1s - loss: 0.0106 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 26/60
374/374 - 1s - loss: 0.0111 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 27/60
374/374 - 1s - loss: 0.0110 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 28/60
374/374 - 1s - loss: 0.0111 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 29/60
374/374 - 1s - loss: 0.0126 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 30/60
374/374 - 1s - loss: 0.0117 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 31/60
374/374 - 1s - loss: 0.0105 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 32/60
374/374 - 1s - loss: 0.0103 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 33/60
374/374 - 1s - loss: 0.0110 - val_loss: 0.0129 - 1s/epoch - 4ms/step
Epoch 34/60
374/374 - 1s - loss: 0.0161 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 35/60
374/374 - 1s - loss: 0.0109 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 36/60
374/374 - 1s - loss: 0.0124 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 37/60
374/374 - 1s - loss: 0.0111 - val_loss: 0.0118 - 1s/epoch - 4ms/step
Epoch 38/60
374/374 - 1s - loss: 0.0182 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 39/60
374/374 - 1s - loss: 0.0113 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 40/60
374/374 - 1s - loss: 0.0110 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 41/60
374/374 - 1s - loss: 0.0107 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 42/60
374/374 - 1s - loss: 0.0106 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 43/60
374/374 - 1s - loss: 0.0108 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 44/60
374/374 - 1s - loss: 0.0110 - val_loss: 0.0144 - 1s/epoch - 4ms/step
Epoch 45/60
374/374 - 1s - loss: 0.0236 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 46/60
374/374 - 1s - loss: 0.0108 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 47/60
374/374 - 1s - loss: 0.0106 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 48/60
374/374 - 1s - loss: 0.0105 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 49/60
374/374 - 1s - loss: 0.0104 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 50/60
374/374 - 1s - loss: 0.0106 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 51/60
374/374 - 1s - loss: 0.0107 - val_loss: 0.0128 - 1s/epoch - 4ms/step
Epoch 52/60
374/374 - 1s - loss: 0.0194 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 53/60
374/374 - 1s - loss: 0.0113 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 54/60
374/374 - 1s - loss: 0.0108 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 55/60
374/374 - 1s - loss: 0.0112 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 56/60
374/374 - 1s - loss: 0.0113 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 57/60
374/374 - 1s - loss: 0.0112 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 58/60
374/374 - 1s - loss: 0.0106 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 59/60
374/374 - 1s - loss: 0.0104 - val_loss: 0.0171 - 1s/epoch - 4ms/step
Epoch 60/60
374/374 - 1s - loss: 0.0145 - val_loss: 0.0105 - 1s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.010499847121536732
  1/332 [..............................] - ETA: 34s 45/332 [===>..........................] - ETA: 0s  89/332 [=======>......................] - ETA: 0s133/332 [===========>..................] - ETA: 0s178/332 [===============>..............] - ETA: 0s223/332 [===================>..........] - ETA: 0s267/332 [=======================>......] - ETA: 0s312/332 [===========================>..] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.0795095089669952
cosine 0.060859678928869156
MAE: 0.038637497
RMSE: 0.08310492
r2: 0.5523643727512956
RMSE zero-vector: 0.2430644284356365
['1.5custom_VAE', 'mse', 256, 60, 0.0005, 0.2, 252, 0.014530704356729984, 0.010499847121536732, 0.0795095089669952, 0.060859678928869156, 0.038637496531009674, 0.0831049233675003, 0.5523643727512956, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 150 0.0012000000000000001 128 2] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
Epoch 1/150
747/747 - 3s - loss: 0.0128 - val_loss: 0.0078 - 3s/epoch - 4ms/step
Epoch 2/150
747/747 - 2s - loss: 0.0074 - val_loss: 0.0073 - 2s/epoch - 3ms/step
Epoch 3/150
747/747 - 2s - loss: 0.0072 - val_loss: 0.0072 - 2s/epoch - 3ms/step
Epoch 4/150
747/747 - 2s - loss: 0.0071 - val_loss: 0.0074 - 2s/epoch - 3ms/step
Epoch 5/150
747/747 - 2s - loss: 0.0070 - val_loss: 0.0070 - 2s/epoch - 3ms/step
Epoch 6/150
747/747 - 2s - loss: 0.0069 - val_loss: 0.0069 - 2s/epoch - 3ms/step
Epoch 7/150
747/747 - 2s - loss: 0.0068 - val_loss: 0.0068 - 2s/epoch - 3ms/step
Epoch 8/150
747/747 - 2s - loss: 0.0068 - val_loss: 0.0067 - 2s/epoch - 3ms/step
Epoch 9/150
747/747 - 2s - loss: 0.0066 - val_loss: 0.0064 - 2s/epoch - 3ms/step
Epoch 10/150
747/747 - 2s - loss: 0.0064 - val_loss: 0.0063 - 2s/epoch - 3ms/step
Epoch 11/150
747/747 - 2s - loss: 0.0063 - val_loss: 0.0063 - 2s/epoch - 3ms/step
Epoch 12/150
747/747 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 3ms/step
Epoch 13/150
747/747 - 2s - loss: 0.0063 - val_loss: 0.0062 - 2s/epoch - 3ms/step
Epoch 14/150
747/747 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 3ms/step
Epoch 15/150
747/747 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 3ms/step
Epoch 16/150
747/747 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 3ms/step
Epoch 17/150
747/747 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 3ms/step
Epoch 18/150
747/747 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 3ms/step
Epoch 19/150
747/747 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 20/150
747/747 - 2s - loss: 0.0062 - val_loss: 0.0062 - 2s/epoch - 3ms/step
Epoch 21/150
747/747 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 22/150
747/747 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 23/150
747/747 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 24/150
747/747 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 25/150
747/747 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 26/150
747/747 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 27/150
747/747 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 28/150
747/747 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 29/150
747/747 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 30/150
747/747 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 31/150
747/747 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 32/150
747/747 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 33/150
747/747 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 34/150
747/747 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 35/150
747/747 - 2s - loss: 0.0061 - val_loss: 0.0061 - 2s/epoch - 3ms/step
Epoch 36/150
747/747 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 37/150
747/747 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 38/150
747/747 - 2s - loss: 0.0061 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 39/150
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 40/150
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 41/150
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 42/150
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 43/150
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 44/150
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 45/150
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 46/150
747/747 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 3ms/step
Epoch 47/150
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 48/150
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 49/150
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 50/150
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 51/150
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 52/150
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 53/150
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 54/150
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 55/150
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 56/150
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 57/150
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 58/150
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 59/150
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 60/150
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 61/150
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 62/150
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 63/150
747/747 - 2s - loss: 0.0060 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 64/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 65/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 66/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 67/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 68/150
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 3ms/step
Epoch 69/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 70/150
747/747 - 3s - loss: 0.0059 - val_loss: 0.0059 - 3s/epoch - 3ms/step
Epoch 71/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 72/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 73/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 74/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 75/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 76/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 77/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 78/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 79/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 80/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 81/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 82/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 83/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 84/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 85/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 86/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 87/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 88/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 89/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 90/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 91/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 92/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 93/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 94/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 95/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 96/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 97/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 98/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 99/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 100/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 101/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 102/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 103/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 104/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 105/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 106/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 107/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 108/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 109/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 3ms/step
Epoch 110/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 3ms/step
Epoch 111/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 112/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 113/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 3ms/step
Epoch 114/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 115/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 3ms/step
Epoch 116/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 3ms/step
Epoch 117/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 118/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 3ms/step
Epoch 119/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 120/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 121/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 122/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 3ms/step
Epoch 123/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 3ms/step
Epoch 124/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 125/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 3ms/step
Epoch 126/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 127/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 3ms/step
Epoch 128/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 3ms/step
Epoch 129/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 3ms/step
Epoch 130/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 131/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 132/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 3ms/step
Epoch 133/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 3ms/step
Epoch 134/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 135/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 3ms/step
Epoch 136/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 137/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 138/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 3ms/step
Epoch 139/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 3ms/step
Epoch 140/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 3ms/step
Epoch 141/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0059 - 2s/epoch - 3ms/step
Epoch 142/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 3ms/step
Epoch 143/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 3ms/step
Epoch 144/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 3ms/step
Epoch 145/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 3ms/step
Epoch 146/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 3ms/step
Epoch 147/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 3ms/step
Epoch 148/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 3ms/step
Epoch 149/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 3ms/step
Epoch 150/150
747/747 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.005828384775668383
  1/332 [..............................] - ETA: 36s 45/332 [===>..........................] - ETA: 0s  89/332 [=======>......................] - ETA: 0s133/332 [===========>..................] - ETA: 0s177/332 [==============>...............] - ETA: 0s222/332 [===================>..........] - ETA: 0s266/332 [=======================>......] - ETA: 0s311/332 [===========================>..] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.10551068549038463
cosine 0.08055341714986801
MAE: 0.043849524
RMSE: 0.095222935
r2: 0.41230122199189734
RMSE zero-vector: 0.2430644284356365
['0.5custom_VAE', 'logcosh', 128, 150, 0.0012000000000000001, 0.2, 252, 0.005893349647521973, 0.005828384775668383, 0.10551068549038463, 0.08055341714986801, 0.04384952411055565, 0.09522293508052826, 0.41230122199189734, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.6 60 0.0005 64 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 2022)         2557830     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 2022)        8088        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 2022)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          509796      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          509796      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         3141490     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 6,727,000
Trainable params: 6,718,408
Non-trainable params: 8,592
__________________________________________________________________________________________________
Epoch 1/60
1493/1493 - 6s - loss: 0.0246 - val_loss: 0.0252 - 6s/epoch - 4ms/step
Epoch 2/60
1493/1493 - 5s - loss: 0.0150 - val_loss: 0.0142 - 5s/epoch - 3ms/step
Epoch 3/60
1493/1493 - 5s - loss: 0.0133 - val_loss: 0.0134 - 5s/epoch - 3ms/step
Epoch 4/60
1493/1493 - 5s - loss: 0.0123 - val_loss: 0.0125 - 5s/epoch - 3ms/step
Epoch 5/60
1493/1493 - 5s - loss: 0.0118 - val_loss: 0.0114 - 5s/epoch - 3ms/step
Epoch 6/60
1493/1493 - 5s - loss: 0.0116 - val_loss: 0.0112 - 5s/epoch - 3ms/step
Epoch 7/60
1493/1493 - 5s - loss: 0.0114 - val_loss: 0.0112 - 5s/epoch - 3ms/step
Epoch 8/60
1493/1493 - 5s - loss: 0.0113 - val_loss: 0.0108 - 5s/epoch - 3ms/step
Epoch 9/60
1493/1493 - 5s - loss: 0.0110 - val_loss: 0.0106 - 5s/epoch - 3ms/step
Epoch 10/60
1493/1493 - 5s - loss: 0.0108 - val_loss: 0.0105 - 5s/epoch - 3ms/step
Epoch 11/60
1493/1493 - 5s - loss: 0.0107 - val_loss: 0.0103 - 5s/epoch - 3ms/step
Epoch 12/60
1493/1493 - 5s - loss: 0.0105 - val_loss: 0.0102 - 5s/epoch - 3ms/step
Epoch 13/60
1493/1493 - 5s - loss: 0.0105 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 14/60
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 15/60
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 16/60
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 17/60
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 18/60
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 19/60
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 20/60
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 21/60
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 22/60
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 23/60
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 24/60
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 25/60
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 26/60
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 27/60
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 28/60
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 29/60
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 30/60
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 31/60
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 32/60
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 33/60
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 34/60
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 35/60
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 36/60
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 37/60
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 38/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 39/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 40/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 41/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 42/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 43/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 44/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 45/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 46/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 47/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 48/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 49/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 50/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 51/60
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 52/60
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 53/60
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 54/60
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 55/60
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 56/60
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 57/60
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 58/60
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 59/60
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 60/60
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009434929117560387
  1/332 [..............................] - ETA: 38s 44/332 [==>...........................] - ETA: 0s  87/332 [======>.......................] - ETA: 0s131/332 [==========>...................] - ETA: 0s175/332 [==============>...............] - ETA: 0s219/332 [==================>...........] - ETA: 0s263/332 [======================>.......] - ETA: 0s308/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.07107395912695057
cosine 0.05446648725216855
MAE: 0.036248244
RMSE: 0.07880906
r2: 0.5974464385659528
RMSE zero-vector: 0.2430644284356365
['1.6custom_VAE', 'mse', 64, 60, 0.0005, 0.2, 252, 0.009693768806755543, 0.009434929117560387, 0.07107395912695057, 0.05446648725216855, 0.03624824434518814, 0.07880906015634537, 0.5974464385659528, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 60 0.0005 256 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          478044      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          478044      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2949844     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 6,311,956
Trainable params: 6,303,868
Non-trainable params: 8,088
__________________________________________________________________________________________________
File geneticVAE_MMmp_gap_custom_VAE1.5_cr0.2_bs256_ep60_loss_mse_lr0.0005_AutoEncoder.h5 exists in folder already, skiping this calculation.
  1/332 [..............................] - ETA: 35s 44/332 [==>...........................] - ETA: 0s  88/332 [======>.......................] - ETA: 0s132/332 [==========>...................] - ETA: 0s176/332 [==============>...............] - ETA: 0s221/332 [==================>...........] - ETA: 0s265/332 [======================>.......] - ETA: 0s309/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.0795095089669952
cosine 0.060859678928869156
MAE: 0.038637497
RMSE: 0.08310492
r2: 0.5523643727512956
RMSE zero-vector: 0.2430644284356365
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
['1.5custom_VAE', 'mse', 256, 60, 0.0005, 0.2, 252, '--', '--', 0.0795095089669952, 0.060859678928869156, 0.038637496531009674, 0.0831049233675003, 0.5523643727512956, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 175 0.001 256 2] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          478044      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          478044      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2949844     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 6,311,956
Trainable params: 6,303,868
Non-trainable params: 8,088
__________________________________________________________________________________________________
Epoch 1/175
374/374 - 2s - loss: 0.0186 - val_loss: 0.0109 - 2s/epoch - 6ms/step
Epoch 2/175
374/374 - 1s - loss: 0.0086 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 3/175
374/374 - 1s - loss: 0.0079 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 4/175
374/374 - 1s - loss: 0.0077 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 5/175
374/374 - 1s - loss: 0.0076 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 6/175
374/374 - 1s - loss: 0.0075 - val_loss: 0.0183 - 1s/epoch - 4ms/step
Epoch 7/175
374/374 - 1s - loss: 0.0075 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 8/175
374/374 - 1s - loss: 0.0073 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 9/175
374/374 - 1s - loss: 0.0071 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 10/175
374/374 - 1s - loss: 0.0071 - val_loss: 0.0086 - 1s/epoch - 4ms/step
Epoch 11/175
374/374 - 1s - loss: 0.0070 - val_loss: 0.0072 - 1s/epoch - 4ms/step
Epoch 12/175
374/374 - 1s - loss: 0.0068 - val_loss: 0.0068 - 1s/epoch - 4ms/step
Epoch 13/175
374/374 - 1s - loss: 0.0066 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 14/175
374/374 - 1s - loss: 0.0066 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 15/175
374/374 - 1s - loss: 0.0064 - val_loss: 0.0159 - 1s/epoch - 4ms/step
Epoch 16/175
374/374 - 1s - loss: 0.0068 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 17/175
374/374 - 1s - loss: 0.0064 - val_loss: 0.0074 - 1s/epoch - 4ms/step
Epoch 18/175
374/374 - 1s - loss: 0.0068 - val_loss: 0.0067 - 1s/epoch - 4ms/step
Epoch 19/175
374/374 - 1s - loss: 0.0065 - val_loss: 0.0069 - 1s/epoch - 4ms/step
Epoch 20/175
374/374 - 1s - loss: 0.0067 - val_loss: 0.0066 - 1s/epoch - 4ms/step
Epoch 21/175
374/374 - 1s - loss: 0.0065 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 22/175
374/374 - 1s - loss: 0.0063 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 23/175
374/374 - 1s - loss: 0.0063 - val_loss: 0.0070 - 1s/epoch - 4ms/step
Epoch 24/175
374/374 - 1s - loss: 0.0072 - val_loss: 0.0069 - 1s/epoch - 4ms/step
Epoch 25/175
374/374 - 1s - loss: 0.0080 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 26/175
374/374 - 1s - loss: 0.0066 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 27/175
374/374 - 1s - loss: 0.0067 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 28/175
374/374 - 1s - loss: 0.0066 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 29/175
374/374 - 1s - loss: 0.0063 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 30/175
374/374 - 1s - loss: 0.0064 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 31/175
374/374 - 1s - loss: 0.0063 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 32/175
374/374 - 1s - loss: 0.0064 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 33/175
374/374 - 1s - loss: 0.0067 - val_loss: 0.0063 - 1s/epoch - 4ms/step
Epoch 34/175
374/374 - 1s - loss: 0.0063 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 35/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 36/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 37/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 38/175
374/374 - 1s - loss: 0.0064 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 39/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 40/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 41/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 42/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 3ms/step
Epoch 43/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0065 - 1s/epoch - 4ms/step
Epoch 44/175
374/374 - 1s - loss: 0.0065 - val_loss: 0.0064 - 1s/epoch - 4ms/step
Epoch 45/175
374/374 - 1s - loss: 0.0064 - val_loss: 0.0070 - 1s/epoch - 4ms/step
Epoch 46/175
374/374 - 1s - loss: 0.0068 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 47/175
374/374 - 2s - loss: 0.0062 - val_loss: 0.0061 - 2s/epoch - 4ms/step
Epoch 48/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 49/175
374/374 - 1s - loss: 0.0063 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 50/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 51/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 52/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 53/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 54/175
374/374 - 1s - loss: 0.0062 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 55/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 56/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 57/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 58/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 59/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 60/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 61/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 62/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0061 - 1s/epoch - 4ms/step
Epoch 63/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 64/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 65/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 66/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 67/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 68/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 69/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0062 - 1s/epoch - 4ms/step
Epoch 70/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 71/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 72/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 73/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 74/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 75/175
374/374 - 2s - loss: 0.0060 - val_loss: 0.0060 - 2s/epoch - 4ms/step
Epoch 76/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 77/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 78/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 79/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 80/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 81/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 82/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 83/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 84/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 85/175
374/374 - 1s - loss: 0.0061 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 86/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 87/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 88/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 89/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 90/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 91/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 92/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0060 - 1s/epoch - 4ms/step
Epoch 93/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 94/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 95/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 96/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 97/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 98/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 99/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 100/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 101/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 102/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 103/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 104/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 105/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 106/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 107/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 108/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 109/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 110/175
374/374 - 1s - loss: 0.0060 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 111/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 112/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 113/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 114/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 115/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 116/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 117/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 118/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 119/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 120/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 121/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 122/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 123/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 124/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 125/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 126/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 127/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 128/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 129/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 130/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 131/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 132/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 133/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 134/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 135/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 136/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 137/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 4ms/step
Epoch 138/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 4ms/step
Epoch 139/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 140/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 141/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 142/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 4ms/step
Epoch 143/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 144/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 145/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 146/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 147/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 148/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 4ms/step
Epoch 149/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 150/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 151/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 152/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 4ms/step
Epoch 153/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 4ms/step
Epoch 154/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 4ms/step
Epoch 155/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 4ms/step
Epoch 156/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 157/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 158/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 4ms/step
Epoch 159/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 160/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 4ms/step
Epoch 161/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 4ms/step
Epoch 162/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 4ms/step
Epoch 163/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 4ms/step
Epoch 164/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 4ms/step
Epoch 165/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 166/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 4ms/step
Epoch 167/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 4ms/step
Epoch 168/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 4ms/step
Epoch 169/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0059 - 1s/epoch - 4ms/step
Epoch 170/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 4ms/step
Epoch 171/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 4ms/step
Epoch 172/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 4ms/step
Epoch 173/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 4ms/step
Epoch 174/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 4ms/step
Epoch 175/175
374/374 - 1s - loss: 0.0059 - val_loss: 0.0058 - 1s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.005833861883729696
  1/332 [..............................] - ETA: 38s 41/332 [==>...........................] - ETA: 0s  84/332 [======>.......................] - ETA: 0s128/332 [==========>...................] - ETA: 0s172/332 [==============>...............] - ETA: 0s216/332 [==================>...........] - ETA: 0s260/332 [======================>.......] - ETA: 0s304/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.10597695829820687
cosine 0.08092781221736299
MAE: 0.044019546
RMSE: 0.09545061
r2: 0.40948751368857683
RMSE zero-vector: 0.2430644284356365
['1.5custom_VAE', 'logcosh', 256, 175, 0.001, 0.2, 252, 0.005884245969355106, 0.005833861883729696, 0.10597695829820687, 0.08092781221736299, 0.04401954635977745, 0.09545060992240906, 0.40948751368857683, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.5 180 0.0008 64 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          478044      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          478044      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2949844     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 6,311,956
Trainable params: 6,303,868
Non-trainable params: 8,088
__________________________________________________________________________________________________
Epoch 1/180
1493/1493 - 5s - loss: 0.0238 - val_loss: 0.0169 - 5s/epoch - 4ms/step
Epoch 2/180
1493/1493 - 5s - loss: 0.0149 - val_loss: 0.0146 - 5s/epoch - 3ms/step
Epoch 3/180
1493/1493 - 5s - loss: 0.0134 - val_loss: 0.0127 - 5s/epoch - 3ms/step
Epoch 4/180
1493/1493 - 5s - loss: 0.0125 - val_loss: 0.0119 - 5s/epoch - 3ms/step
Epoch 5/180
1493/1493 - 5s - loss: 0.0118 - val_loss: 0.0113 - 5s/epoch - 3ms/step
Epoch 6/180
1493/1493 - 5s - loss: 0.0113 - val_loss: 0.0108 - 5s/epoch - 3ms/step
Epoch 7/180
1493/1493 - 5s - loss: 0.0110 - val_loss: 0.0107 - 5s/epoch - 3ms/step
Epoch 8/180
1493/1493 - 5s - loss: 0.0109 - val_loss: 0.0106 - 5s/epoch - 3ms/step
Epoch 9/180
1493/1493 - 5s - loss: 0.0107 - val_loss: 0.0104 - 5s/epoch - 3ms/step
Epoch 10/180
1493/1493 - 5s - loss: 0.0106 - val_loss: 0.0103 - 5s/epoch - 3ms/step
Epoch 11/180
1493/1493 - 5s - loss: 0.0105 - val_loss: 0.0102 - 5s/epoch - 3ms/step
Epoch 12/180
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 13/180
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 14/180
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 15/180
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 16/180
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 17/180
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 18/180
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 19/180
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 20/180
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 21/180
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 22/180
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 23/180
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 24/180
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 25/180
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 26/180
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 27/180
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 28/180
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 29/180
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 30/180
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 31/180
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 32/180
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 33/180
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 34/180
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 35/180
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 36/180
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 37/180
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 38/180
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 39/180
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 40/180
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 41/180
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 42/180
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 43/180
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 44/180
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 45/180
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 46/180
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 47/180
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 48/180
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 49/180
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 50/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 51/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 52/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 53/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 54/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 55/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 56/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 57/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 58/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 59/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 60/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 61/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 62/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 63/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 64/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 65/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 66/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 67/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 68/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 69/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 70/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 71/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 72/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 73/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 74/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 75/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 76/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 77/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 78/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 79/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 80/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 81/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 82/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 83/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 84/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 85/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 86/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 87/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 88/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 89/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 90/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 91/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 92/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 93/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 94/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 95/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 96/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 97/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 98/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 99/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 100/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 101/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 102/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 103/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 104/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 105/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 106/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 107/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 108/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 109/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 110/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 111/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 112/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 113/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 114/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 115/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 116/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 117/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 118/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 119/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 120/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 121/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 122/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 123/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 124/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 125/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 126/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 127/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 128/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 129/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 130/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 131/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 132/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 133/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 134/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 135/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 136/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 137/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 138/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 139/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 140/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 141/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 142/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 143/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 144/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 145/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 146/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 147/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 148/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 149/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 150/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 151/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 152/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 153/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 154/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 155/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 156/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 157/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 158/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 159/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 160/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 161/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 162/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 163/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 164/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 165/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 166/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 167/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 168/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 169/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 170/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 171/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 172/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 173/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 174/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 175/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 176/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 177/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 178/180
1493/1493 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 179/180
1493/1493 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 180/180
1493/1493 - 5s - loss: 0.0094 - val_loss: 0.0092 - 5s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009240531362593174
  1/332 [..............................] - ETA: 39s 40/332 [==>...........................] - ETA: 0s  81/332 [======>.......................] - ETA: 0s124/332 [==========>...................] - ETA: 0s168/332 [==============>...............] - ETA: 0s208/332 [=================>............] - ETA: 0s250/332 [=====================>........] - ETA: 0s293/332 [=========================>....] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.06675910794123856
cosine 0.05117433913307643
MAE: 0.03513973
RMSE: 0.07650516
r2: 0.6206387886414513
RMSE zero-vector: 0.2430644284356365
['1.5custom_VAE', 'mse', 64, 180, 0.0008, 0.2, 252, 0.00944715365767479, 0.009240531362593174, 0.06675910794123856, 0.05117433913307643, 0.03513972833752632, 0.0765051618218422, 0.6206387886414513, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
Saved GA instance to file: ./tmp//ga_instance_generation_1.pkl
[1.5 180 0.0008 64 2] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1896)         2398440     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1896)        7584        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1896)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          478044      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          478044      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2949844     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 6,311,956
Trainable params: 6,303,868
Non-trainable params: 8,088
__________________________________________________________________________________________________
Epoch 1/180
1493/1493 - 5s - loss: 0.0126 - val_loss: 0.0091 - 5s/epoch - 4ms/step
Epoch 2/180
1493/1493 - 5s - loss: 0.0080 - val_loss: 0.0077 - 5s/epoch - 3ms/step
Epoch 3/180
1493/1493 - 5s - loss: 0.0072 - val_loss: 0.0070 - 5s/epoch - 3ms/step
Epoch 4/180
1493/1493 - 5s - loss: 0.0069 - val_loss: 0.0066 - 5s/epoch - 3ms/step
Epoch 5/180
1493/1493 - 5s - loss: 0.0065 - val_loss: 0.0063 - 5s/epoch - 3ms/step
Epoch 6/180
1493/1493 - 5s - loss: 0.0064 - val_loss: 0.0063 - 5s/epoch - 3ms/step
Epoch 7/180
1493/1493 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 8/180
1493/1493 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 9/180
1493/1493 - 5s - loss: 0.0063 - val_loss: 0.0062 - 5s/epoch - 3ms/step
Epoch 10/180
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 11/180
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 12/180
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 13/180
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 14/180
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 15/180
1493/1493 - 5s - loss: 0.0062 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 16/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 17/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 18/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0061 - 5s/epoch - 3ms/step
Epoch 19/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 20/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 21/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 22/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 23/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 24/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 25/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 26/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 27/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 28/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 29/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 30/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 31/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 32/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 33/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 34/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 35/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 36/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 37/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 38/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 39/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 40/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 41/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 42/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 43/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 44/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 45/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 46/180
1493/1493 - 5s - loss: 0.0061 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 47/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 48/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 49/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 50/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 51/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 52/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 53/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 54/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 55/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 56/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 57/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 58/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 59/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 60/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 61/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 62/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 63/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 64/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 65/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 66/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 67/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 68/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 69/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 70/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 71/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 72/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 73/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 74/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 75/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 76/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 77/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 78/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 79/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 80/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 81/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 82/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 83/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 84/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 85/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 86/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 87/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0060 - 5s/epoch - 3ms/step
Epoch 88/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 89/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 90/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 91/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 92/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 93/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 94/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 95/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 96/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 97/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 98/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 99/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 100/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 101/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 102/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 103/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 104/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 105/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 106/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 107/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 108/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 109/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 110/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 111/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 112/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 113/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 114/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 115/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 116/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 117/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 118/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 119/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 120/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 121/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 122/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 123/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 124/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 125/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 126/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 127/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 128/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 129/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 130/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 131/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 132/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 133/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 134/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 135/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 136/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 137/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 138/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 139/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 140/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 141/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 142/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 143/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 144/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 145/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 146/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 147/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 148/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 149/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 150/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 151/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 152/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 153/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 154/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 155/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 156/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 157/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 158/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 159/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 160/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 161/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 162/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 163/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 164/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 165/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 166/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 167/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 168/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 169/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 170/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 171/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 172/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 173/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 174/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 175/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 176/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 177/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 178/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 179/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
Epoch 180/180
1493/1493 - 5s - loss: 0.0060 - val_loss: 0.0059 - 5s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.005886585917323828
  1/332 [..............................] - ETA: 41s 44/332 [==>...........................] - ETA: 0s  87/332 [======>.......................] - ETA: 0s130/332 [==========>...................] - ETA: 0s173/332 [==============>...............] - ETA: 0s217/332 [==================>...........] - ETA: 0s261/332 [======================>.......] - ETA: 0s305/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.10897014185342527
cosine 0.08321674417492313
MAE: 0.044958282
RMSE: 0.09670957
r2: 0.39380751430356953
RMSE zero-vector: 0.2430644284356365
['1.5custom_VAE', 'logcosh', 64, 180, 0.0008, 0.2, 252, 0.005952270235866308, 0.005886585917323828, 0.10897014185342527, 0.08321674417492313, 0.04495828226208687, 0.09670957177877426, 0.39380751430356953, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.4 210 0.0005 256 1] 3
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 505)          638825      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 505)         2020        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 505)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         834133      ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 1,730,002
Trainable params: 1,727,478
Non-trainable params: 2,524
__________________________________________________________________________________________________
Epoch 1/210
374/374 - 2s - loss: 0.0277 - val_loss: 0.0151 - 2s/epoch - 6ms/step
Epoch 2/210
374/374 - 1s - loss: 0.0141 - val_loss: 0.0144 - 1s/epoch - 3ms/step
Epoch 3/210
374/374 - 1s - loss: 0.0137 - val_loss: 0.0161 - 1s/epoch - 3ms/step
Epoch 4/210
374/374 - 1s - loss: 0.0135 - val_loss: 0.0141 - 1s/epoch - 3ms/step
Epoch 5/210
374/374 - 1s - loss: 0.0133 - val_loss: 0.0134 - 1s/epoch - 3ms/step
Epoch 6/210
374/374 - 1s - loss: 0.0132 - val_loss: 0.0154 - 1s/epoch - 3ms/step
Epoch 7/210
374/374 - 1s - loss: 0.0130 - val_loss: 0.0135 - 1s/epoch - 3ms/step
Epoch 8/210
374/374 - 1s - loss: 0.0129 - val_loss: 0.0142 - 1s/epoch - 3ms/step
Epoch 9/210
374/374 - 1s - loss: 0.0128 - val_loss: 0.0128 - 1s/epoch - 3ms/step
Epoch 10/210
374/374 - 1s - loss: 0.0125 - val_loss: 0.0127 - 1s/epoch - 3ms/step
Epoch 11/210
374/374 - 1s - loss: 0.0122 - val_loss: 0.0124 - 1s/epoch - 3ms/step
Epoch 12/210
374/374 - 1s - loss: 0.0119 - val_loss: 0.0119 - 1s/epoch - 3ms/step
Epoch 13/210
374/374 - 1s - loss: 0.0116 - val_loss: 0.0118 - 1s/epoch - 3ms/step
Epoch 14/210
374/374 - 1s - loss: 0.0114 - val_loss: 0.0114 - 1s/epoch - 3ms/step
Epoch 15/210
374/374 - 1s - loss: 0.0112 - val_loss: 0.0112 - 1s/epoch - 3ms/step
Epoch 16/210
374/374 - 1s - loss: 0.0110 - val_loss: 0.0109 - 1s/epoch - 3ms/step
Epoch 17/210
374/374 - 1s - loss: 0.0108 - val_loss: 0.0108 - 1s/epoch - 3ms/step
Epoch 18/210
374/374 - 1s - loss: 0.0107 - val_loss: 0.0110 - 1s/epoch - 3ms/step
Epoch 19/210
374/374 - 1s - loss: 0.0106 - val_loss: 0.0105 - 1s/epoch - 3ms/step
Epoch 20/210
374/374 - 1s - loss: 0.0105 - val_loss: 0.0104 - 1s/epoch - 3ms/step
Epoch 21/210
374/374 - 1s - loss: 0.0104 - val_loss: 0.0102 - 1s/epoch - 3ms/step
Epoch 22/210
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 3ms/step
Epoch 23/210
374/374 - 1s - loss: 0.0102 - val_loss: 0.0101 - 1s/epoch - 3ms/step
Epoch 24/210
374/374 - 1s - loss: 0.0102 - val_loss: 0.0101 - 1s/epoch - 3ms/step
Epoch 25/210
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 3ms/step
Epoch 26/210
374/374 - 1s - loss: 0.0101 - val_loss: 0.0101 - 1s/epoch - 3ms/step
Epoch 27/210
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 3ms/step
Epoch 28/210
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 3ms/step
Epoch 29/210
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 3ms/step
Epoch 30/210
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 3ms/step
Epoch 31/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 3ms/step
Epoch 32/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0099 - 1s/epoch - 3ms/step
Epoch 33/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 3ms/step
Epoch 34/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 3ms/step
Epoch 35/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 3ms/step
Epoch 36/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 3ms/step
Epoch 37/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 3ms/step
Epoch 38/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 3ms/step
Epoch 39/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 40/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 3ms/step
Epoch 41/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 42/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 43/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 44/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0097 - 1s/epoch - 3ms/step
Epoch 45/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 46/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 47/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 48/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 49/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 50/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 51/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 52/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 53/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 54/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 55/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 56/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 57/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 58/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 59/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 60/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 61/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 62/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 63/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 64/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 65/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 66/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 67/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 68/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 69/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 70/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 71/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 72/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 73/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 74/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 75/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 76/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 77/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 78/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 79/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 80/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 81/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 82/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 83/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 84/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 85/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 86/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 87/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 88/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 89/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 90/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 91/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 92/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 93/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 94/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 95/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 96/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 97/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 98/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 99/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 100/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 101/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 102/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 103/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 104/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 105/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 106/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 107/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 108/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 109/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 110/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 111/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 112/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 113/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 114/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 115/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 116/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 117/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 118/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 119/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 120/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 121/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 122/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 123/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 124/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 125/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 126/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 127/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 128/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 129/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 130/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 131/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 132/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 133/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 134/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 135/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 136/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 137/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 138/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 139/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 140/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 141/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 142/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 143/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 144/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 145/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 146/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 147/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 148/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 149/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 150/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 151/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 152/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 153/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 154/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 155/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 156/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 157/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 158/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 159/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 160/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 161/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 162/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 163/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 164/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 165/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 166/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 167/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 168/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 169/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 170/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 171/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 172/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 173/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 174/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 175/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 176/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 177/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 178/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 179/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 180/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 181/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 182/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 183/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 184/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 185/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 186/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 187/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 188/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 189/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 190/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 191/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 192/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 193/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 194/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 195/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 196/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 197/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 198/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 199/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 200/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 201/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 202/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 203/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 204/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 205/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 206/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 207/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 208/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 209/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 210/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.00911119394004345
  1/332 [..............................] - ETA: 37s 43/332 [==>...........................] - ETA: 0s  86/332 [======>.......................] - ETA: 0s130/332 [==========>...................] - ETA: 0s174/332 [==============>...............] - ETA: 0s218/332 [==================>...........] - ETA: 0s262/332 [======================>.......] - ETA: 0s306/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.06441650089456412
cosine 0.0493503987306116
MAE: 0.034434747
RMSE: 0.07515039
r2: 0.6339554307826458
RMSE zero-vector: 0.2430644284356365
['0.4custom_VAE', 'mse', 256, 210, 0.0005, 0.2, 252, 0.009188412688672543, 0.00911119394004345, 0.06441650089456412, 0.0493503987306116, 0.03443474695086479, 0.07515039294958115, 0.6339554307826458, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 150 0.00030000000000000003 256 1] 4
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
Epoch 1/150
374/374 - 2s - loss: 0.0281 - val_loss: 0.0153 - 2s/epoch - 6ms/step
Epoch 2/150
374/374 - 1s - loss: 0.0143 - val_loss: 0.0145 - 1s/epoch - 3ms/step
Epoch 3/150
374/374 - 1s - loss: 0.0139 - val_loss: 0.0158 - 1s/epoch - 3ms/step
Epoch 4/150
374/374 - 1s - loss: 0.0135 - val_loss: 0.0145 - 1s/epoch - 3ms/step
Epoch 5/150
374/374 - 1s - loss: 0.0134 - val_loss: 0.0145 - 1s/epoch - 3ms/step
Epoch 6/150
374/374 - 1s - loss: 0.0132 - val_loss: 0.0141 - 1s/epoch - 3ms/step
Epoch 7/150
374/374 - 1s - loss: 0.0131 - val_loss: 0.0136 - 1s/epoch - 3ms/step
Epoch 8/150
374/374 - 1s - loss: 0.0129 - val_loss: 0.0137 - 1s/epoch - 3ms/step
Epoch 9/150
374/374 - 1s - loss: 0.0128 - val_loss: 0.0130 - 1s/epoch - 4ms/step
Epoch 10/150
374/374 - 1s - loss: 0.0125 - val_loss: 0.0127 - 1s/epoch - 3ms/step
Epoch 11/150
374/374 - 1s - loss: 0.0122 - val_loss: 0.0123 - 1s/epoch - 3ms/step
Epoch 12/150
374/374 - 1s - loss: 0.0118 - val_loss: 0.0118 - 1s/epoch - 3ms/step
Epoch 13/150
374/374 - 1s - loss: 0.0115 - val_loss: 0.0119 - 1s/epoch - 3ms/step
Epoch 14/150
374/374 - 1s - loss: 0.0113 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 15/150
374/374 - 1s - loss: 0.0111 - val_loss: 0.0113 - 1s/epoch - 3ms/step
Epoch 16/150
374/374 - 1s - loss: 0.0110 - val_loss: 0.0118 - 1s/epoch - 3ms/step
Epoch 17/150
374/374 - 1s - loss: 0.0109 - val_loss: 0.0108 - 1s/epoch - 3ms/step
Epoch 18/150
374/374 - 1s - loss: 0.0107 - val_loss: 0.0116 - 1s/epoch - 3ms/step
Epoch 19/150
374/374 - 1s - loss: 0.0107 - val_loss: 0.0105 - 1s/epoch - 3ms/step
Epoch 20/150
374/374 - 1s - loss: 0.0105 - val_loss: 0.0105 - 1s/epoch - 3ms/step
Epoch 21/150
374/374 - 1s - loss: 0.0105 - val_loss: 0.0104 - 1s/epoch - 3ms/step
Epoch 22/150
374/374 - 1s - loss: 0.0104 - val_loss: 0.0106 - 1s/epoch - 3ms/step
Epoch 23/150
374/374 - 1s - loss: 0.0104 - val_loss: 0.0102 - 1s/epoch - 3ms/step
Epoch 24/150
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 3ms/step
Epoch 25/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0102 - 1s/epoch - 3ms/step
Epoch 26/150
374/374 - 1s - loss: 0.0102 - val_loss: 0.0101 - 1s/epoch - 3ms/step
Epoch 27/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 3ms/step
Epoch 28/150
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 3ms/step
Epoch 29/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0100 - 1s/epoch - 3ms/step
Epoch 30/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 3ms/step
Epoch 31/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 3ms/step
Epoch 32/150
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 3ms/step
Epoch 33/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 3ms/step
Epoch 34/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 3ms/step
Epoch 35/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 3ms/step
Epoch 36/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 3ms/step
Epoch 37/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 3ms/step
Epoch 38/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 3ms/step
Epoch 39/150
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 40/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 3ms/step
Epoch 41/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 3ms/step
Epoch 42/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 3ms/step
Epoch 43/150
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 44/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0097 - 1s/epoch - 3ms/step
Epoch 45/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0097 - 1s/epoch - 3ms/step
Epoch 46/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 47/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 48/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 49/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 50/150
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 51/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 52/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 53/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 54/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 55/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 56/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 57/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 58/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 59/150
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 60/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 61/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 62/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 63/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 64/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 65/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 66/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 67/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 68/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 69/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 70/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 71/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 72/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 73/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 74/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 75/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 76/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 77/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 78/150
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 79/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 80/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 81/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 82/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 83/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 84/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 85/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 86/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 87/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 88/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 89/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 90/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 91/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 92/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 93/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 94/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 95/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 96/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 97/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 98/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 99/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 100/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 101/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 102/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 103/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 104/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 105/150
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 106/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 107/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 108/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 109/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 110/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 111/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 112/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 113/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 114/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 115/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 116/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 117/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 118/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 119/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 120/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 121/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 122/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 123/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 124/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 125/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 126/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 127/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 128/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 129/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 130/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 131/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 132/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 133/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 134/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 135/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 136/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 137/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 138/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 139/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 140/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 141/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 142/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 143/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 144/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 145/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 146/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 147/150
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 148/150
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 149/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 150/150
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009172539226710796
  1/332 [..............................] - ETA: 37s 44/332 [==>...........................] - ETA: 0s  88/332 [======>.......................] - ETA: 0s131/332 [==========>...................] - ETA: 0s175/332 [==============>...............] - ETA: 0s219/332 [==================>...........] - ETA: 0s263/332 [======================>.......] - ETA: 0s307/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.06500815799831221
cosine 0.0498013636985456
MAE: 0.034468126
RMSE: 0.0754672
r2: 0.6308627179505202
RMSE zero-vector: 0.2430644284356365
['0.5custom_VAE', 'mse', 256, 150, 0.00030000000000000003, 0.2, 252, 0.009261667728424072, 0.009172539226710796, 0.06500815799831221, 0.0498013636985456, 0.03446812555193901, 0.07546719908714294, 0.6308627179505202, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 180 0.0008 64 1] 5
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
Epoch 1/180
1493/1493 - 5s - loss: 0.0201 - val_loss: 0.0154 - 5s/epoch - 4ms/step
Epoch 2/180
1493/1493 - 5s - loss: 0.0139 - val_loss: 0.0136 - 5s/epoch - 3ms/step
Epoch 3/180
1493/1493 - 5s - loss: 0.0132 - val_loss: 0.0127 - 5s/epoch - 3ms/step
Epoch 4/180
1493/1493 - 5s - loss: 0.0125 - val_loss: 0.0120 - 5s/epoch - 3ms/step
Epoch 5/180
1493/1493 - 5s - loss: 0.0118 - val_loss: 0.0113 - 5s/epoch - 3ms/step
Epoch 6/180
1493/1493 - 5s - loss: 0.0114 - val_loss: 0.0109 - 5s/epoch - 3ms/step
Epoch 7/180
1493/1493 - 5s - loss: 0.0110 - val_loss: 0.0107 - 5s/epoch - 3ms/step
Epoch 8/180
1493/1493 - 5s - loss: 0.0108 - val_loss: 0.0106 - 5s/epoch - 3ms/step
Epoch 9/180
1493/1493 - 5s - loss: 0.0107 - val_loss: 0.0104 - 5s/epoch - 3ms/step
Epoch 10/180
1493/1493 - 5s - loss: 0.0106 - val_loss: 0.0103 - 5s/epoch - 3ms/step
Epoch 11/180
1493/1493 - 5s - loss: 0.0105 - val_loss: 0.0102 - 5s/epoch - 3ms/step
Epoch 12/180
1493/1493 - 5s - loss: 0.0104 - val_loss: 0.0101 - 5s/epoch - 3ms/step
Epoch 13/180
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 14/180
1493/1493 - 5s - loss: 0.0103 - val_loss: 0.0100 - 5s/epoch - 3ms/step
Epoch 15/180
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 16/180
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 17/180
1493/1493 - 5s - loss: 0.0102 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 18/180
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0099 - 5s/epoch - 3ms/step
Epoch 19/180
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 20/180
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 21/180
1493/1493 - 5s - loss: 0.0101 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 22/180
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 23/180
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0098 - 5s/epoch - 3ms/step
Epoch 24/180
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 25/180
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 26/180
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 27/180
1493/1493 - 5s - loss: 0.0100 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 28/180
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 29/180
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0097 - 5s/epoch - 3ms/step
Epoch 30/180
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 31/180
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 32/180
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 33/180
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 34/180
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 35/180
1493/1493 - 5s - loss: 0.0099 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 36/180
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 37/180
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 38/180
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 39/180
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 40/180
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 41/180
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 42/180
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 43/180
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0096 - 5s/epoch - 3ms/step
Epoch 44/180
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 45/180
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 46/180
1493/1493 - 5s - loss: 0.0098 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 47/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 48/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 49/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 50/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 51/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 52/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 53/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 54/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 55/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 56/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 57/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 58/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 59/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0095 - 5s/epoch - 3ms/step
Epoch 60/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 61/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 62/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 63/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 64/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 65/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 66/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 67/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 68/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 69/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 70/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 71/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 72/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 73/180
1493/1493 - 5s - loss: 0.0097 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 74/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 75/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 76/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 77/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 78/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 79/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 80/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 81/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 82/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 83/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 84/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 85/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 86/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 87/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 88/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 89/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 90/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0094 - 5s/epoch - 3ms/step
Epoch 91/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 92/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 93/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 94/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 95/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 96/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 97/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 98/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 99/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 100/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 101/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 102/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 103/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 104/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 105/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 106/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 107/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 108/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 109/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 110/180
1493/1493 - 5s - loss: 0.0096 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 111/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 112/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 113/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 114/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 115/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 116/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 117/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 118/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 119/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 120/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 121/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 122/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 123/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 124/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 125/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 126/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 127/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 128/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 129/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 130/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 131/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 132/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 133/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 134/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 135/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 136/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 137/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 138/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 139/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 140/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 141/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 142/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 143/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 144/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 145/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 146/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 147/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 148/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 149/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 150/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 151/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 152/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 153/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 154/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 155/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 156/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 157/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 158/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 159/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 160/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 161/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 162/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 163/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 164/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 165/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 166/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 167/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 168/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 169/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 170/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 171/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 172/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 173/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 174/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0093 - 5s/epoch - 3ms/step
Epoch 175/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 176/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 177/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 178/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 179/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
Epoch 180/180
1493/1493 - 5s - loss: 0.0095 - val_loss: 0.0092 - 5s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009232030250132084
  1/332 [..............................] - ETA: 40s 43/332 [==>...........................] - ETA: 0s  86/332 [======>.......................] - ETA: 0s129/332 [==========>...................] - ETA: 0s172/332 [==============>...............] - ETA: 0s216/332 [==================>...........] - ETA: 0s260/332 [======================>.......] - ETA: 0s304/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.0677683558434394
cosine 0.051955325948127115
MAE: 0.035409108
RMSE: 0.07703724
r2: 0.6153436066407136
RMSE zero-vector: 0.2430644284356365
['0.5custom_VAE', 'mse', 64, 180, 0.0008, 0.2, 252, 0.009454607032239437, 0.009232030250132084, 0.0677683558434394, 0.051955325948127115, 0.0354091078042984, 0.07703723758459091, 0.6153436066407136, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'numpy.int64'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[1.4 210 0.0005 256 1] 6
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 1769)         2237785     ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 1769)        7076        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 1769)         0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          446040      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          446040      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         2756677     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 5,893,618
Trainable params: 5,886,038
Non-trainable params: 7,580
__________________________________________________________________________________________________
Epoch 1/210
374/374 - 2s - loss: 0.0350 - val_loss: 0.0216 - 2s/epoch - 6ms/step
Epoch 2/210
374/374 - 1s - loss: 0.0154 - val_loss: 0.0475 - 1s/epoch - 4ms/step
Epoch 3/210
374/374 - 1s - loss: 0.0157 - val_loss: 0.0181 - 1s/epoch - 4ms/step
Epoch 4/210
374/374 - 1s - loss: 0.0140 - val_loss: 0.0169 - 1s/epoch - 4ms/step
Epoch 5/210
374/374 - 1s - loss: 0.0138 - val_loss: 0.0271 - 1s/epoch - 4ms/step
Epoch 6/210
374/374 - 1s - loss: 0.0138 - val_loss: 0.0242 - 1s/epoch - 4ms/step
Epoch 7/210
374/374 - 1s - loss: 0.0133 - val_loss: 0.0136 - 1s/epoch - 4ms/step
Epoch 8/210
374/374 - 1s - loss: 0.0127 - val_loss: 0.0666 - 1s/epoch - 4ms/step
Epoch 9/210
374/374 - 1s - loss: 0.0129 - val_loss: 0.0124 - 1s/epoch - 4ms/step
Epoch 10/210
374/374 - 1s - loss: 0.0119 - val_loss: 0.0172 - 1s/epoch - 4ms/step
Epoch 11/210
374/374 - 1s - loss: 0.0120 - val_loss: 0.0117 - 1s/epoch - 4ms/step
Epoch 12/210
374/374 - 1s - loss: 0.0114 - val_loss: 0.0146 - 1s/epoch - 4ms/step
Epoch 13/210
374/374 - 1s - loss: 0.0114 - val_loss: 0.0176 - 1s/epoch - 4ms/step
Epoch 14/210
374/374 - 1s - loss: 0.0115 - val_loss: 0.0117 - 1s/epoch - 4ms/step
Epoch 15/210
374/374 - 1s - loss: 0.0112 - val_loss: 0.0122 - 1s/epoch - 4ms/step
Epoch 16/210
374/374 - 1s - loss: 0.0117 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 17/210
374/374 - 1s - loss: 0.0108 - val_loss: 0.0115 - 1s/epoch - 4ms/step
Epoch 18/210
374/374 - 1s - loss: 0.0113 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 19/210
374/374 - 1s - loss: 0.0107 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 20/210
374/374 - 1s - loss: 0.0107 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 21/210
374/374 - 1s - loss: 0.0105 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 22/210
374/374 - 1s - loss: 0.0112 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 23/210
374/374 - 1s - loss: 0.0110 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 24/210
374/374 - 1s - loss: 0.0122 - val_loss: 0.0136 - 1s/epoch - 4ms/step
Epoch 25/210
374/374 - 1s - loss: 0.0245 - val_loss: 0.0124 - 1s/epoch - 4ms/step
Epoch 26/210
374/374 - 1s - loss: 0.0199 - val_loss: 0.0126 - 1s/epoch - 4ms/step
Epoch 27/210
374/374 - 1s - loss: 0.0142 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 28/210
374/374 - 1s - loss: 0.0121 - val_loss: 0.0116 - 1s/epoch - 4ms/step
Epoch 29/210
374/374 - 1s - loss: 0.0121 - val_loss: 0.0113 - 1s/epoch - 4ms/step
Epoch 30/210
374/374 - 1s - loss: 0.0114 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 31/210
374/374 - 1s - loss: 0.0113 - val_loss: 0.0112 - 1s/epoch - 4ms/step
Epoch 32/210
374/374 - 1s - loss: 0.0117 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 33/210
374/374 - 1s - loss: 0.0109 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 34/210
374/374 - 1s - loss: 0.0109 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 35/210
374/374 - 1s - loss: 0.0117 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 36/210
374/374 - 1s - loss: 0.0108 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 37/210
374/374 - 1s - loss: 0.0108 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 38/210
374/374 - 1s - loss: 0.0107 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 39/210
374/374 - 1s - loss: 0.0106 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 40/210
374/374 - 1s - loss: 0.0120 - val_loss: 0.0105 - 1s/epoch - 4ms/step
Epoch 41/210
374/374 - 1s - loss: 0.0106 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 42/210
374/374 - 1s - loss: 0.0109 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 43/210
374/374 - 1s - loss: 0.0105 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 44/210
374/374 - 1s - loss: 0.0104 - val_loss: 0.0108 - 1s/epoch - 4ms/step
Epoch 45/210
374/374 - 1s - loss: 0.0107 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 46/210
374/374 - 1s - loss: 0.0111 - val_loss: 0.0111 - 1s/epoch - 4ms/step
Epoch 47/210
374/374 - 1s - loss: 0.0119 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 48/210
374/374 - 1s - loss: 0.0104 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 49/210
374/374 - 1s - loss: 0.0104 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 50/210
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 51/210
374/374 - 1s - loss: 0.0102 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 52/210
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 53/210
374/374 - 1s - loss: 0.0103 - val_loss: 0.0126 - 1s/epoch - 4ms/step
Epoch 54/210
374/374 - 1s - loss: 0.0133 - val_loss: 0.0110 - 1s/epoch - 4ms/step
Epoch 55/210
374/374 - 1s - loss: 0.0118 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 56/210
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 57/210
374/374 - 1s - loss: 0.0104 - val_loss: 0.0102 - 1s/epoch - 4ms/step
Epoch 58/210
374/374 - 1s - loss: 0.0104 - val_loss: 0.0103 - 1s/epoch - 4ms/step
Epoch 59/210
374/374 - 1s - loss: 0.0106 - val_loss: 0.0107 - 1s/epoch - 4ms/step
Epoch 60/210
374/374 - 1s - loss: 0.0107 - val_loss: 0.0106 - 1s/epoch - 4ms/step
Epoch 61/210
374/374 - 1s - loss: 0.0108 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 62/210
374/374 - 1s - loss: 0.0102 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 63/210
374/374 - 1s - loss: 0.0102 - val_loss: 0.0119 - 1s/epoch - 4ms/step
Epoch 64/210
374/374 - 1s - loss: 0.0114 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 65/210
374/374 - 1s - loss: 0.0101 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 66/210
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 67/210
374/374 - 1s - loss: 0.0101 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 68/210
374/374 - 1s - loss: 0.0111 - val_loss: 0.0104 - 1s/epoch - 4ms/step
Epoch 69/210
374/374 - 1s - loss: 0.0105 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 70/210
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 71/210
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 72/210
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 73/210
374/374 - 1s - loss: 0.0100 - val_loss: 0.0109 - 1s/epoch - 4ms/step
Epoch 74/210
374/374 - 1s - loss: 0.0104 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 75/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 76/210
374/374 - 1s - loss: 0.0100 - val_loss: 0.0101 - 1s/epoch - 4ms/step
Epoch 77/210
374/374 - 1s - loss: 0.0101 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 78/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 79/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 80/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 81/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 82/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 83/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0100 - 1s/epoch - 4ms/step
Epoch 84/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 85/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 86/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 87/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0098 - 1s/epoch - 4ms/step
Epoch 88/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 89/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 90/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 91/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 92/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0099 - 1s/epoch - 4ms/step
Epoch 93/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 94/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0097 - 1s/epoch - 4ms/step
Epoch 95/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 96/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 97/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 98/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 99/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 100/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 101/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 102/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 103/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 104/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 105/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 106/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 107/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 108/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 109/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 110/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 111/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 112/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 113/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 4ms/step
Epoch 114/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 115/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 116/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 117/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 118/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 119/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 120/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 121/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 122/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 123/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 124/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 125/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 126/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 127/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 128/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 129/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 130/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 131/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 132/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 133/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 134/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 135/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 136/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 137/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 4ms/step
Epoch 138/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 139/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 140/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 141/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 142/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 143/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 144/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 145/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 146/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 147/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 148/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 149/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 150/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 151/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 152/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 153/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 154/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 155/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 156/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 157/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 158/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 159/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 160/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 161/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 162/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 163/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 164/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 165/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 166/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 167/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 168/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 169/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 170/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 171/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 172/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 173/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 174/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 175/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 176/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 177/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 178/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 179/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 180/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 181/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 182/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 183/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 184/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 185/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 186/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 187/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 188/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 189/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 190/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 191/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 192/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 193/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 194/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 195/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 196/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 197/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 198/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 199/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 200/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 201/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 202/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 203/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 204/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 205/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 206/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 207/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 208/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 209/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
Epoch 210/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 4ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.009195756167173386
  1/332 [..............................] - ETA: 39s 43/332 [==>...........................] - ETA: 0s  86/332 [======>.......................] - ETA: 0s129/332 [==========>...................] - ETA: 0s173/332 [==============>...............] - ETA: 0s217/332 [==================>...........] - ETA: 0s261/332 [======================>.......] - ETA: 0s305/332 [==========================>...] - ETA: 0s332/332 [==============================] - 1s 1ms/step
correlation 0.06748676400026464
cosine 0.05172676136144997
MAE: 0.03507439
RMSE: 0.07684334
r2: 0.617277565838066
RMSE zero-vector: 0.2430644284356365
['1.4custom_VAE', 'mse', 256, 210, 0.0005, 0.2, 252, 0.00928334891796112, 0.009195756167173386, 0.06748676400026464, 0.05172676136144997, 0.0350743904709816, 0.07684334367513657, 0.617277565838066, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 210 0.00030000000000000003 256 1] 7
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
Epoch 1/210
374/374 - 2s - loss: 0.0291 - val_loss: 0.0156 - 2s/epoch - 6ms/step
Epoch 2/210
374/374 - 1s - loss: 0.0143 - val_loss: 0.0146 - 1s/epoch - 3ms/step
Epoch 3/210
374/374 - 1s - loss: 0.0138 - val_loss: 0.0166 - 1s/epoch - 3ms/step
Epoch 4/210
374/374 - 1s - loss: 0.0135 - val_loss: 0.0141 - 1s/epoch - 3ms/step
Epoch 5/210
374/374 - 1s - loss: 0.0134 - val_loss: 0.0136 - 1s/epoch - 3ms/step
Epoch 6/210
374/374 - 1s - loss: 0.0132 - val_loss: 0.0141 - 1s/epoch - 3ms/step
Epoch 7/210
374/374 - 1s - loss: 0.0131 - val_loss: 0.0136 - 1s/epoch - 3ms/step
Epoch 8/210
374/374 - 1s - loss: 0.0130 - val_loss: 0.0154 - 1s/epoch - 3ms/step
Epoch 9/210
374/374 - 1s - loss: 0.0129 - val_loss: 0.0130 - 1s/epoch - 3ms/step
Epoch 10/210
374/374 - 1s - loss: 0.0126 - val_loss: 0.0131 - 1s/epoch - 3ms/step
Epoch 11/210
374/374 - 1s - loss: 0.0125 - val_loss: 0.0125 - 1s/epoch - 3ms/step
Epoch 12/210
374/374 - 1s - loss: 0.0124 - val_loss: 0.0125 - 1s/epoch - 3ms/step
Epoch 13/210
374/374 - 1s - loss: 0.0122 - val_loss: 0.0124 - 1s/epoch - 3ms/step
Epoch 14/210
374/374 - 1s - loss: 0.0121 - val_loss: 0.0122 - 1s/epoch - 3ms/step
Epoch 15/210
374/374 - 1s - loss: 0.0120 - val_loss: 0.0118 - 1s/epoch - 3ms/step
Epoch 16/210
374/374 - 1s - loss: 0.0116 - val_loss: 0.0114 - 1s/epoch - 3ms/step
Epoch 17/210
374/374 - 1s - loss: 0.0113 - val_loss: 0.0112 - 1s/epoch - 3ms/step
Epoch 18/210
374/374 - 1s - loss: 0.0110 - val_loss: 0.0110 - 1s/epoch - 3ms/step
Epoch 19/210
374/374 - 1s - loss: 0.0108 - val_loss: 0.0107 - 1s/epoch - 3ms/step
Epoch 20/210
374/374 - 1s - loss: 0.0106 - val_loss: 0.0105 - 1s/epoch - 3ms/step
Epoch 21/210
374/374 - 1s - loss: 0.0105 - val_loss: 0.0103 - 1s/epoch - 3ms/step
Epoch 22/210
374/374 - 1s - loss: 0.0103 - val_loss: 0.0103 - 1s/epoch - 3ms/step
Epoch 23/210
374/374 - 1s - loss: 0.0103 - val_loss: 0.0102 - 1s/epoch - 3ms/step
Epoch 24/210
374/374 - 1s - loss: 0.0102 - val_loss: 0.0101 - 1s/epoch - 3ms/step
Epoch 25/210
374/374 - 1s - loss: 0.0101 - val_loss: 0.0101 - 1s/epoch - 3ms/step
Epoch 26/210
374/374 - 1s - loss: 0.0101 - val_loss: 0.0099 - 1s/epoch - 3ms/step
Epoch 27/210
374/374 - 1s - loss: 0.0100 - val_loss: 0.0099 - 1s/epoch - 3ms/step
Epoch 28/210
374/374 - 1s - loss: 0.0100 - val_loss: 0.0100 - 1s/epoch - 3ms/step
Epoch 29/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 3ms/step
Epoch 30/210
374/374 - 1s - loss: 0.0099 - val_loss: 0.0098 - 1s/epoch - 3ms/step
Epoch 31/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0098 - 1s/epoch - 3ms/step
Epoch 32/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 3ms/step
Epoch 33/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 3ms/step
Epoch 34/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 3ms/step
Epoch 35/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0097 - 1s/epoch - 3ms/step
Epoch 36/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 37/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0097 - 1s/epoch - 3ms/step
Epoch 38/210
374/374 - 1s - loss: 0.0098 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 39/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 40/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 41/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 42/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0096 - 1s/epoch - 3ms/step
Epoch 43/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 44/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0096 - 1s/epoch - 4ms/step
Epoch 45/210
374/374 - 1s - loss: 0.0097 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 46/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 47/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 48/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 49/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 50/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0095 - 1s/epoch - 3ms/step
Epoch 51/210
374/374 - 1s - loss: 0.0096 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 52/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 53/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 54/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 55/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 56/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 57/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 58/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 59/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 60/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 61/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 62/210
374/374 - 1s - loss: 0.0095 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 63/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 64/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 65/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 66/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0094 - 1s/epoch - 3ms/step
Epoch 67/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 68/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 69/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 70/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 71/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 72/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 73/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 74/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 4ms/step
Epoch 75/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 76/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 77/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 78/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 79/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 80/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 81/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 82/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 83/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 84/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 85/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 86/210
374/374 - 1s - loss: 0.0094 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 87/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 88/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 89/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 90/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 91/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 92/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0093 - 1s/epoch - 3ms/step
Epoch 93/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 94/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 95/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 96/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 97/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 98/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 99/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 100/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 101/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 102/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 103/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 104/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 105/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 106/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 107/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 108/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 109/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 110/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 111/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 112/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 113/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 114/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 115/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 116/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 117/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 118/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 119/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 120/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 121/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 122/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 123/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 124/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 125/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 126/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 127/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 128/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 129/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 130/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 131/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 132/210
374/374 - 1s - loss: 0.0093 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 133/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 134/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 135/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 136/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 137/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 138/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 139/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 140/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 141/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0092 - 1s/epoch - 3ms/step
Epoch 142/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 143/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 144/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 145/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 146/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 147/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 148/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 149/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 150/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 151/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 152/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 153/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 154/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 155/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 156/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 157/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 158/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 159/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 160/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 161/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 162/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 163/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 164/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 165/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 166/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 167/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 168/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 169/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 170/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 171/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 172/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 173/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 174/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 175/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 176/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 177/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 178/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 179/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 180/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 181/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 182/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 183/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 184/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 185/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 186/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 187/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 188/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 189/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 190/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 191/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 192/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 193/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 4ms/step
Epoch 194/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 195/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 196/210
374/374 - 1s - loss: 0.0091 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 197/210
374/374 - 1s - loss: 0.0091 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 198/210
374/374 - 1s - loss: 0.0091 - val_loss: 0.0091 - 1s/epoch - 4ms/step
Epoch 199/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 200/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0090 - 1s/epoch - 3ms/step
Epoch 201/210
374/374 - 1s - loss: 0.0091 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 202/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 203/210
374/374 - 1s - loss: 0.0091 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 204/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 205/210
374/374 - 1s - loss: 0.0091 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 206/210
374/374 - 1s - loss: 0.0092 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 207/210
374/374 - 1s - loss: 0.0091 - val_loss: 0.0091 - 1s/epoch - 3ms/step
Epoch 208/210
374/374 - 1s - loss: 0.0091 - val_loss: 0.0090 - 1s/epoch - 3ms/step
Epoch 209/210
374/374 - 1s - loss: 0.0091 - val_loss: 0.0090 - 1s/epoch - 3ms/step
Epoch 210/210
374/374 - 1s - loss: 0.0091 - val_loss: 0.0091 - 1s/epoch - 3ms/step
COMPRESSED VECTOR SIZE: 252
Loss in the autoencoder: 0.00907708890736103
  1/332 [..............................] - ETA: 37s 44/332 [==>...........................] - ETA: 0s  87/332 [======>.......................] - ETA: 0s131/332 [==========>...................] - ETA: 0s175/332 [==============>...............] - ETA: 0s219/332 [==================>...........] - ETA: 0s263/332 [======================>.......] - ETA: 0s307/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.06264709738843899
cosine 0.047997341958522975
MAE: 0.034130983
RMSE: 0.0741544
r2: 0.6435939911372249
RMSE zero-vector: 0.2430644284356365
['0.5custom_VAE', 'mse', 256, 210, 0.00030000000000000003, 0.2, 252, 0.009138920344412327, 0.00907708890736103, 0.06264709738843899, 0.047997341958522975, 0.03413098305463791, 0.07415439933538437, 0.6435939911372249, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.4 210 0.0005 256 1] 8
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 505)          638825      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 505)         2020        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 505)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          127512      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         834133      ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 1,730,002
Trainable params: 1,727,478
Non-trainable params: 2,524
__________________________________________________________________________________________________
File geneticVAE_MMmp_gap_custom_VAE0.4_cr0.2_bs256_ep210_loss_mse_lr0.0005_AutoEncoder.h5 exists in folder already, skiping this calculation.
  1/332 [..............................] - ETA: 33s 44/332 [==>...........................] - ETA: 0s  87/332 [======>.......................] - ETA: 0s130/332 [==========>...................] - ETA: 0s174/332 [==============>...............] - ETA: 0s218/332 [==================>...........] - ETA: 0s262/332 [======================>.......] - ETA: 0s306/332 [==========================>...] - ETA: 0s332/332 [==============================] - 0s 1ms/step
correlation 0.06441650089456412
cosine 0.0493503987306116
MAE: 0.034434747
RMSE: 0.07515039
r2: 0.6339554307826458
RMSE zero-vector: 0.2430644284356365
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
['0.4custom_VAE', 'mse', 256, 210, 0.0005, 0.2, 252, '--', '--', 0.06441650089456412, 0.0493503987306116, 0.03443474695086479, 0.07515039294958115, 0.6339554307826458, 0.2430644284356365] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>, <class 'float'>]
[0.5 150 0.0007 256 1] 9
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
Model: "VAE"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_enc (InputLayer)         [(None, 1264)]       0           []                               
                                                                                                  
 dense_enc0 (Dense)             (None, 632)          799480      ['input_enc[0][0]']              
                                                                                                  
 batch_normalization (BatchNorm  (None, 632)         2528        ['dense_enc0[0][0]']             
 alization)                                                                                       
                                                                                                  
 re_lu (ReLU)                   (None, 632)          0           ['batch_normalization[0][0]']    
                                                                                                  
 bottleneck_zmean (Dense)       (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck_zlog (Dense)        (None, 252)          159516      ['re_lu[0][0]']                  
                                                                                                  
 bottleneck (SamplingLayer)     (None, 252)          0           ['bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
 decoder_model (Functional)     (None, 1264)         1027300     ['bottleneck[0][0]']             
                                                                                                  
 vae_loss_layer (VAELossLayer)  (None, 1264)         0           ['input_enc[0][0]',              
                                                                  'decoder_model[0][0]',          
                                                                  'bottleneck_zmean[0][0]',       
                                                                  'bottleneck_zlog[0][0]']        
                                                                                                  
==================================================================================================
Total params: 2,148,340
Trainable params: 2,145,308
Non-trainable params: 3,032
__________________________________________________________________________________________________
Epoch 1/150
2023-02-21 21:11:45.324088: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 53654272 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 17956864/23676715008
2023-02-21 21:11:45.324123: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22896212864
MaxInUse:                  22907836532
NumAllocs:                   266336331
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-21 21:11:45.324184: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-21 21:11:45.324189: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 106
2023-02-21 21:11:45.324192: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 100
2023-02-21 21:11:45.324195: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 429
2023-02-21 21:11:45.324198: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-21 21:11:45.324201: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 54
2023-02-21 21:11:45.324203: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 176
2023-02-21 21:11:45.324206: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 129
2023-02-21 21:11:45.324209: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 22
2023-02-21 21:11:45.324211: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 186
2023-02-21 21:11:45.324214: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 22
2023-02-21 21:11:45.324217: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 22
2023-02-21 21:11:45.324219: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 22
2023-02-21 21:11:45.324222: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 75
2023-02-21 21:11:45.324231: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 21
2023-02-21 21:11:45.324234: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 72
2023-02-21 21:11:45.324237: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 21
2023-02-21 21:11:45.324240: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1783152, 9
2023-02-21 21:11:45.324242: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 75
2023-02-21 21:11:45.324245: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2038176, 9
2023-02-21 21:11:45.324248: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 9
2023-02-21 21:11:45.324250: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 14
2023-02-21 21:11:45.324253: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 9
2023-02-21 21:11:45.324256: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 48
2023-02-21 21:11:45.324258: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 14
2023-02-21 21:11:45.324261: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 6
2023-02-21 21:11:45.324264: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 50
2023-02-21 21:11:45.324266: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 6
2023-02-21 21:11:45.324269: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 6
2023-02-21 21:11:45.324272: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 6
2023-02-21 21:11:45.324274: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 23
2023-02-21 21:11:45.324277: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 42
Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
Training of this model failed.
['0.5custom_VAE', 'mse', 256, 150, 0.0007, 0.2, 252, '--', '--', '--', '--', '--', '--', '--', '--'] [<class 'str'>, <class 'str'>, <class 'int'>, <class 'int'>, <class 'float'>, <class 'numpy.float64'>, <class 'int'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]
ATTENTION: MAE is a string, setting it to 1000000
It means that the autoencoder failed to train so solution ([0.5 150 0.0007 256 1]) is not valid.
Saved GA instance to file: ./tmp//ga_instance_generation_2.pkl
[1.5 210 0.0005 256 1] 2
./tmp/ already created.
Shape of dataset to encode: (106113, 1264)
2023-02-21 21:11:53.354583: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 9586176 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 17956864/23676715008
2023-02-21 21:11:53.354612: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                     21639593984
InUse:                     22915385160
MaxInUse:                  22915385164
NumAllocs:                   266336336
MaxAllocSize:                482853056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2023-02-21 21:11:53.354673: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2023-02-21 21:11:53.354683: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 92
2023-02-21 21:11:53.354686: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 98
2023-02-21 21:11:53.354688: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16, 1
2023-02-21 21:11:53.354698: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1008, 429
2023-02-21 21:11:53.354701: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2023-02-21 21:11:53.354704: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2020, 54
2023-02-21 21:11:53.354707: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2528, 176
2023-02-21 21:11:53.354709: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 5056, 129
2023-02-21 21:11:53.354712: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7076, 22
2023-02-21 21:11:53.354715: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 7584, 186
2023-02-21 21:11:53.354717: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8088, 22
2023-02-21 21:11:53.354720: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10112, 22
2023-02-21 21:11:53.354722: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12640, 22
2023-02-21 21:11:53.354725: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 254016, 75
2023-02-21 21:11:53.354728: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 509040, 21
2023-02-21 21:11:53.354730: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 637056, 72
2023-02-21 21:11:53.354733: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1274112, 21
2023-02-21 21:11:53.354736: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1783152, 9
2023-02-21 21:11:53.354738: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1911168, 75
2023-02-21 21:11:53.354741: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2038176, 9
2023-02-21 21:11:53.354744: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2548224, 9
2023-02-21 21:11:53.354746: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 2553280, 14
2023-02-21 21:11:53.354749: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3185280, 9
2023-02-21 21:11:53.354751: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 3195392, 48
2023-02-21 21:11:53.354754: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 6390784, 14
2023-02-21 21:11:53.354757: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8944064, 6
2023-02-21 21:11:53.354759: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 9586176, 52
2023-02-21 21:11:53.354762: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 10223232, 6
2023-02-21 21:11:53.354765: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 12781568, 6
2023-02-21 21:11:53.354767: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 15976960, 6
2023-02-21 21:11:53.354770: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 53654272, 23
2023-02-21 21:11:53.354773: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 482853056, 42
2023-02-21 21:11:53.390500: W tensorflow/core/framework/op_kernel.cc:1768] RESOURCE_EXHAUSTED: failed to allocate memory
Traceback (most recent call last):
  File "genetic.py", line 47, in <module>
    main()
  File "genetic.py", line 21, in main
    genetic_hypertune_autoencoder(prefix_name = 'geneticVAE_MMmp_gap',
  File "/auto/globalscratch/users/r/g/rgouvea/ProjectVAE_MODNet/GeneticAlgorithmTest/GeneticVAE_MMmpgap/cr_0.6/../../genetic_hypertune.py", line 206, in genetic_hypertune_autoencoder
    ga_instance.run()
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py", line 1413, in run
    self.last_generation_fitness = self.cal_pop_fitness()
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/pygad/pygad.py", line 1235, in cal_pop_fitness
    fitness = self.fitness_func(sol, sol_idx)
  File "/auto/globalscratch/users/r/g/rgouvea/ProjectVAE_MODNet/GeneticAlgorithmTest/GeneticVAE_MMmpgap/cr_0.6/../../genetic_hypertune.py", line 73, in fitness_func
    results_dict=train_autoencoder(prefix_name = prefix_name, 
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/autoencoder_tools-0.0.2-py3.8.egg/autoencoder_tools/autoencoder_setup.py", line 357, in train_autoencoder
    model = create_autoencoder(n_inputs=n_inputs, layers_structure=layers_structure, 
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/autoencoder_tools-0.0.2-py3.8.egg/autoencoder_tools/autoencoder_setup.py", line 550, in create_autoencoder
    e = Dense(neurons_per_layer[i], name=f'dense_enc{i}')(input_layer)
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/keras/utils/traceback_utils.py", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/ucl/modl/rgouvea/anaconda3/envs/env_tfmodnet/lib/python3.8/site-packages/keras/backend.py", line 2100, in random_uniform
    return tf.random.stateless_uniform(
tensorflow.python.framework.errors_impl.ResourceExhaustedError: {{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:AddV2]
Tue Feb 21 21:12:02 CET 2023
done
